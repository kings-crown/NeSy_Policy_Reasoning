{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ArqgnsfGhgHI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-17 18:12:35,821] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO 02-17 18:12:36 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import textwrap\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import Future\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Callable, Dict, List, Optional, TYPE_CHECKING, Union\n",
    "from unittest.mock import patch\n",
    "import dataset\n",
    "import datasets\n",
    "\n",
    "import trl\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import transformers\n",
    "from accelerate.utils import broadcast_object_list, gather_object\n",
    "from dataclasses import dataclass, field\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    TrainerCallback,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from trl import ModelConfig, ScriptArguments, TrlParser, get_peft_config\n",
    "from trl.data_utils import (\n",
    "    apply_chat_template,\n",
    "    is_conversational,\n",
    "    maybe_apply_chat_template,\n",
    ")\n",
    "from trl.models import unwrap_model_for_generation\n",
    "from trl.trainer import GRPOTrainer\n",
    "from trl.trainer.grpo_config import GRPOConfig\n",
    "from trl.trainer.utils import pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bz7whDjhk4jJ"
   },
   "outputs": [],
   "source": [
    "#from utils.callbacks import get_callbacks\n",
    "def get_callbacks(train_config, model_config) -> List[TrainerCallback]:\n",
    "    callbacks = []\n",
    "    for callback_name in train_config.callbacks:\n",
    "        if callback_name not in CALLBACKS:\n",
    "            raise ValueError(f\"Callback {callback_name} not found in CALLBACKS.\")\n",
    "        callbacks.append(CALLBACKS[callback_name](model_config))\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Checker(object):\n",
    "    \"\"\"A modified version of the Draft, Sketch, Prove proof-checking client.\n",
    "    (https://github.com/albertqjiang/draft_sketch_prove/blob/main/autoformalization/checker.py)\n",
    "\n",
    "    This checker supports Isabelle2022 via the new version of PISA\n",
    "    (https://albertqjiang.github.io/Portal-to-ISAbelle/).\n",
    "\n",
    "    It supports checking a miniF2F-style proof via `check`.\n",
    "\n",
    "    Finally, it replaces `sledgehammer` with a call to `normalhammer`.\n",
    "    \"\"\"\n",
    "    def __init__(self, working_dir, isa_path, theory_file_path, port=9000):\n",
    "        sys.path.append(os.environ.get('PISA_PATH', ''))\n",
    "        try:\n",
    "            from pisa_client import initialise_env\n",
    "            self.initialise_env = initialise_env\n",
    "        except ImportError:\n",
    "            print(\"Set $PISA_PATH to /yourpath/to/Portal-to-ISAbelle/src/main/python\")\n",
    "\n",
    "        self.working_dir = working_dir\n",
    "        self.isa_path = isa_path\n",
    "        self.theory_file_path = theory_file_path\n",
    "        self.port = port\n",
    "\n",
    "    def _initialize(self):\n",
    "        \"\"\"Initialize the PISA environment.\"\"\"\n",
    "        env = self.initialise_env(\n",
    "            self.port,\n",
    "            isa_path=self.isa_path,\n",
    "            theory_file_path=self.theory_file_path,\n",
    "            working_directory=self.working_dir\n",
    "        )\n",
    "        return env\n",
    "\n",
    "    def _exit(self, env):\n",
    "        \"\"\"Exit the environment and clean up resources.\"\"\"\n",
    "        try:\n",
    "            env.post('exit')\n",
    "        except Exception:\n",
    "            pass\n",
    "        os.system(\"ps aux | grep Isabelle | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "        os.system(\"ps aux | grep poly | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "\n",
    "    def _parse_output(self, obs):\n",
    "        \"\"\"Parse the sledgehammer output, returning the relevant part.\"\"\"\n",
    "        return obs.split('<hammer>')[0] if '<hammer>' in obs else ''\n",
    "\n",
    "    def _run_step(self, step, i, tls_name, env):\n",
    "        \"\"\"Run a single proof step.\"\"\"\n",
    "        try:\n",
    "            obs, reward, done, metadata = env.step_to_top_level_state(\n",
    "                action=step,\n",
    "                tls_name=tls_name,\n",
    "                new_name=f'default_{i}'\n",
    "            )\n",
    "            return obs, reward, done, metadata, None\n",
    "        except Exception as e:\n",
    "            return '', 0, False, None, str(e)\n",
    "\n",
    "    def _run_sledgehammer(self, step, i, tls_name, env):\n",
    "        \"\"\"Run sledgehammer or fallback heuristics on a step.\"\"\"\n",
    "        heuristics = [\n",
    "            'by auto', 'by simp', 'by blast', 'by fastforce',\n",
    "            'by force', 'by eval', 'by presburger', 'by sos',\n",
    "            'by arith', 'by linarith', 'by (auto simp: field_simps)'\n",
    "        ]\n",
    "        for heuristic in heuristics:\n",
    "            step_ = step.replace('normalhammer', heuristic)\n",
    "            obs, reward, done, metadata, error = self._run_step(step_, i, tls_name, env)\n",
    "            if error is None:\n",
    "                obs = f'{heuristic} <hammer> {obs}'\n",
    "                return obs, reward, done, metadata, error\n",
    "        return self._run_step(step.replace(\"normalhammer\", \"sledgehammer\"), i, tls_name, env)\n",
    "\n",
    "    def check(self, statement_and_proof):\n",
    "        \"\"\"Check the given proof.\"\"\"\n",
    "        env = self._initialize()\n",
    "        env.initialise()\n",
    "\n",
    "        theory = self.wrap_theorem(statement_and_proof)\n",
    "        steps = self.get_parsed(env, theory)\n",
    "\n",
    "        result = self._check(env, steps)\n",
    "        self._exit(env)\n",
    "\n",
    "        # Output the result\n",
    "        #print(\"\\n==== Success: %s\" % result['success'])\n",
    "        #print(\"--- Complete proof:\\n%s\" % result['theorem_and_proof'])\n",
    "        return result\n",
    "\n",
    "    def _check(self, env, steps):\n",
    "        \"\"\"Run the proof steps and collect results.\"\"\"\n",
    "        success, reason, done = False, '', False\n",
    "        step_results = []\n",
    "        tls_name = 'default'\n",
    "\n",
    "        for i, step in enumerate(steps):\n",
    "            time0 = time.time()\n",
    "            if 'normalhammer' in step or 'sledgehammer' in step:\n",
    "                obs, reward, done, metadata, error = self._run_sledgehammer(step, i, tls_name, env)\n",
    "            else:\n",
    "                obs, reward, done, metadata, error = self._run_step(step, i, tls_name, env)\n",
    "\n",
    "            step_time = time.time() - time0\n",
    "            step_results.append({\n",
    "                'index': i, 'step': step, \n",
    "                'output': self._parse_output(obs), \n",
    "                'step_time': step_time\n",
    "            })\n",
    "\n",
    "            if error:\n",
    "                reason = error\n",
    "                break\n",
    "            tls_name = f'default_{i}'\n",
    "\n",
    "        success = done and reward == 1.0\n",
    "        return {\n",
    "            'success': success,\n",
    "            'reason': reason,\n",
    "            'num_steps': len(steps),\n",
    "            'last_step': len(step_results),\n",
    "            'step_results': step_results,\n",
    "            'theorem_and_proof': self.reconstruct(step_results) if success else ''\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct(step_results):\n",
    "        \"\"\"Reconstruct the complete proof.\"\"\"\n",
    "        return '\\n'.join(\n",
    "            step_result['output'].strip() if step_result['output'] else step_result['step'].strip()\n",
    "            for step_result in step_results[1:]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_theorem(theorem):\n",
    "        \"\"\"Wrap the theorem in a theory file.\"\"\"\n",
    "        return (\n",
    "            'theory Interactive imports HOL.HOL Complex_Main '\n",
    "            '\"HOL-Library.Code_Target_Numeral\" \"HOL-Library.Sum_of_Squares\" '\n",
    "            '\"Symmetric_Polynomials.Vieta\" \"HOL-Computational_Algebra.Computational_Algebra\" '\n",
    "            '\"HOL-Number_Theory.Number_Theory\" \\n begin\\n%s' % theorem\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parsed(env, theory):\n",
    "        \"\"\"Parse the theory and extract proof steps.\"\"\"\n",
    "        raw_steps = env.post(f\"<parse text> ${theory}\")\n",
    "        steps = [s.strip() for s in raw_steps.split('<SEP>') if s.strip() and s != '$']\n",
    "        processed_steps = []\n",
    "        for i, step in enumerate(steps):\n",
    "            if step.lower() == \"then\" and (i == 0 or steps[i - 1].startswith(\"proof\")):\n",
    "                continue\n",
    "            processed_steps.append(step)\n",
    "        return processed_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1UNDdSjJl1vw"
   },
   "outputs": [],
   "source": [
    "#from grpo_trainer import GRPOTrainer\n",
    "\n",
    "\n",
    "RewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]\n",
    "\n",
    "\n",
    "class GRPOTrainer(GRPOTrainer):\n",
    "    # base trl GRPO_trainer\n",
    "    def compute_loss(\n",
    "        self, model, inputs, return_outputs=False, num_items_in_batch=None\n",
    "    ):\n",
    "        if return_outputs:\n",
    "            raise ValueError(\"The GRPOTrainer does not support returning outputs\")\n",
    "\n",
    "        device = self.accelerator.device\n",
    "        prompts = [x[\"prompt\"] for x in inputs]\n",
    "        prompts_text = [\n",
    "            maybe_apply_chat_template(example, self.processing_class)[\"prompt\"]\n",
    "            for example in inputs\n",
    "        ]\n",
    "        prompt_inputs = self.processing_class(\n",
    "            prompts_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            padding_side=\"left\",\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        prompt_inputs = super()._prepare_inputs(prompt_inputs)\n",
    "\n",
    "        if self.max_prompt_length is not None:\n",
    "            prompt_inputs[\"input_ids\"] = prompt_inputs[\"input_ids\"][\n",
    "                :, -self.max_prompt_length :\n",
    "            ]\n",
    "            prompt_inputs[\"attention_mask\"] = prompt_inputs[\"attention_mask\"][\n",
    "                :, -self.max_prompt_length :\n",
    "            ]\n",
    "\n",
    "        # Generate completions using either vLLM or regular generation\n",
    "        if self.args.use_vllm:\n",
    "            # First, have main process load weights if needed\n",
    "            if self.state.global_step != self._last_loaded_step:\n",
    "                with unwrap_model_for_generation(\n",
    "                    model, self.accelerator\n",
    "                ) as unwrapped_model:\n",
    "                    state_dict = unwrapped_model.state_dict()\n",
    "                if self.accelerator.is_main_process:\n",
    "                    llm_model = (\n",
    "                        self.llm.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "                    )\n",
    "                    llm_model.load_weights(state_dict.items())\n",
    "                self._last_loaded_step = self.state.global_step\n",
    "\n",
    "            # Generate completions using vLLM: gather all prompts and use them in a single call in the main process\n",
    "            all_prompts_text = gather_object(prompts_text)\n",
    "            if self.accelerator.is_main_process:\n",
    "                outputs = self.llm.generate(\n",
    "                    all_prompts_text,\n",
    "                    sampling_params=self.sampling_params,\n",
    "                    use_tqdm=False,\n",
    "                )\n",
    "                completion_ids = [\n",
    "                    out.token_ids\n",
    "                    for completions in outputs\n",
    "                    for out in completions.outputs\n",
    "                ]\n",
    "                for output in outputs:\n",
    "                    print(\"-\" * 100)\n",
    "                    print(\"\\n\\n\\n\")\n",
    "                    prompt = output.prompt\n",
    "                    for output_t in output.outputs:\n",
    "                        # print(completion_ids)\n",
    "                        print(\"=\" * 100)\n",
    "                        generated_text = output_t.text\n",
    "                        print(\"【USER】: \", prompt)\n",
    "                        print(\"\\n【ASSISTANT】:\", generated_text)\n",
    "            else:\n",
    "                completion_ids = [None] * len(all_prompts_text) * self.num_generations\n",
    "\n",
    "            # Broadcast the completions from the main process to all processes, ensuring each process receives its\n",
    "            # corresponding slice.\n",
    "            completion_ids = broadcast_object_list(completion_ids, from_process=0)\n",
    "            process_slice = slice(\n",
    "                self.accelerator.process_index * len(prompts) * self.num_generations,\n",
    "                (self.accelerator.process_index + 1)\n",
    "                * len(prompts)\n",
    "                * self.num_generations,\n",
    "            )\n",
    "            completion_ids = completion_ids[process_slice]\n",
    "\n",
    "            # Pad the completions, and concatenate them with the prompts\n",
    "            completion_ids = [\n",
    "                torch.tensor(ids, device=device) for ids in completion_ids\n",
    "            ]\n",
    "            completion_ids = pad(\n",
    "                completion_ids, padding_value=self.processing_class.pad_token_id\n",
    "            )\n",
    "            prompt_inputs_repeated = torch.repeat_interleave(\n",
    "                prompt_inputs[\"input_ids\"], self.num_generations, dim=0\n",
    "            ).to(device)\n",
    "            prompt_completion_ids = torch.cat(\n",
    "                [prompt_inputs_repeated, completion_ids], dim=1\n",
    "            )\n",
    "        else:\n",
    "            # Regular generation path\n",
    "            with unwrap_model_for_generation(\n",
    "                model, self.accelerator\n",
    "            ) as unwrapped_model:\n",
    "                prompt_inputs[\"input_ids\"] = prompt_inputs[\"input_ids\"].to(device)\n",
    "                prompt_inputs[\"attention_mask\"] = prompt_inputs[\"attention_mask\"].to(\n",
    "                    device\n",
    "                )\n",
    "\n",
    "                prompt_completion_ids = unwrapped_model.generate(\n",
    "                    **prompt_inputs, generation_config=self.generation_config\n",
    "                )\n",
    "\n",
    "        prompt_length = prompt_inputs[\"input_ids\"].size(1)\n",
    "        completion_ids = prompt_completion_ids[:, prompt_length:]\n",
    "\n",
    "        # Get the per-token log probabilities for the completions for the model and the reference model\n",
    "        def get_per_token_logps(model, input_ids, num_logits_to_keep):\n",
    "            # We add 1 to `num_logits_to_keep` because the last logits of the sequence is later excluded\n",
    "            logits = model(\n",
    "                input_ids, num_logits_to_keep=num_logits_to_keep + 1\n",
    "            ).logits  # (B, L, V)\n",
    "            logits = logits[\n",
    "                :, :-1, :\n",
    "            ]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred\n",
    "\n",
    "            # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\n",
    "            per_token_logps = []\n",
    "            for logits_row, input_ids_row in zip(\n",
    "                logits, input_ids[:, -num_logits_to_keep:]\n",
    "            ):\n",
    "                log_probs = logits_row.log_softmax(dim=-1)\n",
    "                token_log_prob = torch.gather(\n",
    "                    log_probs, dim=1, index=input_ids_row.unsqueeze(1)\n",
    "                ).squeeze(1)\n",
    "                per_token_logps.append(token_log_prob)\n",
    "            return torch.stack(per_token_logps)\n",
    "\n",
    "        num_logits_to_keep = completion_ids.size(\n",
    "            1\n",
    "        )  # we only need to compute the logits for the completion tokens\n",
    "        per_token_logps = get_per_token_logps(\n",
    "            model, prompt_completion_ids, num_logits_to_keep\n",
    "        )\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            if self.ref_model is not None:\n",
    "                ref_per_token_logps = get_per_token_logps(\n",
    "                    self.ref_model, prompt_completion_ids, num_logits_to_keep\n",
    "                )\n",
    "            else:\n",
    "                with self.accelerator.unwrap_model(model).disable_adapter():\n",
    "                    ref_per_token_logps = get_per_token_logps(\n",
    "                        model, prompt_completion_ids, num_logits_to_keep\n",
    "                    )\n",
    "\n",
    "        # Compute the KL divergence between the model and the reference model\n",
    "        per_token_kl = (\n",
    "            torch.exp(ref_per_token_logps - per_token_logps)\n",
    "            - (ref_per_token_logps - per_token_logps)\n",
    "            - 1\n",
    "        )\n",
    "\n",
    "        # Mask everything after the first EOS token\n",
    "        is_eos = completion_ids == self.processing_class.eos_token_id\n",
    "        eos_idx = torch.full(\n",
    "            (is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device\n",
    "        )\n",
    "        eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\n",
    "        sequence_indices = torch.arange(is_eos.size(1), device=device).expand(\n",
    "            is_eos.size(0), -1\n",
    "        )\n",
    "        completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "        # Decode the generated completions\n",
    "        completions = self.processing_class.batch_decode(\n",
    "            completion_ids, skip_special_tokens=True\n",
    "        )\n",
    "        if is_conversational(inputs[0]):\n",
    "            completions = [\n",
    "                [{\"role\": \"assistant\", \"content\": completion}]\n",
    "                for completion in completions\n",
    "            ]\n",
    "\n",
    "        # Compute the rewards\n",
    "        prompts = [prompt for prompt in prompts for _ in range(self.num_generations)]\n",
    "\n",
    "        rewards_per_func = torch.zeros(\n",
    "            len(prompts), len(self.reward_funcs), device=device\n",
    "        )\n",
    "        for i, (reward_func, reward_processing_class) in enumerate(\n",
    "            zip(self.reward_funcs, self.reward_processing_classes)\n",
    "        ):\n",
    "            if isinstance(reward_func, PreTrainedModel):\n",
    "                if is_conversational(inputs[0]):\n",
    "                    messages = [\n",
    "                        {\"messages\": p + c} for p, c in zip(prompts, completions)\n",
    "                    ]\n",
    "                    texts = [\n",
    "                        apply_chat_template(x, reward_processing_class)[\"text\"]\n",
    "                        for x in messages\n",
    "                    ]\n",
    "                else:\n",
    "                    texts = [p + c for p, c in zip(prompts, completions)]\n",
    "                reward_inputs = reward_processing_class(\n",
    "                    texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    padding_side=\"right\",\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "                reward_inputs = super()._prepare_inputs(reward_inputs)\n",
    "                with torch.inference_mode():\n",
    "                    rewards_per_func[:, i] = reward_func(**reward_inputs).logits[\n",
    "                        :, 0\n",
    "                    ]  # Shape (B*G,)\n",
    "            else:\n",
    "                # Repeat all input columns (but \"prompt\" and \"completion\") to match the number of generations\n",
    "                reward_kwargs = {\n",
    "                    key: []\n",
    "                    for key in inputs[0].keys()\n",
    "                    if key not in [\"prompt\", \"completion\"]\n",
    "                }\n",
    "                for key in reward_kwargs:\n",
    "                    for example in inputs:\n",
    "                        # Repeat each value in the column for `num_generations` times\n",
    "                        reward_kwargs[key].extend([example[key]] * self.num_generations)\n",
    "                output_reward_func = reward_func(\n",
    "                    prompts=prompts, completions=completions, **reward_kwargs\n",
    "                )\n",
    "                rewards_per_func[:, i] = torch.tensor(\n",
    "                    output_reward_func, dtype=torch.float32, device=device\n",
    "                )\n",
    "\n",
    "        # Sum the rewards from all reward functions\n",
    "        rewards = rewards_per_func.sum(dim=1)\n",
    "\n",
    "        # Compute grouped-wise rewards\n",
    "        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n",
    "        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n",
    "\n",
    "        # Normalize the rewards to compute the advantages\n",
    "        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(\n",
    "            self.num_generations, dim=0\n",
    "        )\n",
    "        std_grouped_rewards = std_grouped_rewards.repeat_interleave(\n",
    "            self.num_generations, dim=0\n",
    "        )\n",
    "        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n",
    "\n",
    "        # x - x.detach() allows for preserving gradients from x\n",
    "        per_token_loss = torch.exp(\n",
    "            per_token_logps - per_token_logps.detach()\n",
    "        ) * advantages.unsqueeze(1)\n",
    "        per_token_loss = -(per_token_loss - self.beta * per_token_kl)\n",
    "        loss = (\n",
    "            (per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n",
    "        ).mean()\n",
    "\n",
    "        # Log the metrics\n",
    "        completion_length = (\n",
    "            self.accelerator.gather_for_metrics(completion_mask.sum(1))\n",
    "            .float()\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        self._metrics[\"completion_length\"].append(completion_length)\n",
    "\n",
    "        reward_per_func = self.accelerator.gather_for_metrics(rewards_per_func).mean(0)\n",
    "        for i, reward_func in enumerate(self.reward_funcs):\n",
    "            if isinstance(reward_func, PreTrainedModel):\n",
    "                reward_func_name = reward_func.config._name_or_path.split(\"/\")[-1]\n",
    "            else:\n",
    "                reward_func_name = reward_func.__name__\n",
    "            self._metrics[f\"rewards/{reward_func_name}\"].append(\n",
    "                reward_per_func[i].item()\n",
    "            )\n",
    "\n",
    "        self._metrics[\"reward\"].append(\n",
    "            self.accelerator.gather_for_metrics(rewards).mean().item()\n",
    "        )\n",
    "\n",
    "        self._metrics[\"reward_std\"].append(\n",
    "            self.accelerator.gather_for_metrics(std_grouped_rewards).mean().item()\n",
    "        )\n",
    "\n",
    "        mean_kl = (\n",
    "            (per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n",
    "        ).mean()\n",
    "        self._metrics[\"kl\"].append(\n",
    "            self.accelerator.gather_for_metrics(mean_kl).mean().item()\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SIsFSQYik_vJ"
   },
   "outputs": [],
   "source": [
    "#from .evaluation import run_benchmark_jobs\n",
    "def run_benchmark_jobs(training_args: Union[\"SFTConfig\", \"GRPOConfig\"], model_args: \"ModelConfig\") -> None:\n",
    "    benchmarks = training_args.benchmarks\n",
    "    if len(benchmarks) == 1 and benchmarks[0] == \"all\":\n",
    "        benchmarks = get_lighteval_tasks()\n",
    "        # Evaluate on all supported benchmarks. Later we may want to include a `chat` option\n",
    "        # that just evaluates on `ifeval` and `mt_bench` etc.\n",
    "\n",
    "    for benchmark in benchmarks:\n",
    "        print(f\"Launching benchmark `{benchmark}`\")\n",
    "        if benchmark in get_lighteval_tasks():\n",
    "            run_lighteval_job(benchmark, training_args, model_args)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown benchmark {benchmark}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zGcuLNQyiTOz"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOConfig(trl.GRPOConfig):\n",
    "    \"\"\"\n",
    "    args for callbacks, benchmarks etc\n",
    "    \"\"\"\n",
    "\n",
    "    benchmarks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The benchmarks to run after training.\"},\n",
    "    )\n",
    "    callbacks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The callbacks to run during training.\"},\n",
    "    )\n",
    "    system_prompt: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The optional system prompt to use for benchmarking.\"},\n",
    "    )\n",
    "    hub_model_revision: Optional[str] = field(\n",
    "        default=\"main\", metadata={\"help\": \"The Hub model branch to push the model to.\"}\n",
    "    )\n",
    "    overwrite_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to overwrite the Hub revision.\"}\n",
    "    )\n",
    "    push_to_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to push to a Hub revision/branch.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SFTConfig(trl.SFTConfig):\n",
    "    \"\"\"\n",
    "    args for callbacks, benchmarks etc\n",
    "    \"\"\"\n",
    "\n",
    "    benchmarks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The benchmarks to run after training.\"},\n",
    "    )\n",
    "    callbacks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The callbacks to run during training.\"},\n",
    "    )\n",
    "    system_prompt: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The optional system prompt to use for benchmarking.\"},\n",
    "    )\n",
    "    hub_model_revision: Optional[str] = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The Hub model branch to push the model to.\"},\n",
    "    )\n",
    "    overwrite_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to overwrite the Hub revision.\"}\n",
    "    )\n",
    "    push_to_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to push to a Hub revision/branch.\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4USTS_ADlIA1"
   },
   "outputs": [],
   "source": [
    "#from .hub import push_to_hub_revision\n",
    "def push_to_hub_revision(training_args: SFTConfig | GRPOConfig, extra_ignore_patterns=[]) -> Future:\n",
    "    \"\"\"Pushes the model to branch on a Hub repo.\"\"\"\n",
    "\n",
    "    # Create a repo if it doesn't exist yet\n",
    "    repo_url = create_repo(repo_id=training_args.hub_model_id, private=True, exist_ok=True)\n",
    "    # Get initial commit to branch from\n",
    "    initial_commit = list_repo_commits(training_args.hub_model_id)[-1]\n",
    "    # Now create the branch we'll be pushing to\n",
    "    create_branch(\n",
    "        repo_id=training_args.hub_model_id,\n",
    "        branch=training_args.hub_model_revision,\n",
    "        revision=initial_commit.commit_id,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "    logger.info(f\"Created target repo at {repo_url}\")\n",
    "    logger.info(f\"Pushing to the Hub revision {training_args.hub_model_revision}...\")\n",
    "    ignore_patterns = [\"checkpoint-*\", \"*.pth\"]\n",
    "    ignore_patterns.extend(extra_ignore_patterns)\n",
    "    future = upload_folder(\n",
    "        repo_id=training_args.hub_model_id,\n",
    "        folder_path=training_args.output_dir,\n",
    "        revision=training_args.hub_model_revision,\n",
    "        commit_message=f\"Add {training_args.hub_model_revision} checkpoint\",\n",
    "        ignore_patterns=ignore_patterns,\n",
    "        run_as_future=True,\n",
    "    )\n",
    "    logger.info(f\"Pushed to {repo_url} revision {training_args.hub_model_revision} successfully!\")\n",
    "\n",
    "    return future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_isabelle_snippet(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts Isabelle proof content from text, covering different types of Isabelle snippets,\n",
    "    including multi-line proofs, lemmas, and structured blocks.\n",
    "    \"\"\"\n",
    "    # Improved regex pattern to capture Isabelle code blocks and proof sections\n",
    "    pattern = r\"```isabelle(.*?)qed```\"\n",
    "    matches = re.findall(pattern, text, flags=re.DOTALL)\n",
    "    return \"\\n\".join(matches) if matches else \" NONE\"\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Checks if the model output has the form:\n",
    "       <think>...</think><answer>...</answer>\n",
    "    \"\"\"\n",
    "    pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    \n",
    "    # Print model generated outputs\n",
    "    #for content in completion_contents:\n",
    "        #print(\"\\nMODEL GENERATED OUTPUT:\")\n",
    "        #print(content)\n",
    "        #print(\"-\" * 60)\n",
    "\n",
    "    matches = [re.match(pattern, content) for content in completion_contents]\n",
    "    rewards = [1.0 if match else 0.0 for match in matches]\n",
    "    #print(\"\\nFormat rewards:\", rewards)\n",
    "    return rewards\n",
    "\n",
    "def reasoning_steps_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Checks for multiple steps or structural markers:\n",
    "       - Step 1:, Step 2:\n",
    "       - Numbered lines (e.g., \"1.\", \"2.\" at start)\n",
    "       - Bullet points (\"-\",\"*\")\n",
    "       - Transition words (First, Second, Next, Finally)\n",
    "    \"\"\"\n",
    "    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    \n",
    "    # Print model generated outputs\n",
    "    #for content in completion_contents:\n",
    "        #print(\"\\nMODEL GENERATED OUTPUT:\")\n",
    "        #print(content)\n",
    "        #print(\"-\" * 60)\n",
    "\n",
    "    matches = [len(re.findall(pattern, content)) for content in completion_contents]\n",
    "    # Encourage at least 3 structural markers\n",
    "    rewards = [min(1.0, count / 3) for count in matches]\n",
    "    #print(\"\\nReasoning-steps rewards:\", rewards)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def checker_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Uses the provided `checker` instance to verify model-generated proofs.\n",
    "    Prints out the model's completion text before checking.\n",
    "    Returns a simple binary reward (1.0 if success, 0.0 if failure).\n",
    "    \"\"\"\n",
    "    # Extract the model outputs from the completions\n",
    "    \n",
    "    contents = [extract_isabelle_snippet(c[0][\"content\"]) for c in completions]\n",
    "    #print(contents)\n",
    "    rewards = []\n",
    "\n",
    "    for content in contents:\n",
    "        # Print out the model-generated output\n",
    "        #print(\"\\n[Model Output]:\")\n",
    "        #print(content)\n",
    "        checker = Checker(\n",
    "            working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
    "            isa_path='/home/siai/Isabelle2022',\n",
    "            theory_file_path='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "            port=9000\n",
    "        )\n",
    "\n",
    "        result = checker.check(content)\n",
    "        \n",
    "\n",
    "        # If the checker indicates success, assign a reward of 1.0, otherwise 0.0\n",
    "        if result.get(\"success\", False):\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    #print(\"\\nChecker rewards:\", rewards)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "REWARD_FUNCS_REGISTRY = {\n",
    "    \"format\": format_reward,\n",
    "    \"reasoning_steps\": reasoning_steps_reward,\n",
    "    \"isabelle_verification\": checker_reward,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "q-os0NYmhmP1"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOScriptArguments(ScriptArguments):\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"reasoning_steps\", \"format\", \"isabelle_verification\" ],\n",
    "        metadata={\n",
    "            \"help\": f\"List of reward functions. Possible values: {', '.join(REWARD_FUNCS_REGISTRY.keys())}\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "l1b8IlQxmYS1"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOScriptArguments(ScriptArguments):\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"reasoning_steps\", \"format\", \"isabelle_verification\" ],\n",
    "        metadata={\n",
    "            \"help\": f\"List of reward functions. Possible values: {', '.join(REWARD_FUNCS_REGISTRY.keys())}\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\"\"\"\n",
    "A conversation between User and Assistant. The user provides a mathematical statement, and the Assistant responds with a structured Isabelle proof including any necessary lemmas or sub-lemmas.\n",
    "\n",
    "Follow these rules and format constraints:\n",
    "\n",
    "1) **Chain of Thought**:  \n",
    "   - Enclose your internal reasoning steps in `<think>...</think>`. This represents the Assistant’s thought process or justification sequence.\n",
    "\n",
    "2) **Lemma or Sub-proof Invocation**:  \n",
    "   - When introducing or referencing additional lemmas or sub-lemmas, enclose them in `<invoke>...</invoke>`. For example, `<invoke>lemma helper_lemma</invoke>`.\n",
    "\n",
    "3) **Final Answer**:  \n",
    "   - Enclose the fully fleshed-out proof (in valid Isabelle syntax) in `<answer>...</answer>`. \n",
    "   - MAKE SURE TO ENCLOSE THE ISABELLE CONTENT WITHIN ```isabelle and qed```:\n",
    "\n",
    "     ```isabelle\n",
    "     lemma <lemma_name>:\n",
    "       assumes \"<assumptions>\"\n",
    "       shows \"<goal>\"\n",
    "     proof -\n",
    "       ...\n",
    "     qed\n",
    "     ```\n",
    "\n",
    "4) **User Context**:  \n",
    "   - The user may provide partial solutions or additional context. Incorporate these if relevant, maintaining correctness and coherence.\n",
    "\n",
    "5) **Overall Structure**:  \n",
    "   - You may optionally include a high-level summary in `<reasoning>...</reasoning>`. \n",
    "   - **However**, you must include `<think>...</think>` for your chain-of-thought and `<answer>...</answer>` for your final formal proof. \n",
    "   - If you propose or reference a sub-proof, put it in `<invoke>...</invoke>` blocks.\n",
    "\n",
    "Example Output Skeleton:\n",
    "<reasoning>\n",
    "  [High-level or public explanation of the proof approach]\n",
    "</reasoning>\n",
    "<think>\n",
    "  [Detailed chain-of-thought or reasoning steps]\n",
    "</think>\n",
    "<invoke>\n",
    "  [Additional lemma or sub-proof details]\n",
    "</invoke>\n",
    "<answer>\n",
    "  [Final Isabelle theorem and proof]\n",
    "</answer>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AIgFJH5Omegn"
   },
   "outputs": [],
   "source": [
    "def main(script_args, training_args, model_args):\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "\n",
    "    ###############\n",
    "    # Setup logging\n",
    "    ###############\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process a small summary\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Model parameters {model_args}\")\n",
    "    logger.info(f\"Script parameters {script_args}\")\n",
    "    logger.info(f\"Data parameters {training_args}\")\n",
    "\n",
    "    # Check for last checkpoint\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir):\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint=}.\")\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\n",
    "\n",
    "    # Get reward functions\n",
    "    reward_funcs = [REWARD_FUNCS_REGISTRY[func] for func in script_args.reward_funcs]\n",
    "\n",
    "\n",
    "    # Format into conversation\n",
    "    def make_conversation(example):\n",
    "        return {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": example[\"natural_language_statement\"]},\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(make_conversation)\n",
    "    for split in dataset:\n",
    "        if \"messages\" in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].remove_columns(\"messages\")\n",
    "\n",
    "    logger.info(\"*** Initializing model kwargs ***\")\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "\n",
    "    training_args.gradient_checkpointing = True\n",
    "    model_kwargs = dict(\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        attn_implementation=model_args.attn_implementation,\n",
    "        torch_dtype=torch_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path, load_in_4bit=False, **model_kwargs\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        model_args.model_name_or_path,\n",
    "    )\n",
    "    #############################\n",
    "    # Initialize the GRPO trainer\n",
    "    #############################\n",
    "    trainer = GRPOTrainer(\n",
    "        # model=model_args.model_name_or_path,\n",
    "        model=model,\n",
    "        reward_funcs=reward_funcs,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[script_args.dataset_train_split],\n",
    "        eval_dataset=(\n",
    "            dataset[script_args.dataset_test_split]\n",
    "            if training_args.eval_strategy != \"no\"\n",
    "            else None\n",
    "        ),\n",
    "        callbacks=get_callbacks(training_args, model_args),\n",
    "    )\n",
    "\n",
    "    ###############\n",
    "    # Training loop\n",
    "    ###############\n",
    "    logger.info(\"*** Train ***\")\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    metrics = train_result.metrics\n",
    "    metrics[\"train_samples\"] = len(dataset[script_args.dataset_train_split])\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    ##################################\n",
    "    # Save model and create model card\n",
    "    ##################################\n",
    "    logger.info(\"*** Save model ***\")\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "    logger.info(f\"Model saved to {training_args.output_dir}\")\n",
    "\n",
    "    # Save everything else on main process\n",
    "    kwargs = {\n",
    "        \"dataset_name\": script_args.dataset_name,\n",
    "        \"tags\": [\"OvO-R1\"],\n",
    "    }\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        trainer.create_model_card(**kwargs)\n",
    "        # Restore k,v cache for fast inference\n",
    "        trainer.model.config.use_cache = True\n",
    "        trainer.model.config.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GKC3z82fmlem"
   },
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    \"notebook\",  # sys.argv[0] is the script name in a real execution\n",
    "    \"--model_name_or_path\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"--model_revision\", \"main\",\n",
    "    \"--torch_dtype\", \"bfloat16\",\n",
    "    \"--attn_implementation\", \"flash_attention_2\",\n",
    "\n",
    "    \"--dataset_name\", \"kings-crown/Isabelle_SFT\",\n",
    "    #\"--dataset_configs\", \"train\",\n",
    "    #\"--num_processes\", \"3\",\n",
    "\n",
    "    \"--bf16\", \"true\",\n",
    "    \"--use_vllm\", \"false\",\n",
    "    #\"--vllm_device\", \"auto\",\n",
    "    #\"--vllm_gpu_memory_utilization\", \"0.7\",\n",
    "    \"--do_eval\", \"false\",\n",
    "    \"--eval_strategy\", \"no\",\n",
    "    \"--eval_steps\", \"10\",\n",
    "    \"--gradient_accumulation_steps\", \"4\",\n",
    "    \"--gradient_checkpointing\", \"true\",\n",
    "    \"--gradient_checkpointing_kwargs\", '{\"use_reentrant\": false}',\n",
    "    \"--hub_strategy\", \"every_save\",\n",
    "    \"--learning_rate\", \"3.0e-06\",\n",
    "    \"--log_level\", \"info\",\n",
    "    \"--logging_steps\", \"10\",\n",
    "    \"--logging_strategy\", \"steps\",\n",
    "    \"--lr_scheduler_type\", \"cosine\",\n",
    "    \"--max_prompt_length\", \"256\",\n",
    "    \"--num_generations\", \"2\",\n",
    "    \"--max_completion_length\", \"1024\",\n",
    "    \"--max_steps\", \"-1\",\n",
    "    \"--num_train_epochs\", \"3\",\n",
    "    \"--output_dir\", \"output/OvO-R1_instruct\",\n",
    "    \"--overwrite_output_dir\", \"true\",\n",
    "    \"--per_device_eval_batch_size\", \"1\",\n",
    "    \"--per_device_train_batch_size\", \"2\",\n",
    "    \"--push_to_hub\", \"false\",\n",
    "    \"--report_to\", \"wandb\",\n",
    "    \"--save_strategy\", \"epoch\",\n",
    "    \"--seed\", \"42\",\n",
    "    \"--warmup_ratio\", \"0.1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AoymiaP6mkX6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 18:12:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2 distributed training: True, 16-bits training: False\n",
      "2025-02-17 18:12:38 - INFO - __main__ - Model parameters ModelConfig(model_name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', model_revision='main', torch_dtype='bfloat16', trust_remote_code=False, attn_implementation='flash_attention_2', use_peft=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', use_rslora=False, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)\n",
      "2025-02-17 18:12:38 - INFO - __main__ - Script parameters GRPOScriptArguments(dataset_name='kings-crown/Isabelle_SFT', dataset_config=None, dataset_train_split='train', dataset_test_split='test', gradient_checkpointing_use_reentrant=False, ignore_bias_buffers=False, reward_funcs=['reasoning_steps', 'format', 'isabelle_verification'])\n",
      "2025-02-17 18:12:38 - INFO - __main__ - Data parameters GRPOConfig(\n",
      "_n_gpu=2,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "benchmarks=[],\n",
      "beta=0.04,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "callbacks=[],\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=10.0,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_model_revision=main,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/OvO-R1_instruct/runs/Feb17_18-12-38_siai-4,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_completion_length=1024,\n",
      "max_grad_norm=1.0,\n",
      "max_prompt_length=256,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "model_init_kwargs=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_generations=2,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=output/OvO-R1_instruct,\n",
      "overwrite_hub_revision=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_revision=False,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output/OvO-R1_instruct,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "system_prompt=None,\n",
      "temperature=0.9,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "use_vllm=False,\n",
      "vllm_device=auto,\n",
      "vllm_gpu_memory_utilization=0.9,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 18:12:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 18:12:39 - INFO - datasets.info - Loading Dataset info from /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset isabelle_sft (/home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 18:12:39 - INFO - datasets.builder - Found cached dataset isabelle_sft (/home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 18:12:39 - INFO - datasets.info - Loading Dataset info from /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0/cache-3397b9a863630808.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 18:12:39 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0/cache-3397b9a863630808.arrow\n",
      "2025-02-17 18:12:39 - INFO - __main__ - *** Initializing model kwargs ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:696] 2025-02-17 18:12:39,145 >> loading configuration file config.json from cache at /home/siai/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-17 18:12:39,147 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3904] 2025-02-17 18:12:39,181 >> loading weights file model.safetensors from cache at /home/siai/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa/model.safetensors\n",
      "[INFO|modeling_utils.py:1582] 2025-02-17 18:12:39,189 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[WARNING|logging.py:328] 2025-02-17 18:12:39,191 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "[INFO|configuration_utils.py:1140] 2025-02-17 18:12:39,192 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4888] 2025-02-17 18:12:39,240 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4896] 2025-02-17 18:12:39,240 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-02-17 18:12:39,275 >> loading configuration file generation_config.json from cache at /home/siai/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa/generation_config.json\n",
      "[INFO|configuration_utils.py:1140] 2025-02-17 18:12:39,276 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151646,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2034] 2025-02-17 18:12:39,479 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-17 18:12:39,479 >> loading file tokenizer.json from cache at /home/siai/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-17 18:12:39,480 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-17 18:12:39,480 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-17 18:12:39,480 >> loading file tokenizer_config.json from cache at /home/siai/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-17 18:12:39,480 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-02-17 18:12:39,757 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|trainer.py:741] 2025-02-17 18:12:40,745 >> Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 18:12:41 - INFO - __main__ - *** Train ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2369] 2025-02-17 18:12:41,289 >> ***** Running training *****\n",
      "[INFO|trainer.py:2370] 2025-02-17 18:12:41,290 >>   Num examples = 971\n",
      "[INFO|trainer.py:2371] 2025-02-17 18:12:41,290 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2372] 2025-02-17 18:12:41,291 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2374] 2025-02-17 18:12:41,291 >>   Training with DataParallel so batch size has been adjusted to: 4\n",
      "[INFO|trainer.py:2375] 2025-02-17 18:12:41,291 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2376] 2025-02-17 18:12:41,292 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2377] 2025-02-17 18:12:41,292 >>   Total optimization steps = 180\n",
      "[INFO|trainer.py:2378] 2025-02-17 18:12:41,293 >>   Number of trainable parameters = 1,777,088,000\n",
      "[INFO|integration_utils.py:817] 2025-02-17 18:12:41,295 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbalaji-vir1997\u001b[0m (\u001b[33mbalaji-vir1997-stevens-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/siai/NeSy_T/wandb/run-20250217_181241-w89vfk4c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/w89vfk4c' target=\"_blank\">output/OvO-R1_instruct</a></strong> to <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/w89vfk4c' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/w89vfk4c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:328] 2025-02-17 18:12:42,331 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 53:20:15, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3910] 2025-02-18 12:13:49,687 >> Saving model checkpoint to output/OvO-R1_instruct/checkpoint-61\n",
      "[INFO|configuration_utils.py:420] 2025-02-18 12:13:49,691 >> Configuration saved in output/OvO-R1_instruct/checkpoint-61/config.json\n",
      "[INFO|configuration_utils.py:909] 2025-02-18 12:13:49,692 >> Configuration saved in output/OvO-R1_instruct/checkpoint-61/generation_config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-02-18 12:13:54,454 >> Model weights saved in output/OvO-R1_instruct/checkpoint-61/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-18 12:13:54,465 >> tokenizer config file saved in output/OvO-R1_instruct/checkpoint-61/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-18 12:13:54,466 >> Special tokens file saved in output/OvO-R1_instruct/checkpoint-61/special_tokens_map.json\n",
      "[INFO|trainer.py:3910] 2025-02-19 06:26:31,710 >> Saving model checkpoint to output/OvO-R1_instruct/checkpoint-122\n",
      "[INFO|configuration_utils.py:420] 2025-02-19 06:26:31,713 >> Configuration saved in output/OvO-R1_instruct/checkpoint-122/config.json\n",
      "[INFO|configuration_utils.py:909] 2025-02-19 06:26:31,714 >> Configuration saved in output/OvO-R1_instruct/checkpoint-122/generation_config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-02-19 06:26:36,410 >> Model weights saved in output/OvO-R1_instruct/checkpoint-122/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-19 06:26:36,412 >> tokenizer config file saved in output/OvO-R1_instruct/checkpoint-122/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-19 06:26:36,413 >> Special tokens file saved in output/OvO-R1_instruct/checkpoint-122/special_tokens_map.json\n",
      "[INFO|trainer.py:3910] 2025-02-19 23:49:46,891 >> Saving model checkpoint to output/OvO-R1_instruct/checkpoint-180\n",
      "[INFO|configuration_utils.py:420] 2025-02-19 23:49:46,895 >> Configuration saved in output/OvO-R1_instruct/checkpoint-180/config.json\n",
      "[INFO|configuration_utils.py:909] 2025-02-19 23:49:46,898 >> Configuration saved in output/OvO-R1_instruct/checkpoint-180/generation_config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-02-19 23:49:51,722 >> Model weights saved in output/OvO-R1_instruct/checkpoint-180/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-19 23:49:51,726 >> tokenizer config file saved in output/OvO-R1_instruct/checkpoint-180/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-19 23:49:51,727 >> Special tokens file saved in output/OvO-R1_instruct/checkpoint-180/special_tokens_map.json\n",
      "[INFO|trainer.py:3910] 2025-02-19 23:49:59,832 >> Saving model checkpoint to output/OvO-R1_instruct/checkpoint-180\n",
      "[INFO|configuration_utils.py:420] 2025-02-19 23:49:59,840 >> Configuration saved in output/OvO-R1_instruct/checkpoint-180/config.json\n",
      "[INFO|configuration_utils.py:909] 2025-02-19 23:49:59,842 >> Configuration saved in output/OvO-R1_instruct/checkpoint-180/generation_config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-02-19 23:50:08,463 >> Model weights saved in output/OvO-R1_instruct/checkpoint-180/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-19 23:50:08,469 >> tokenizer config file saved in output/OvO-R1_instruct/checkpoint-180/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-19 23:50:08,472 >> Special tokens file saved in output/OvO-R1_instruct/checkpoint-180/special_tokens_map.json\n",
      "[INFO|trainer.py:2643] 2025-02-19 23:50:20,232 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  total_flos               =                0GF\n",
      "  train_loss               =                0.0\n",
      "  train_runtime            = 2 days, 5:37:38.94\n",
      "  train_samples            =                971\n",
      "  train_samples_per_second =              0.015\n",
      "  train_steps_per_second   =              0.001\n",
      "2025-02-19 23:50:20 - INFO - __main__ - *** Save model ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3910] 2025-02-19 23:50:20,241 >> Saving model checkpoint to output/OvO-R1_instruct\n",
      "[INFO|configuration_utils.py:420] 2025-02-19 23:50:20,243 >> Configuration saved in output/OvO-R1_instruct/config.json\n",
      "[INFO|configuration_utils.py:909] 2025-02-19 23:50:20,244 >> Configuration saved in output/OvO-R1_instruct/generation_config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-02-19 23:50:25,055 >> Model weights saved in output/OvO-R1_instruct/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-02-19 23:50:25,057 >> tokenizer config file saved in output/OvO-R1_instruct/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-02-19 23:50:25,058 >> Special tokens file saved in output/OvO-R1_instruct/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-19 23:50:25 - INFO - __main__ - Model saved to output/OvO-R1_instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:420] 2025-02-19 23:50:25,159 >> Configuration saved in output/OvO-R1_instruct/config.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = TrlParser((GRPOScriptArguments, GRPOConfig, ModelConfig))\n",
    "    script_args, training_args, model_args = parser.parse_args_and_config()\n",
    "    main(script_args, training_args, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
