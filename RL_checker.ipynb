{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ArqgnsfGhgHI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-25 18:02:42,152] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO 02-25 18:02:43 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import textwrap\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import Future\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Callable, Dict, List, Optional, TYPE_CHECKING, Union\n",
    "from unittest.mock import patch\n",
    "import dataset\n",
    "import datasets\n",
    "\n",
    "import trl\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import transformers\n",
    "from accelerate.utils import broadcast_object_list, gather_object\n",
    "from dataclasses import dataclass, field\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    TrainerCallback,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from trl import ModelConfig, ScriptArguments, TrlParser, get_peft_config\n",
    "from trl.data_utils import (\n",
    "    apply_chat_template,\n",
    "    is_conversational,\n",
    "    maybe_apply_chat_template,\n",
    ")\n",
    "from trl.models import unwrap_model_for_generation\n",
    "from trl.trainer import GRPOTrainer\n",
    "from trl.trainer.grpo_config import GRPOConfig\n",
    "from trl.trainer.utils import pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bz7whDjhk4jJ"
   },
   "outputs": [],
   "source": [
    "#from utils.callbacks import get_callbacks\n",
    "def get_callbacks(train_config, model_config) -> List[TrainerCallback]:\n",
    "    callbacks = []\n",
    "    for callback_name in train_config.callbacks:\n",
    "        if callback_name not in CALLBACKS:\n",
    "            raise ValueError(f\"Callback {callback_name} not found in CALLBACKS.\")\n",
    "        callbacks.append(CALLBACKS[callback_name](model_config))\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Checker(object):\n",
    "    \"\"\"A modified version of the Draft, Sketch, Prove proof-checking client.\n",
    "    (https://github.com/albertqjiang/draft_sketch_prove/blob/main/autoformalization/checker.py)\n",
    "\n",
    "    This checker supports Isabelle2022 via the new version of PISA\n",
    "    (https://albertqjiang.github.io/Portal-to-ISAbelle/).\n",
    "\n",
    "    It supports checking a miniF2F-style proof via `check`.\n",
    "\n",
    "    Finally, it replaces `sledgehammer` with a call to `normalhammer`.\n",
    "    \"\"\"\n",
    "    def __init__(self, working_dir, isa_path, theory_file_path, port=9000):\n",
    "        sys.path.append(os.environ.get('PISA_PATH', ''))\n",
    "        try:\n",
    "            from pisa_client import initialise_env\n",
    "            self.initialise_env = initialise_env\n",
    "        except ImportError:\n",
    "            print(\"Set $PISA_PATH to /yourpath/to/Portal-to-ISAbelle/src/main/python\")\n",
    "\n",
    "        self.working_dir = working_dir\n",
    "        self.isa_path = isa_path\n",
    "        self.theory_file_path = theory_file_path\n",
    "        self.port = port\n",
    "\n",
    "    def _initialize(self):\n",
    "        \"\"\"Initialize the PISA environment.\"\"\"\n",
    "        env = self.initialise_env(\n",
    "            self.port,\n",
    "            isa_path=self.isa_path,\n",
    "            theory_file_path=self.theory_file_path,\n",
    "            working_directory=self.working_dir\n",
    "        )\n",
    "        return env\n",
    "\n",
    "    def _exit(self, env):\n",
    "        \"\"\"Exit the environment and clean up resources.\"\"\"\n",
    "        try:\n",
    "            env.post('exit')\n",
    "        except Exception:\n",
    "            pass\n",
    "        os.system(\"ps aux | grep Isabelle | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "        os.system(\"ps aux | grep poly | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "\n",
    "    def _parse_output(self, obs):\n",
    "        \"\"\"Parse the sledgehammer output, returning the relevant part.\"\"\"\n",
    "        return obs.split('<hammer>')[0] if '<hammer>' in obs else ''\n",
    "\n",
    "    def _run_step(self, step, i, tls_name, env):\n",
    "        \"\"\"Run a single proof step.\"\"\"\n",
    "        try:\n",
    "            obs, reward, done, metadata = env.step_to_top_level_state(\n",
    "                action=step,\n",
    "                tls_name=tls_name,\n",
    "                new_name=f'default_{i}'\n",
    "            )\n",
    "            return obs, reward, done, metadata, None\n",
    "        except Exception as e:\n",
    "            return '', 0, False, None, str(e)\n",
    "\n",
    "    def _run_sledgehammer(self, step, i, tls_name, env):\n",
    "        \"\"\"Run sledgehammer or fallback heuristics on a step.\"\"\"\n",
    "        heuristics = [\n",
    "            'by auto', 'by simp', 'by blast', 'by fastforce',\n",
    "            'by force', 'by eval', 'by presburger', 'by sos',\n",
    "            'by arith', 'by linarith', 'by (auto simp: field_simps)'\n",
    "        ]\n",
    "        for heuristic in heuristics:\n",
    "            step_ = step.replace('normalhammer', heuristic)\n",
    "            obs, reward, done, metadata, error = self._run_step(step_, i, tls_name, env)\n",
    "            if error is None:\n",
    "                obs = f'{heuristic} <hammer> {obs}'\n",
    "                return obs, reward, done, metadata, error\n",
    "        return self._run_step(step.replace(\"normalhammer\", \"sledgehammer\"), i, tls_name, env)\n",
    "\n",
    "    def check(self, statement_and_proof):\n",
    "        \"\"\"Check the given proof.\"\"\"\n",
    "        env = self._initialize()\n",
    "        env.initialise()\n",
    "\n",
    "        theory = self.wrap_theorem(statement_and_proof)\n",
    "        steps = self.get_parsed(env, theory)\n",
    "\n",
    "        result = self._check(env, steps)\n",
    "        self._exit(env)\n",
    "\n",
    "        # Output the result\n",
    "        #print(\"\\n==== Success: %s\" % result['success'])\n",
    "        #print(\"--- Complete proof:\\n%s\" % result['theorem_and_proof'])\n",
    "        return result\n",
    "\n",
    "    def _check(self, env, steps):\n",
    "        \"\"\"Run the proof steps and collect results.\"\"\"\n",
    "        success, reason, done = False, '', False\n",
    "        step_results = []\n",
    "        tls_name = 'default'\n",
    "\n",
    "        for i, step in enumerate(steps):\n",
    "            time0 = time.time()\n",
    "            if 'normalhammer' in step or 'sledgehammer' in step:\n",
    "                obs, reward, done, metadata, error = self._run_sledgehammer(step, i, tls_name, env)\n",
    "            else:\n",
    "                obs, reward, done, metadata, error = self._run_step(step, i, tls_name, env)\n",
    "\n",
    "            step_time = time.time() - time0\n",
    "            step_results.append({\n",
    "                'index': i, 'step': step, \n",
    "                'output': self._parse_output(obs), \n",
    "                'step_time': step_time\n",
    "            })\n",
    "\n",
    "            if error:\n",
    "                reason = error\n",
    "                break\n",
    "            tls_name = f'default_{i}'\n",
    "\n",
    "        success = done and reward == 1.0\n",
    "        return {\n",
    "            'success': success,\n",
    "            'reason': reason,\n",
    "            'num_steps': len(steps),\n",
    "            'last_step': len(step_results),\n",
    "            'step_results': step_results,\n",
    "            'theorem_and_proof': self.reconstruct(step_results) if success else ''\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct(step_results):\n",
    "        \"\"\"Reconstruct the complete proof.\"\"\"\n",
    "        return '\\n'.join(\n",
    "            step_result['output'].strip() if step_result['output'] else step_result['step'].strip()\n",
    "            for step_result in step_results[1:]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_theorem(theorem):\n",
    "        \"\"\"Wrap the theorem in a theory file.\"\"\"\n",
    "        return (\n",
    "            'theory Interactive imports HOL.HOL Complex_Main '\n",
    "            '\"HOL-Library.Code_Target_Numeral\" \"HOL-Library.Sum_of_Squares\" '\n",
    "            '\"Symmetric_Polynomials.Vieta\" \"HOL-Computational_Algebra.Computational_Algebra\" '\n",
    "            '\"HOL-Number_Theory.Number_Theory\" \\n begin\\n%s' % theorem\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parsed(env, theory):\n",
    "        \"\"\"Parse the theory and extract proof steps.\"\"\"\n",
    "        raw_steps = env.post(f\"<parse text> ${theory}\")\n",
    "        steps = [s.strip() for s in raw_steps.split('<SEP>') if s.strip() and s != '$']\n",
    "        processed_steps = []\n",
    "        for i, step in enumerate(steps):\n",
    "            if step.lower() == \"then\" and (i == 0 or steps[i - 1].startswith(\"proof\")):\n",
    "                continue\n",
    "            processed_steps.append(step)\n",
    "        return processed_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1UNDdSjJl1vw"
   },
   "outputs": [],
   "source": [
    "#from grpo_trainer import GRPOTrainer\n",
    "\n",
    "\n",
    "RewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]\n",
    "\n",
    "\n",
    "class GRPOTrainer(GRPOTrainer):\n",
    "    # base trl GRPO_trainer\n",
    "    def compute_loss(\n",
    "        self, model, inputs, return_outputs=False, num_items_in_batch=None\n",
    "    ):\n",
    "        if return_outputs:\n",
    "            raise ValueError(\"The GRPOTrainer does not support returning outputs\")\n",
    "\n",
    "        device = self.accelerator.device\n",
    "        prompts = [x[\"prompt\"] for x in inputs]\n",
    "        prompts_text = [\n",
    "            maybe_apply_chat_template(example, self.processing_class)[\"prompt\"]\n",
    "            for example in inputs\n",
    "        ]\n",
    "        prompt_inputs = self.processing_class(\n",
    "            prompts_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            padding_side=\"left\",\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        prompt_inputs = super()._prepare_inputs(prompt_inputs)\n",
    "\n",
    "        if self.max_prompt_length is not None:\n",
    "            prompt_inputs[\"input_ids\"] = prompt_inputs[\"input_ids\"][\n",
    "                :, -self.max_prompt_length :\n",
    "            ]\n",
    "            prompt_inputs[\"attention_mask\"] = prompt_inputs[\"attention_mask\"][\n",
    "                :, -self.max_prompt_length :\n",
    "            ]\n",
    "\n",
    "        # Generate completions using either vLLM or regular generation\n",
    "        if self.args.use_vllm:\n",
    "            # First, have main process load weights if needed\n",
    "            if self.state.global_step != self._last_loaded_step:\n",
    "                with unwrap_model_for_generation(\n",
    "                    model, self.accelerator\n",
    "                ) as unwrapped_model:\n",
    "                    state_dict = unwrapped_model.state_dict()\n",
    "                if self.accelerator.is_main_process:\n",
    "                    llm_model = (\n",
    "                        self.llm.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "                    )\n",
    "                    llm_model.load_weights(state_dict.items())\n",
    "                self._last_loaded_step = self.state.global_step\n",
    "\n",
    "            # Generate completions using vLLM: gather all prompts and use them in a single call in the main process\n",
    "            all_prompts_text = gather_object(prompts_text)\n",
    "            if self.accelerator.is_main_process:\n",
    "                outputs = self.llm.generate(\n",
    "                    all_prompts_text,\n",
    "                    sampling_params=self.sampling_params,\n",
    "                    use_tqdm=False,\n",
    "                )\n",
    "                completion_ids = [\n",
    "                    out.token_ids\n",
    "                    for completions in outputs\n",
    "                    for out in completions.outputs\n",
    "                ]\n",
    "                for output in outputs:\n",
    "                    print(\"-\" * 100)\n",
    "                    print(\"\\n\\n\\n\")\n",
    "                    prompt = output.prompt\n",
    "                    for output_t in output.outputs:\n",
    "                        # print(completion_ids)\n",
    "                        print(\"=\" * 100)\n",
    "                        generated_text = output_t.text\n",
    "                        print(\"【USER】: \", prompt)\n",
    "                        print(\"\\n【ASSISTANT】:\", generated_text)\n",
    "            else:\n",
    "                completion_ids = [None] * len(all_prompts_text) * self.num_generations\n",
    "\n",
    "            # Broadcast the completions from the main process to all processes, ensuring each process receives its\n",
    "            # corresponding slice.\n",
    "            completion_ids = broadcast_object_list(completion_ids, from_process=0)\n",
    "            process_slice = slice(\n",
    "                self.accelerator.process_index * len(prompts) * self.num_generations,\n",
    "                (self.accelerator.process_index + 1)\n",
    "                * len(prompts)\n",
    "                * self.num_generations,\n",
    "            )\n",
    "            completion_ids = completion_ids[process_slice]\n",
    "\n",
    "            # Pad the completions, and concatenate them with the prompts\n",
    "            completion_ids = [\n",
    "                torch.tensor(ids, device=device) for ids in completion_ids\n",
    "            ]\n",
    "            completion_ids = pad(\n",
    "                completion_ids, padding_value=self.processing_class.pad_token_id\n",
    "            )\n",
    "            prompt_inputs_repeated = torch.repeat_interleave(\n",
    "                prompt_inputs[\"input_ids\"], self.num_generations, dim=0\n",
    "            ).to(device)\n",
    "            prompt_completion_ids = torch.cat(\n",
    "                [prompt_inputs_repeated, completion_ids], dim=1\n",
    "            )\n",
    "        else:\n",
    "            # Regular generation path\n",
    "            with unwrap_model_for_generation(\n",
    "                model, self.accelerator\n",
    "            ) as unwrapped_model:\n",
    "                prompt_inputs[\"input_ids\"] = prompt_inputs[\"input_ids\"].to(device)\n",
    "                prompt_inputs[\"attention_mask\"] = prompt_inputs[\"attention_mask\"].to(\n",
    "                    device\n",
    "                )\n",
    "\n",
    "                prompt_completion_ids = unwrapped_model.generate(\n",
    "                    **prompt_inputs, generation_config=self.generation_config\n",
    "                )\n",
    "\n",
    "        prompt_length = prompt_inputs[\"input_ids\"].size(1)\n",
    "        completion_ids = prompt_completion_ids[:, prompt_length:]\n",
    "\n",
    "        # Get the per-token log probabilities for the completions for the model and the reference model\n",
    "        def get_per_token_logps(model, input_ids, num_logits_to_keep):\n",
    "            # We add 1 to `num_logits_to_keep` because the last logits of the sequence is later excluded\n",
    "            logits = model(\n",
    "                input_ids, num_logits_to_keep=num_logits_to_keep + 1\n",
    "            ).logits  # (B, L, V)\n",
    "            logits = logits[\n",
    "                :, :-1, :\n",
    "            ]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred\n",
    "\n",
    "            # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\n",
    "            per_token_logps = []\n",
    "            for logits_row, input_ids_row in zip(\n",
    "                logits, input_ids[:, -num_logits_to_keep:]\n",
    "            ):\n",
    "                log_probs = logits_row.log_softmax(dim=-1)\n",
    "                token_log_prob = torch.gather(\n",
    "                    log_probs, dim=1, index=input_ids_row.unsqueeze(1)\n",
    "                ).squeeze(1)\n",
    "                per_token_logps.append(token_log_prob)\n",
    "            return torch.stack(per_token_logps)\n",
    "\n",
    "        num_logits_to_keep = completion_ids.size(\n",
    "            1\n",
    "        )  # we only need to compute the logits for the completion tokens\n",
    "        per_token_logps = get_per_token_logps(\n",
    "            model, prompt_completion_ids, num_logits_to_keep\n",
    "        )\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            if self.ref_model is not None:\n",
    "                ref_per_token_logps = get_per_token_logps(\n",
    "                    self.ref_model, prompt_completion_ids, num_logits_to_keep\n",
    "                )\n",
    "            else:\n",
    "                with self.accelerator.unwrap_model(model).disable_adapter():\n",
    "                    ref_per_token_logps = get_per_token_logps(\n",
    "                        model, prompt_completion_ids, num_logits_to_keep\n",
    "                    )\n",
    "\n",
    "        # Compute the KL divergence between the model and the reference model\n",
    "        per_token_kl = (\n",
    "            torch.exp(ref_per_token_logps - per_token_logps)\n",
    "            - (ref_per_token_logps - per_token_logps)\n",
    "            - 1\n",
    "        )\n",
    "\n",
    "        # Mask everything after the first EOS token\n",
    "        is_eos = completion_ids == self.processing_class.eos_token_id\n",
    "        eos_idx = torch.full(\n",
    "            (is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device\n",
    "        )\n",
    "        eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\n",
    "        sequence_indices = torch.arange(is_eos.size(1), device=device).expand(\n",
    "            is_eos.size(0), -1\n",
    "        )\n",
    "        completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "        # Decode the generated completions\n",
    "        completions = self.processing_class.batch_decode(\n",
    "            completion_ids, skip_special_tokens=True\n",
    "        )\n",
    "        if is_conversational(inputs[0]):\n",
    "            completions = [\n",
    "                [{\"role\": \"assistant\", \"content\": completion}]\n",
    "                for completion in completions\n",
    "            ]\n",
    "\n",
    "        # Compute the rewards\n",
    "        prompts = [prompt for prompt in prompts for _ in range(self.num_generations)]\n",
    "\n",
    "        rewards_per_func = torch.zeros(\n",
    "            len(prompts), len(self.reward_funcs), device=device\n",
    "        )\n",
    "        for i, (reward_func, reward_processing_class) in enumerate(\n",
    "            zip(self.reward_funcs, self.reward_processing_classes)\n",
    "        ):\n",
    "            if isinstance(reward_func, PreTrainedModel):\n",
    "                if is_conversational(inputs[0]):\n",
    "                    messages = [\n",
    "                        {\"messages\": p + c} for p, c in zip(prompts, completions)\n",
    "                    ]\n",
    "                    texts = [\n",
    "                        apply_chat_template(x, reward_processing_class)[\"text\"]\n",
    "                        for x in messages\n",
    "                    ]\n",
    "                else:\n",
    "                    texts = [p + c for p, c in zip(prompts, completions)]\n",
    "                reward_inputs = reward_processing_class(\n",
    "                    texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    padding_side=\"right\",\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "                reward_inputs = super()._prepare_inputs(reward_inputs)\n",
    "                with torch.inference_mode():\n",
    "                    rewards_per_func[:, i] = reward_func(**reward_inputs).logits[\n",
    "                        :, 0\n",
    "                    ]  # Shape (B*G,)\n",
    "            else:\n",
    "                # Repeat all input columns (but \"prompt\" and \"completion\") to match the number of generations\n",
    "                reward_kwargs = {\n",
    "                    key: []\n",
    "                    for key in inputs[0].keys()\n",
    "                    if key not in [\"prompt\", \"completion\"]\n",
    "                }\n",
    "                for key in reward_kwargs:\n",
    "                    for example in inputs:\n",
    "                        # Repeat each value in the column for `num_generations` times\n",
    "                        reward_kwargs[key].extend([example[key]] * self.num_generations)\n",
    "                output_reward_func = reward_func(\n",
    "                    prompts=prompts, completions=completions, **reward_kwargs\n",
    "                )\n",
    "                rewards_per_func[:, i] = torch.tensor(\n",
    "                    output_reward_func, dtype=torch.float32, device=device\n",
    "                )\n",
    "\n",
    "        # Sum the rewards from all reward functions\n",
    "        rewards = rewards_per_func.sum(dim=1)\n",
    "\n",
    "        # Compute grouped-wise rewards\n",
    "        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n",
    "        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n",
    "\n",
    "        # Normalize the rewards to compute the advantages\n",
    "        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(\n",
    "            self.num_generations, dim=0\n",
    "        )\n",
    "        std_grouped_rewards = std_grouped_rewards.repeat_interleave(\n",
    "            self.num_generations, dim=0\n",
    "        )\n",
    "        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n",
    "\n",
    "        # x - x.detach() allows for preserving gradients from x\n",
    "        per_token_loss = torch.exp(\n",
    "            per_token_logps - per_token_logps.detach()\n",
    "        ) * advantages.unsqueeze(1)\n",
    "        per_token_loss = -(per_token_loss - self.beta * per_token_kl)\n",
    "        loss = (\n",
    "            (per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n",
    "        ).mean()\n",
    "\n",
    "        # Log the metrics\n",
    "        completion_length = (\n",
    "            self.accelerator.gather_for_metrics(completion_mask.sum(1))\n",
    "            .float()\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        self._metrics[\"completion_length\"].append(completion_length)\n",
    "\n",
    "        reward_per_func = self.accelerator.gather_for_metrics(rewards_per_func).mean(0)\n",
    "        for i, reward_func in enumerate(self.reward_funcs):\n",
    "            if isinstance(reward_func, PreTrainedModel):\n",
    "                reward_func_name = reward_func.config._name_or_path.split(\"/\")[-1]\n",
    "            else:\n",
    "                reward_func_name = reward_func.__name__\n",
    "            self._metrics[f\"rewards/{reward_func_name}\"].append(\n",
    "                reward_per_func[i].item()\n",
    "            )\n",
    "\n",
    "        self._metrics[\"reward\"].append(\n",
    "            self.accelerator.gather_for_metrics(rewards).mean().item()\n",
    "        )\n",
    "\n",
    "        self._metrics[\"reward_std\"].append(\n",
    "            self.accelerator.gather_for_metrics(std_grouped_rewards).mean().item()\n",
    "        )\n",
    "\n",
    "        mean_kl = (\n",
    "            (per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n",
    "        ).mean()\n",
    "        self._metrics[\"kl\"].append(\n",
    "            self.accelerator.gather_for_metrics(mean_kl).mean().item()\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SIsFSQYik_vJ"
   },
   "outputs": [],
   "source": [
    "#from .evaluation import run_benchmark_jobs\n",
    "def run_benchmark_jobs(training_args: Union[\"SFTConfig\", \"GRPOConfig\"], model_args: \"ModelConfig\") -> None:\n",
    "    benchmarks = training_args.benchmarks\n",
    "    if len(benchmarks) == 1 and benchmarks[0] == \"all\":\n",
    "        benchmarks = get_lighteval_tasks()\n",
    "        # Evaluate on all supported benchmarks. Later we may want to include a `chat` option\n",
    "        # that just evaluates on `ifeval` and `mt_bench` etc.\n",
    "\n",
    "    for benchmark in benchmarks:\n",
    "        print(f\"Launching benchmark `{benchmark}`\")\n",
    "        if benchmark in get_lighteval_tasks():\n",
    "            run_lighteval_job(benchmark, training_args, model_args)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown benchmark {benchmark}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zGcuLNQyiTOz"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOConfig(trl.GRPOConfig):\n",
    "    \"\"\"\n",
    "    args for callbacks, benchmarks etc\n",
    "    \"\"\"\n",
    "\n",
    "    benchmarks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The benchmarks to run after training.\"},\n",
    "    )\n",
    "    callbacks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The callbacks to run during training.\"},\n",
    "    )\n",
    "    system_prompt: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The optional system prompt to use for benchmarking.\"},\n",
    "    )\n",
    "    hub_model_revision: Optional[str] = field(\n",
    "        default=\"main\", metadata={\"help\": \"The Hub model branch to push the model to.\"}\n",
    "    )\n",
    "    overwrite_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to overwrite the Hub revision.\"}\n",
    "    )\n",
    "    push_to_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to push to a Hub revision/branch.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SFTConfig(trl.SFTConfig):\n",
    "    \"\"\"\n",
    "    args for callbacks, benchmarks etc\n",
    "    \"\"\"\n",
    "\n",
    "    benchmarks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The benchmarks to run after training.\"},\n",
    "    )\n",
    "    callbacks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The callbacks to run during training.\"},\n",
    "    )\n",
    "    system_prompt: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The optional system prompt to use for benchmarking.\"},\n",
    "    )\n",
    "    hub_model_revision: Optional[str] = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The Hub model branch to push the model to.\"},\n",
    "    )\n",
    "    overwrite_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to overwrite the Hub revision.\"}\n",
    "    )\n",
    "    push_to_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to push to a Hub revision/branch.\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4USTS_ADlIA1"
   },
   "outputs": [],
   "source": [
    "#from .hub import push_to_hub_revision\n",
    "def push_to_hub_revision(training_args: SFTConfig | GRPOConfig, extra_ignore_patterns=[]) -> Future:\n",
    "    \"\"\"Pushes the model to branch on a Hub repo.\"\"\"\n",
    "\n",
    "    # Create a repo if it doesn't exist yet\n",
    "    repo_url = create_repo(repo_id=training_args.hub_model_id, private=True, exist_ok=True)\n",
    "    # Get initial commit to branch from\n",
    "    initial_commit = list_repo_commits(training_args.hub_model_id)[-1]\n",
    "    # Now create the branch we'll be pushing to\n",
    "    create_branch(\n",
    "        repo_id=training_args.hub_model_id,\n",
    "        branch=training_args.hub_model_revision,\n",
    "        revision=initial_commit.commit_id,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "    logger.info(f\"Created target repo at {repo_url}\")\n",
    "    logger.info(f\"Pushing to the Hub revision {training_args.hub_model_revision}...\")\n",
    "    ignore_patterns = [\"checkpoint-*\", \"*.pth\"]\n",
    "    ignore_patterns.extend(extra_ignore_patterns)\n",
    "    future = upload_folder(\n",
    "        repo_id=training_args.hub_model_id,\n",
    "        folder_path=training_args.output_dir,\n",
    "        revision=training_args.hub_model_revision,\n",
    "        commit_message=f\"Add {training_args.hub_model_revision} checkpoint\",\n",
    "        ignore_patterns=ignore_patterns,\n",
    "        run_as_future=True,\n",
    "    )\n",
    "    logger.info(f\"Pushed to {repo_url} revision {training_args.hub_model_revision} successfully!\")\n",
    "\n",
    "    return future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_isabelle_snippet(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts Isabelle proof content from a string, covering different types of Isabelle snippets,\n",
    "    including multi-line proofs, lemmas, and structured blocks.\n",
    "    \"\"\"\n",
    "    # Ensure the text is a string and not a list\n",
    "    if isinstance(text, list):\n",
    "        text = text[0]  # Access the first element if it's a list\n",
    "    pattern = r\"```isabelle(.*?)```\"\n",
    "    matches = re.findall(pattern, text, flags=re.DOTALL)\n",
    "    return matches[0] if matches else \" NONE\"\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Checks if the model output has the form:\n",
    "       <think>...</think><answer>...</answer>\n",
    "    \"\"\"\n",
    "    pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    \n",
    "    # Print model generated outputs\n",
    "    #for content in completion_contents:\n",
    "        #print(\"\\nMODEL GENERATED OUTPUT:\")\n",
    "        #print(content)\n",
    "        #print(\"-\" * 60)\n",
    "\n",
    "    matches = [re.match(pattern, content) for content in completion_contents]\n",
    "    rewards = [1.0 if match else 0.0 for match in matches]\n",
    "    #print(\"\\nFormat rewards:\", rewards)\n",
    "    return rewards\n",
    "\n",
    "def reasoning_steps_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Checks for multiple steps or structural markers:\n",
    "       - Step 1:, Step 2:\n",
    "       - Numbered lines (e.g., \"1.\", \"2.\" at start)\n",
    "       - Bullet points (\"-\",\"*\")\n",
    "       - Transition words (First, Second, Next, Finally)\n",
    "    \"\"\"\n",
    "    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    \n",
    "    # Print model generated outputs\n",
    "    #for content in completion_contents:\n",
    "        #print(\"\\nMODEL GENERATED OUTPUT:\")\n",
    "        #print(content)\n",
    "        #print(\"-\" * 60)\n",
    "\n",
    "    matches = [len(re.findall(pattern, content)) for content in completion_contents]\n",
    "    # Encourage at least 3 structural markers\n",
    "    rewards = [min(1.0, count / 3) for count in matches]\n",
    "    #print(\"\\nReasoning-steps rewards:\", rewards)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def checker_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Uses the provided `checker` instance to verify model-generated proofs.\n",
    "    Prints out the model's completion text before checking.\n",
    "    Returns a simple binary reward (1.0 if success, 0.0 if failure).\n",
    "    \"\"\"\n",
    "    # Extract the model outputs from the completions\n",
    "    \n",
    "    contents = [extract_isabelle_snippet(c[0][\"content\"]) for c in completions]\n",
    "    #print(contents)\n",
    "    rewards = []\n",
    "\n",
    "    for content in contents:\n",
    "        # Print out the model-generated output\n",
    "        #print(\"\\n[Model Output]:\")\n",
    "        #print(content)\n",
    "        checker = Checker(\n",
    "            working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
    "            isa_path='/home/siai/Isabelle2022',\n",
    "            theory_file_path='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "            port=9000\n",
    "        )\n",
    "\n",
    "        result = checker.check(content)\n",
    "        \n",
    "\n",
    "        # If the checker indicates success, assign a reward of 1.0, otherwise 0.0\n",
    "        if result.get(\"success\", False):\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    #print(\"\\nChecker rewards:\", rewards)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "REWARD_FUNCS_REGISTRY = {\n",
    "    \"format\": format_reward,\n",
    "    \"reasoning_steps\": reasoning_steps_reward,\n",
    "    \"isabelle_verification\": checker_reward,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "q-os0NYmhmP1"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOScriptArguments(ScriptArguments):\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"reasoning_steps\", \"format\", \"isabelle_verification\" ],\n",
    "        metadata={\n",
    "            \"help\": f\"List of reward functions. Possible values: {', '.join(REWARD_FUNCS_REGISTRY.keys())}\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "l1b8IlQxmYS1"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOScriptArguments(ScriptArguments):\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"reasoning_steps\", \"format\", \"isabelle_verification\" ],\n",
    "        metadata={\n",
    "            \"help\": f\"List of reward functions. Possible values: {', '.join(REWARD_FUNCS_REGISTRY.keys())}\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\"\"\"\n",
    "A conversation between User and Assistant. The user provides a mathematical statement, and the Assistant responds with a structured Isabelle proof including any necessary lemmas or sub-lemmas.\n",
    "\n",
    "Follow these rules and format constraints:\n",
    "\n",
    "1) **Chain of Thought**:  \n",
    "   - Enclose your internal reasoning steps in `<think>...</think>`. This represents the Assistant’s thought process or justification sequence.\n",
    "\n",
    "2) **Lemma or Sub-proof Invocation**:  \n",
    "   - When introducing or referencing additional lemmas or sub-lemmas, enclose them in `<invoke>...</invoke>`. For example, `<invoke>lemma helper_lemma</invoke>`.\n",
    "\n",
    "3) **Final Answer**:  \n",
    "   - Enclose the fully fleshed-out proof (in valid Isabelle syntax) in `<answer>...</answer>`. \n",
    "   - MAKE SURE TO ENCLOSE THE ISABELLE CONTENT WITHIN ```isabelle and qed```:\n",
    "\n",
    "     ```isabelle\n",
    "     lemma <lemma_name>:\n",
    "       assumes \"<assumptions>\"\n",
    "       shows \"<goal>\"\n",
    "     proof -\n",
    "       ...\n",
    "     qed\n",
    "     ```\n",
    "\n",
    "4) **User Context**:  \n",
    "   - The user may provide partial solutions or additional context. Incorporate these if relevant, maintaining correctness and coherence.\n",
    "\n",
    "5) **Overall Structure**:  \n",
    "   - You may optionally include a high-level summary in `<reasoning>...</reasoning>`. \n",
    "   - **However**, you must include `<think>...</think>` for your chain-of-thought and `<answer>...</answer>` for your final formal proof. \n",
    "   - If you propose or reference a sub-proof, put it in `<invoke>...</invoke>` blocks.\n",
    "\n",
    "Example Output Skeleton:\n",
    "<reasoning>\n",
    "  [High-level or public explanation of the proof approach]\n",
    "</reasoning>\n",
    "<think>\n",
    "  [Detailed chain-of-thought or reasoning steps]\n",
    "</think>\n",
    "<invoke>\n",
    "  [Additional lemma or sub-proof details]\n",
    "</invoke>\n",
    "<answer>\n",
    "  [Final Isabelle theorem and proof]\n",
    "</answer>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AIgFJH5Omegn"
   },
   "outputs": [],
   "source": [
    "def main(script_args, training_args, model_args):\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "\n",
    "    ###############\n",
    "    # Setup logging\n",
    "    ###############\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process a small summary\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Model parameters {model_args}\")\n",
    "    logger.info(f\"Script parameters {script_args}\")\n",
    "    logger.info(f\"Data parameters {training_args}\")\n",
    "\n",
    "    # Check for last checkpoint\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir):\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint=}.\")\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\n",
    "\n",
    "    # Get reward functions\n",
    "    reward_funcs = [REWARD_FUNCS_REGISTRY[func] for func in script_args.reward_funcs]\n",
    "\n",
    "\n",
    "    # Format into conversation\n",
    "    def make_conversation(example):\n",
    "        return {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": example[\"natural_language_statement\"]},\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(make_conversation)\n",
    "    for split in dataset:\n",
    "        if \"messages\" in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].remove_columns(\"messages\")\n",
    "\n",
    "    logger.info(\"*** Initializing model kwargs ***\")\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "\n",
    "    training_args.gradient_checkpointing = True\n",
    "    model_kwargs = dict(\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        attn_implementation=model_args.attn_implementation,\n",
    "        torch_dtype=torch_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path, load_in_4bit=False, **model_kwargs\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        model_args.model_name_or_path,\n",
    "    )\n",
    "    #############################\n",
    "    # Initialize the GRPO trainer\n",
    "    #############################\n",
    "    trainer = GRPOTrainer(\n",
    "        # model=model_args.model_name_or_path,\n",
    "        model=model,\n",
    "        reward_funcs=reward_funcs,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[script_args.dataset_train_split],\n",
    "        eval_dataset=(\n",
    "            dataset[script_args.dataset_test_split]\n",
    "            if training_args.eval_strategy != \"no\"\n",
    "            else None\n",
    "        ),\n",
    "        callbacks=get_callbacks(training_args, model_args),\n",
    "    )\n",
    "\n",
    "    ###############\n",
    "    # Training loop\n",
    "    ###############\n",
    "    logger.info(\"*** Train ***\")\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    metrics = train_result.metrics\n",
    "    metrics[\"train_samples\"] = len(dataset[script_args.dataset_train_split])\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    ##################################\n",
    "    # Save model and create model card\n",
    "    ##################################\n",
    "    logger.info(\"*** Save model ***\")\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "    logger.info(f\"Model saved to {training_args.output_dir}\")\n",
    "\n",
    "    # Save everything else on main process\n",
    "    kwargs = {\n",
    "        \"dataset_name\": script_args.dataset_name,\n",
    "        \"tags\": [\"OvO-R1\"],\n",
    "    }\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        trainer.create_model_card(**kwargs)\n",
    "        # Restore k,v cache for fast inference\n",
    "        trainer.model.config.use_cache = True\n",
    "        trainer.model.config.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GKC3z82fmlem"
   },
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    \"notebook\",  # sys.argv[0] is the script name in a real execution\n",
    "    \"--model_name_or_path\", \"kings-crown/Isabelle_mvp_sft\",\n",
    "    \"--model_revision\", \"main\",\n",
    "    \"--torch_dtype\", \"bfloat16\",\n",
    "    \"--attn_implementation\", \"flash_attention_2\",\n",
    "\n",
    "    \"--dataset_name\", \"kings-crown/Isabelle_SFT\",\n",
    "    #\"--dataset_configs\", \"train\",\n",
    "    #\"--num_processes\", \"3\",\n",
    "\n",
    "    \"--bf16\", \"true\",\n",
    "    \"--use_vllm\", \"false\",\n",
    "    \"--vllm_device\", \"auto\",\n",
    "    \"--vllm_gpu_memory_utilization\", \"0.7\",\n",
    "    \"--do_eval\", \"false\",\n",
    "    \"--eval_strategy\", \"no\",\n",
    "    \"--eval_steps\", \"10\",\n",
    "    \"--gradient_accumulation_steps\", \"4\",\n",
    "    \"--gradient_checkpointing\", \"true\",\n",
    "    \"--gradient_checkpointing_kwargs\", '{\"use_reentrant\": false}',\n",
    "    \"--hub_strategy\", \"every_save\",\n",
    "    \"--learning_rate\", \"3.0e-06\",\n",
    "    \"--log_level\", \"info\",\n",
    "    \"--logging_steps\", \"10\",\n",
    "    \"--logging_strategy\", \"steps\",\n",
    "    \"--lr_scheduler_type\", \"cosine\",\n",
    "    \"--max_prompt_length\", \"256\",\n",
    "    \"--num_generations\", \"2\",\n",
    "    \"--max_completion_length\", \"1024\",\n",
    "    \"--max_steps\", \"-1\",\n",
    "    \"--num_train_epochs\", \"3\",\n",
    "    \"--output_dir\", \"output/OvO-R1_instruct\",\n",
    "    \"--overwrite_output_dir\", \"true\",\n",
    "    \"--per_device_eval_batch_size\", \"1\",\n",
    "    \"--per_device_train_batch_size\", \"2\",\n",
    "    \"--push_to_hub\", \"false\",\n",
    "    \"--report_to\", \"wandb\",\n",
    "    \"--save_strategy\", \"epoch\",\n",
    "    \"--seed\", \"42\",\n",
    "    \"--warmup_ratio\", \"0.1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AoymiaP6mkX6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 18:02:44 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2 distributed training: True, 16-bits training: False\n",
      "2025-02-25 18:02:44 - INFO - __main__ - Model parameters ModelConfig(model_name_or_path='kings-crown/Isabelle_mvp_sft', model_revision='main', torch_dtype='bfloat16', trust_remote_code=False, attn_implementation='flash_attention_2', use_peft=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', use_rslora=False, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)\n",
      "2025-02-25 18:02:44 - INFO - __main__ - Script parameters GRPOScriptArguments(dataset_name='kings-crown/Isabelle_SFT', dataset_config=None, dataset_train_split='train', dataset_test_split='test', gradient_checkpointing_use_reentrant=False, ignore_bias_buffers=False, reward_funcs=['reasoning_steps', 'format', 'isabelle_verification'])\n",
      "2025-02-25 18:02:44 - INFO - __main__ - Data parameters GRPOConfig(\n",
      "_n_gpu=2,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "benchmarks=[],\n",
      "beta=0.04,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "callbacks=[],\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=10.0,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_model_revision=main,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/OvO-R1_instruct/runs/Feb25_18-02-44_siai-4,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_completion_length=1024,\n",
      "max_grad_norm=1.0,\n",
      "max_prompt_length=256,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "model_init_kwargs=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_generations=2,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=output/OvO-R1_instruct,\n",
      "overwrite_hub_revision=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_revision=False,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output/OvO-R1_instruct,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "system_prompt=None,\n",
      "temperature=0.9,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "use_vllm=False,\n",
      "vllm_device=auto,\n",
      "vllm_gpu_memory_utilization=0.7,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 18:02:44 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 18:02:44 - INFO - datasets.info - Loading Dataset info from /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset isabelle_sft (/home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 18:02:44 - INFO - datasets.builder - Found cached dataset isabelle_sft (/home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 18:02:44 - INFO - datasets.info - Loading Dataset info from /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0/cache-3397b9a863630808.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 18:02:44 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/siai/.cache/huggingface/datasets/kings-crown___isabelle_sft/default/0.0.0/23f119c2e04ad4362b448479afa3aa27af62c2e0/cache-3397b9a863630808.arrow\n",
      "2025-02-25 18:02:44 - INFO - __main__ - *** Initializing model kwargs ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:696] 2025-02-25 18:02:44,958 >> loading configuration file config.json from cache at /home/siai/.cache/huggingface/hub/models--kings-crown--Isabelle_mvp_sft/snapshots/f257a7e4ecf969a83ba70e7eb05f665af286c175/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-25 18:02:44,961 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"kings-crown/Isabelle_mvp_sft\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151646,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 151654,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"unsloth_version\": \"2025.2.15\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3904] 2025-02-25 18:02:44,996 >> loading weights file model.safetensors from cache at /home/siai/.cache/huggingface/hub/models--kings-crown--Isabelle_mvp_sft/snapshots/f257a7e4ecf969a83ba70e7eb05f665af286c175/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1582] 2025-02-25 18:02:44,997 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[WARNING|logging.py:328] 2025-02-25 18:02:44,999 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "[INFO|configuration_utils.py:1140] 2025-02-25 18:02:45,000 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151646,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"pad_token_id\": 151654,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4627d3545d674061ad3e5b6ccf86f25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4888] 2025-02-25 18:02:46,155 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4896] 2025-02-25 18:02:46,155 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at kings-crown/Isabelle_mvp_sft.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-02-25 18:02:46,182 >> loading configuration file generation_config.json from cache at /home/siai/.cache/huggingface/hub/models--kings-crown--Isabelle_mvp_sft/snapshots/f257a7e4ecf969a83ba70e7eb05f665af286c175/generation_config.json\n",
      "[INFO|configuration_utils.py:1140] 2025-02-25 18:02:46,182 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151646,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_length\": 131072,\n",
      "  \"pad_token_id\": 151654,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kings-crown/Isabelle_mvp_sft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2034] 2025-02-25 18:02:46,792 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-25 18:02:46,793 >> loading file tokenizer.json from cache at /home/siai/.cache/huggingface/hub/models--kings-crown--Isabelle_mvp_sft/snapshots/f257a7e4ecf969a83ba70e7eb05f665af286c175/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-25 18:02:46,793 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-25 18:02:46,794 >> loading file special_tokens_map.json from cache at /home/siai/.cache/huggingface/hub/models--kings-crown--Isabelle_mvp_sft/snapshots/f257a7e4ecf969a83ba70e7eb05f665af286c175/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-25 18:02:46,794 >> loading file tokenizer_config.json from cache at /home/siai/.cache/huggingface/hub/models--kings-crown--Isabelle_mvp_sft/snapshots/f257a7e4ecf969a83ba70e7eb05f665af286c175/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-25 18:02:46,794 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-02-25 18:02:47,046 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|trainer.py:741] 2025-02-25 18:02:48,357 >> Using auto half precision backend\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 113.00 MiB is free. Process 4001509 has 25.43 GiB memory in use. Including non-PyTorch memory, this process has 21.96 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 189.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4009088/3597659164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrlParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRPOScriptArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRPOConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mscript_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args_and_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4009088/834268615.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(script_args, training_args, model_args)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Initialize the GRPO trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m#############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     trainer = GRPOTrainer(\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m# model=model_args.model_name_or_path,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, reward_funcs, args, train_dataset, eval_dataset, processing_class, reward_processing_classes, callbacks, optimizers, peft_config)\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_deepspeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_funcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mprepare_model\u001b[0;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[1;32m   1450\u001b[0m                 )\n\u001b[1;32m   1451\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdevice_placement\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1452\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevaluation_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m             if self.distributed_type in (\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3108\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3109\u001b[0m                 )\n\u001b[0;32m-> 3110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 113.00 MiB is free. Process 4001509 has 25.43 GiB memory in use. Including non-PyTorch memory, this process has 21.96 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 189.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = TrlParser((GRPOScriptArguments, GRPOConfig, ModelConfig))\n",
    "    script_args, training_args, model_args = parser.parse_args_and_config()\n",
    "    main(script_args, training_args, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
