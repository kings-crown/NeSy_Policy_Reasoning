{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import wandb\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import load_dataset, Dataset\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import  Trainer, TrainingArguments\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from trl import (\n",
    "    GRPOTrainer,\n",
    "    ModelConfig as TRLModelConfig,\n",
    "    ScriptArguments,\n",
    "    TrlParser,\n",
    "    get_peft_config,\n",
    "    GRPOConfig\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Next_Steps\n",
    "# normalization and symbolic steps:\n",
    "# from latex2sympy2_extended import NormalizationConfig\n",
    "# from math_verify import LatexExtractionConfig, parse, verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_process_isabelle_input(input_text):\n",
    "    \"\"\"\n",
    "    Cleans and formats Isabelle input, ensuring proper handling of:\n",
    "    - Removes unnecessary escape sequences.\n",
    "    - Correctly retains Isabelle-specific symbols.\n",
    "    - Ensures readability without adding extra characters.\n",
    "    \"\"\"\n",
    "    # Remove unnecessary backslashes from Isabelle symbols\n",
    "    input_text = re.sub(r'\\\\<', r'<', input_text)\n",
    "    input_text = re.sub(r'\\\\>', r'>', input_text)\n",
    "    \n",
    "    # Normalize spaces, remove redundant whitespace\n",
    "    input_text = re.sub(r'\\s+', ' ', input_text).strip()\n",
    "\n",
    "    # Ensure proper formatting for Isabelle constructs\n",
    "    input_text = input_text.replace(\"lemma\", \"\\nlemma\") \\\n",
    "                           .replace(\"Extracted:\", \"\\nExtracted:\\n\") \\\n",
    "                           .replace(\"declare\", \"\\ndeclare\") \\\n",
    "                           .replace(\"deduce\", \"\\ndeduce\")\n",
    "\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Checker(object):\n",
    "    \"\"\"A modified version of the Draft, Sketch, Prove proof-checking client.\n",
    "    (https://github.com/albertqjiang/draft_sketch_prove/blob/main/autoformalization/checker.py)\n",
    "\n",
    "    This checker supports Isabelle2022 via the new version of PISA\n",
    "    (https://albertqjiang.github.io/Portal-to-ISAbelle/).\n",
    "\n",
    "    It supports checking a miniF2F-style proof via `check`.\n",
    "\n",
    "    Finally, it replaces `sledgehammer` with a call to `normalhammer`.\n",
    "    \"\"\"\n",
    "    def __init__(self, working_dir, isa_path, theory_file, port=9000):\n",
    "        sys.path.append(os.environ.get('PISA_PATH', ''))\n",
    "        try:\n",
    "            from pisa_client import initialise_env\n",
    "            self.initialise_env = initialise_env\n",
    "        except ImportError:\n",
    "            print(\"Set $PISA_PATH to /yourpath/to/Portal-to-ISAbelle/src/main/python\")\n",
    "\n",
    "        self.working_dir = working_dir\n",
    "        self.isa_path = isa_path\n",
    "        self.theory_file = theory_file\n",
    "        self.port = port\n",
    "\n",
    "    def _initialize(self):\n",
    "        \"\"\"Initialize the PISA environment.\"\"\"\n",
    "        env = self.initialise_env(\n",
    "            self.port,\n",
    "            isa_path=self.isa_path,\n",
    "            theory_file=self.theory_file,\n",
    "            working_directory=self.working_dir\n",
    "        )\n",
    "        return env\n",
    "\n",
    "    def _exit(self, env):\n",
    "        \"\"\"Exit the environment and clean up resources.\"\"\"\n",
    "        try:\n",
    "            env.post('exit')\n",
    "        except Exception:\n",
    "            pass\n",
    "        os.system(\"ps aux | grep Isabelle | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "        os.system(\"ps aux | grep poly | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "\n",
    "    def _parse_output(self, obs):\n",
    "        \"\"\"Parse the sledgehammer output, returning the relevant part.\"\"\"\n",
    "        return obs.split('<hammer>')[0] if '<hammer>' in obs else ''\n",
    "\n",
    "    def _run_step(self, step, i, tls_name, env):\n",
    "        \"\"\"Run a single proof step.\"\"\"\n",
    "        try:\n",
    "            obs, reward, done, metadata = env.step_to_top_level_state(\n",
    "                action=step,\n",
    "                tls_name=tls_name,\n",
    "                new_name=f'default_{i}'\n",
    "            )\n",
    "            return obs, reward, done, metadata, None\n",
    "        except Exception as e:\n",
    "            return '', 0, False, None, str(e)\n",
    "\n",
    "    def _run_sledgehammer(self, step, i, tls_name, env):\n",
    "        \"\"\"Run sledgehammer or fallback heuristics on a step.\"\"\"\n",
    "        heuristics = [\n",
    "            'by auto', 'by simp', 'by blast', 'by fastforce',\n",
    "            'by force', 'by eval', 'by presburger', 'by sos',\n",
    "            'by arith', 'by linarith', 'by (auto simp: field_simps)'\n",
    "        ]\n",
    "        for heuristic in heuristics:\n",
    "            step_ = step.replace('normalhammer', heuristic)\n",
    "            obs, reward, done, metadata, error = self._run_step(step_, i, tls_name, env)\n",
    "            if error is None:\n",
    "                obs = f'{heuristic} <hammer> {obs}'\n",
    "                return obs, reward, done, metadata, error\n",
    "        return self._run_step(step.replace(\"normalhammer\", \"sledgehammer\"), i, tls_name, env)\n",
    "\n",
    "    def check(self, statement_and_proof):\n",
    "        \"\"\"Check the given proof.\"\"\"\n",
    "        env = self._initialize()\n",
    "        env.initialise()\n",
    "\n",
    "        theory = self.wrap_theorem(statement_and_proof)\n",
    "        steps = self.get_parsed(env, theory)\n",
    "\n",
    "        result = self._check(env, steps)\n",
    "        self._exit(env)\n",
    "\n",
    "        # Output the result\n",
    "        #print(result['success'])\n",
    "        print(\"\\n==== Success: %s\" % result['success'])\n",
    "        print(\"--- Complete proof:\\n%s\" % result['theorem_and_proof'])\n",
    "        return result\n",
    "\n",
    "    def _check(self, env, steps):\n",
    "        \"\"\"Run the proof steps and collect results.\"\"\"\n",
    "        success, reason, done = False, '', False\n",
    "        step_results = []\n",
    "        tls_name = 'default'\n",
    "\n",
    "        for i, step in enumerate(steps):\n",
    "            time0 = time.time()\n",
    "            if 'normalhammer' in step or 'sledgehammer' in step:\n",
    "                obs, reward, done, metadata, error = self._run_sledgehammer(step, i, tls_name, env)\n",
    "            else:\n",
    "                obs, reward, done, metadata, error = self._run_step(step, i, tls_name, env)\n",
    "\n",
    "            step_time = time.time() - time0\n",
    "            step_results.append({\n",
    "                'index': i, 'step': step, \n",
    "                'output': self._parse_output(obs), \n",
    "                'step_time': step_time\n",
    "            })\n",
    "\n",
    "            if error:\n",
    "                reason = error\n",
    "                break\n",
    "            tls_name = f'default_{i}'\n",
    "\n",
    "        success = done and reward == 1.0\n",
    "        return {\n",
    "            'success': success,\n",
    "            'reason': reason,\n",
    "            'num_steps': len(steps),\n",
    "            'last_step': len(step_results),\n",
    "            'step_results': step_results,\n",
    "            'theorem_and_proof': self.reconstruct(step_results) if success else ''\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct(step_results):\n",
    "        \"\"\"Reconstruct the complete proof.\"\"\"\n",
    "        return '\\n'.join(\n",
    "            step_result['output'].strip() if step_result['output'] else step_result['step'].strip()\n",
    "            for step_result in step_results[1:]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_theorem(theorem):\n",
    "        \"\"\"Wrap the theorem in a theory file.\"\"\"\n",
    "        return (\n",
    "            'theory Interactive imports HOL.HOL Complex_Main '\n",
    "            '\"HOL-Library.Code_Target_Numeral\" \"HOL-Library.Sum_of_Squares\" '\n",
    "            '\"Symmetric_Polynomials.Vieta\" \"HOL-Computational_Algebra.Computational_Algebra\" '\n",
    "            '\"HOL-Number_Theory.Number_Theory\" \\n begin\\n%s' % theorem\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parsed(env, theory):\n",
    "        \"\"\"Parse the theory and extract proof steps.\"\"\"\n",
    "        raw_steps = env.post(f\"<parse text> ${theory}\")\n",
    "        steps = [s.strip() for s in raw_steps.split('<SEP>') if s.strip() and s != '$']\n",
    "        processed_steps = []\n",
    "        for i, step in enumerate(steps):\n",
    "            if step.lower() == \"then\" and (i == 0 or steps[i - 1].startswith(\"proof\")):\n",
    "                continue\n",
    "            processed_steps.append(step)\n",
    "        return processed_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ['PISA_PATH'] = '/home/siai/Portal-to-ISAbelle/src/main/python'\n",
    "\n",
    "\n",
    "verifier = Checker(\n",
    "    working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
    "    isa_path='/home/siai/Isabelle2022',\n",
    "    theory_file='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "    port=9000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "initialise_env() got an unexpected keyword argument 'theory_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1997944/3455519503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \"\"\"\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheorem_and_sledgehammer_proof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1997944/1050070636.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, statement_and_proof)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_and_proof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;34m\"\"\"Check the given proof.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1997944/1050070636.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;34m\"\"\"Initialize the PISA environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         env = self.initialise_env(\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0misa_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misa_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: initialise_env() got an unexpected keyword argument 'theory_file'"
     ]
    }
   ],
   "source": [
    "theorem_and_sledgehammer_proof = \"\"\"theorem amc12a_2008_p8:\n",
    "  fixes x y::real\n",
    "  assumes h0: \"0 < x \\<and> 0 < y\"\n",
    "    and h1: \"y^3 = 1\"\n",
    "    and h2: \"6 * x^2 = 2 * (6 * y^2)\"\n",
    "  shows \"x^3 = 2 * sqrt 2\"\n",
    "  using assms\n",
    "  by (smt (verit, best) mult_cancel_left2 one_power2 \n",
    "      power2_eq_square power2_le_imp_le \n",
    "      power2_sum power3_eq_cube power_Suc_less \n",
    "      power_commutes power_gt1_lemma real_le_lsqrt \n",
    "      real_le_rsqrt)\n",
    "\"\"\"\n",
    "\n",
    "result = verifier.check(theorem_and_sledgehammer_proof)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ProofTreeScriptArguments(ScriptArguments):\n",
    "    \"\"\"\n",
    "    Script arguments for training proof trees with reinforcement learning.\n",
    "    \"\"\"\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"local_correctness\"],\n",
    "        metadata={\"help\": \"List of reward functions: 'local_correctness', 'global_correctness', etc.\"},\n",
    "    )\n",
    "    dataset_name: str = field(\n",
    "        default=\"kings-crown/Putnam\",\n",
    "        metadata={\"help\": \"Name or path of the dataset (Hugging Face).\"}\n",
    "    )\n",
    "    dataset_config: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Dataset configuration name (if applicable).\"}\n",
    "    )\n",
    "    dataset_train_split: str = field(\n",
    "        default=\"train\",\n",
    "        metadata={\"help\": \"Name of the training split in the dataset.\"}\n",
    "    )\n",
    "    dataset_test_split: str = field(\n",
    "        default=\"test\",\n",
    "        metadata={\"help\": \"Name of the test split in the dataset.\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    Model configuration.\n",
    "    \"\"\"\n",
    "    model_name_or_path: str = field(\n",
    "        default=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        metadata={\"help\": \"Pretrained model name or path from the HuggingFace hub or local directory.\"}\n",
    "    )\n",
    "    use_peft: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use PEFT for parameter-efficient fine-tuning.\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user provides a mathematical statement, and the Assistant \"\n",
    "    \"generates a structured proof with hierarchical lemmas and steps. Please follow these guidelines:\\n\\n\"\n",
    "    \"1) Encapsulate your reasoning or chain-of-thought in <think> ... </think>\\n\"\n",
    "    \"2) When you propose a lemma or a sub-proof, use <invoke> ... </invoke>\\n\"\n",
    "    \"3) Provide the final statement or conclusion in <answer> ... </answer>\\n\"\n",
    "    \"4) The user might provide context or partial solutions.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(dataset_split, verifier):\n",
    "    \"\"\"\n",
    "    Map each example in the dataset split to a conversation prompt.\n",
    "    Adds fields:\n",
    "      - 'prompt': the constructed conversation as text.\n",
    "      - 'target': the ground-truth Isabelle translation.\n",
    "    \"\"\"\n",
    "    def map_fn(example):\n",
    "        conversation = build_conversation_prompt(example, verifier=verifier)\n",
    "        example[\"prompt\"] = convert_conversation_to_text(conversation)\n",
    "        example[\"target\"] = example[\"isabelle_translation\"]\n",
    "        return example\n",
    "    return dataset_split.map(map_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(example):\n",
    "    \"\"\"\n",
    "    Format (or verify) each example from the dataset.\n",
    "\n",
    "    Expects the example to contain at least:\n",
    "    - 'problem': The main statement or question\n",
    "    - 'context': Additional context or partial solutions\n",
    "    - 'isabelle_translation': A string with the theorem statement in Isabelle\n",
    "    - 'depth_limit': An integer specifying how deep we can expand the proof tree --- Representational dataset with 2 as default\n",
    "    \"\"\"\n",
    "    required_keys = [\"problem\", \"context\", \"isabelle_translation\", \"depth_limit\"]\n",
    "    for key in required_keys:\n",
    "        if key not in example:\n",
    "            raise ValueError(f\"Dataset example missing required field: '{key}'\")\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conversation_prompt(example, verifier=None):\n",
    "    \"\"\"\n",
    "    Build a conversation-style prompt with system/user roles.\n",
    "\n",
    "    We incorporate:\n",
    "      - The SYSTEM_PROMPT,\n",
    "      - The user's 'problem',\n",
    "      - The user's 'context' as well,\n",
    "      - (Optional) A quick verification or summary of the Isabelle statement\n",
    "\n",
    "    Returns a list of conversation turns (dict with role and content).\n",
    "    \"\"\"\n",
    "    conversation = []\n",
    "\n",
    "    # Start with the system prompt\n",
    "    conversation.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "\n",
    "    # The user provides the problem\n",
    "    conversation.append({\"role\": \"user\", \"content\": example[\"problem\"]})\n",
    "\n",
    "    # The user also has some context or partial solutions\n",
    "    if example[\"context\"]:\n",
    "        conversation.append({\"role\": \"user\", \"content\": example[\"context\"]})\n",
    "\n",
    "    # If you want to do an immediate check on the isabelle_translation:\n",
    "    if verifier is not None:\n",
    "        check_result = verifier.check(example[\"isabelle_translation\"])\n",
    "        feedback = (\n",
    "            \"<Verified the Isabelle statement successfully>\"\n",
    "            if check_result[\"success\"]\n",
    "            else f\"<Isabelle check error: {check_result['reason']}>\"\n",
    "        )\n",
    "        # The assistant provides verification feedback\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": feedback})\n",
    "    else:\n",
    "        # Provide a placeholder if no verifier\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": \"<No verification performed>\"} )\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to improve logic, simple version taken from the decompose paper\n",
    "\n",
    "def is_globally_correct(proof_text, verifier):\n",
    "    \"\"\"\n",
    "    Check global correctness by verifying the final proof text with the external Checker.\n",
    "    If verification passes, we return True, else False.\n",
    "    \"\"\"\n",
    "    result = verifier.check(proof_text)\n",
    "    return result[\"success\"]\n",
    "\n",
    "def find_leaf_nodes(text):\n",
    "    \"\"\"\n",
    "    Identify if text is a 'leaf' step. For instance, if there's no <invoke>, we might consider it a leaf.\n",
    "    Return True if no further expansions are indicated.\n",
    "    \"\"\"\n",
    "    # A simple heuristic: if the text does NOT have <invoke> tags, we treat it as a leaf (no further expansions).\n",
    "    return (\"<invoke>\" not in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_correctness_reward(prompts, completions, verifier, **kwargs):\n",
    "    rewards = []\n",
    "    for proof_text in completions:\n",
    "        try:\n",
    "            verified = verifier.check(proof_text)[\"success\"]\n",
    "            rewards.append(1.0 if verified else 0.0)\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def global_correctness_reward(prompts, completions, verifier, **kwargs):\n",
    "    rewards = []\n",
    "    for proof_text in completions:\n",
    "        try:\n",
    "            verified = verifier.check(proof_text)[\"success\"]\n",
    "            has_answer_block = \"<answer>\" in proof_text and \"</answer>\" in proof_text\n",
    "            rewards.append(1.0 if (verified and has_answer_block) else 0.0)\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "VERIFIER = None\n",
    "\n",
    "def local_correctness_reward_wrapper(prompts, completions, **kwargs):\n",
    "    verifier = kwargs.get(\"verifier\", VERIFIER)\n",
    "    if verifier is None:\n",
    "        raise ValueError(\"No verifier provided to the reward function.\")\n",
    "    # Pass prompts, then completions, then verifier\n",
    "    return local_correctness_reward(prompts, completions, verifier, **kwargs)\n",
    "\n",
    "def global_correctness_reward_wrapper(prompts, completions, **kwargs):\n",
    "    verifier = kwargs.get(\"verifier\", VERIFIER)\n",
    "    if verifier is None:\n",
    "        raise ValueError(\"No verifier provided to the reward function.\")\n",
    "    return global_correctness_reward(prompts, completions, verifier, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "reward_funcs_registry = {\n",
    "    \"local_correctness\": local_correctness_reward_wrapper,\n",
    "    \"global_correctness\": global_correctness_reward_wrapper, \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A simple MLP that classifies each sequence embedding as 'correct' vs 'incorrect' (2 classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ValueFunction, self).__init__()\n",
    "        self.linear = torch.nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.linear(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_proof_trees(model, tokenizer, batch, verifier, top_k_percentage=0.3, max_length=256):\n",
    "    \"\"\"\n",
    "    Generate candidate proofs for each example in 'batch'.\n",
    "    We do 1) an 'invoke' version, 2) a 'no_invoke' version, and also incorporate ground-truth mixing.\n",
    "    \"\"\"\n",
    "    proof_trees = []\n",
    "    lengths = [len(ex[\"problem\"].split()) for ex in batch]\n",
    "    sorted_idx = np.argsort(lengths)\n",
    "    top_k_thresh = int(len(batch) * top_k_percentage)\n",
    "    top_k_indices = sorted_idx[-top_k_thresh:]  # highest length => might need sub-lemmas\n",
    "\n",
    "    for i, example in enumerate(batch):\n",
    "        conversation = build_conversation_prompt(example, verifier=verifier)\n",
    "        user_prompt = convert_conversation_to_text(conversation)\n",
    "        if i in top_k_indices:\n",
    "            user_prompt_invoke = user_prompt + \"\\nUse sub-lemmas: <invoke>Propose lemma</invoke>\\n\"\n",
    "        else:\n",
    "            user_prompt_invoke = user_prompt\n",
    "\n",
    "    \n",
    "        proof_text_invoke = run_generation(model, tokenizer, user_prompt_invoke, max_length=max_length)\n",
    "        proof_text_no_invoke = run_generation(model, tokenizer, user_prompt, max_length=max_length)\n",
    "\n",
    "      \n",
    "        single_tree_invoke = [proof_text_invoke]\n",
    "        single_tree_no_invoke = [proof_text_no_invoke]\n",
    "\n",
    "        if find_leaf_nodes(proof_text_invoke) or \"<invoke>\" in proof_text_invoke:\n",
    "            proof_trees.append(single_tree_invoke)\n",
    "        if find_leaf_nodes(proof_text_no_invoke) or \"<invoke>\" in proof_text_no_invoke:\n",
    "            proof_trees.append(single_tree_no_invoke)\n",
    "\n",
    "\n",
    "        ground_truth = example[\"isabelle_translation\"]\n",
    "        gt_tree = [ground_truth]\n",
    "        proof_trees.append(gt_tree)\n",
    "\n",
    "    return proof_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_conversation_to_text(conversation):\n",
    "    \"\"\"\n",
    "    Converts the conversation (list of {role, content}) into a single textual prompt that T5 or GPT can understand.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for turn in conversation:\n",
    "        if turn[\"role\"] == \"system\":\n",
    "            lines.append(f\"[SYSTEM] {turn['content']}\")\n",
    "        elif turn[\"role\"] == \"user\":\n",
    "            lines.append(f\"[USER] {turn['content']}\")\n",
    "        else:  # assistant\n",
    "            lines.append(f\"[ASSISTANT] {turn['content']}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(model, tokenizer, prompt_text, max_length=256):\n",
    "    \"\"\"\n",
    "    Run the model's generate() method on the prompt text and return the decoded string.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True)\n",
    "    gen_output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    decoded = tokenizer.decode(gen_output[0], skip_special_tokens=True)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_rewards(proof_trees, verifier, reward_func):\n",
    "    \"\"\"\n",
    "    Go through each 'proof tree'\n",
    "    \"\"\"\n",
    "    completions = [\"\\n\".join(tree) for tree in proof_trees]\n",
    "    reward_values = reward_func(completions, verifier)\n",
    "    return reward_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(proof_trees, value_function, gamma=0.99):\n",
    "    \"\"\"\n",
    "    For each proof tree, produce a weight that includes discounting and the value function's predictions.\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    for tree in proof_trees:\n",
    "        weight = 1.0\n",
    "        for depth, node_text in enumerate(tree):\n",
    "            dummy_input_vec = torch.rand(1, 768) #dummy for first pass\n",
    "            pred = value_function(dummy_input_vec)  # shape [1,2]\n",
    "            pred_prob = torch.softmax(pred, dim=-1)[0, 1].item()  # Probability of \"correct\"\n",
    "            weight *= (pred_prob * (gamma ** depth))\n",
    "        weights.append(weight)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(proof_trees, rewards, weights):\n",
    "    \"\"\"\n",
    "    Create (prompt, target, weight) examples for each node in each proof tree.\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    for tree, reward, weight in zip(proof_trees, rewards, weights):\n",
    "        target_text = \"\\n\".join(tree)\n",
    "        prompt_text = \"Proof attempt:\"\n",
    "        # Weighted by the final reward * the computed weight\n",
    "        final_weight = reward * weight\n",
    "        example = {\n",
    "            \"prompt\": prompt_text,\n",
    "            \"target\": target_text,\n",
    "            \"weight\": final_weight\n",
    "        }\n",
    "        training_data.append(example)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_reinforce(model, tokenizer, value_function, proof_trees, rewards,\n",
    "                               optimizer, replay_buffer, gamma=0.99):\n",
    "\n",
    "    weights = compute_weights(proof_trees, value_function, gamma=gamma)\n",
    "    training_data = prepare_training_data(proof_trees, rewards, weights)\n",
    "    replay_buffer.extend(training_data)\n",
    "    replay_buffer = replay_buffer[-2000:]\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for example in replay_buffer:\n",
    "        if example[\"weight\"] <= 0:\n",
    "            continue\n",
    "        inputs = tokenizer(example[\"prompt\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            targets = tokenizer(example[\"target\"], return_tensors=\"pt\", truncation=True, padding=True)[\"input_ids\"]\n",
    "\n",
    "        outputs = model(**inputs, labels=targets)\n",
    "        # Weighted loss\n",
    "        loss = outputs.loss * example[\"weight\"]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # reward=1 as label=1, reward=0 as label=0, ignoring partial credit for now.\n",
    "    train_value_function(value_function, proof_trees, rewards)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_value_function(value_function, proof_trees, rewards):\n",
    "    \"\"\"\n",
    "    Simple approach: if the proof tree had reward=1, label all nodes as \"correct\" (class=1);\n",
    "    else label them as \"incorrect\" (class=0).\n",
    "    \"\"\"\n",
    "    optimizer_vf = torch.optim.Adam(value_function.parameters(), lr=1e-4)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    value_function.train()\n",
    "    for tree, reward in zip(proof_trees, rewards):\n",
    "        label = torch.tensor([1 if reward == 1.0 else 0])\n",
    "        for _ in tree:  # each node in the tree\n",
    "            # Dummy embedding\n",
    "            node_vec = torch.rand(1, 768)\n",
    "            pred = value_function(node_vec)  # shape [1,2]\n",
    "            loss = loss_fn(pred, label)\n",
    "            optimizer_vf.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_vf.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(script_args: ProofTreeScriptArguments, training_args, model_args: ModelConfig):\n",
    "\n",
    "    wandb.init(project=\"ProofGeneration\", reinit=True)\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)]\n",
    "    )\n",
    "    logger.setLevel(training_args.get_process_log_level())\n",
    "\n",
    "    # Load and format the dataset.\n",
    "    dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    \n",
    "    # Initialize the Checker for proof verification.\n",
    "    verifier = Checker(\n",
    "        working_dir='/home/balaji/Isabelle2022/src/HOL/Examples',\n",
    "        isa_path='/home/balaji/Isabelle2022',\n",
    "        theory_file='/home/balaji/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "        port=9000\n",
    "    )\n",
    "    global VERIFIER\n",
    "    VERIFIER = verifier\n",
    "\n",
    "    \n",
    "    # Construct training (and evaluation) datasets with conversation prompts.\n",
    "    train_dataset = construct_dataset(dataset[script_args.dataset_train_split], verifier)\n",
    "    eval_dataset = (\n",
    "        construct_dataset(dataset[script_args.dataset_test_split], verifier)\n",
    "        if training_args.eval_strategy != \"no\" else None\n",
    "    )\n",
    "    \n",
    "    # Load the pretrained model and tokenizer.\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Obtain optional PEFT configuration.\n",
    "    peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        lora_dropout=0.05,\n",
    "    )\n",
    "    \n",
    "    # Select reward functions from the registry.\n",
    "    selected_reward_funcs = [\n",
    "        reward_funcs_registry[rf]\n",
    "        for rf in script_args.reward_funcs if rf in reward_funcs_registry\n",
    "    ]\n",
    "    if not selected_reward_funcs:\n",
    "        raise ValueError(\"No valid reward functions selected.\")\n",
    "    reward_funcs = selected_reward_funcs  # GRPOTrainer expects a list.\n",
    "    \n",
    "    # Build GRPO training configuration.\n",
    "    grpo_config = GRPOConfig(\n",
    "            output_dir=\"outputs\",          \n",
    "            run_name=\"my_run\",             \n",
    "            learning_rate=5e-6,\n",
    "            adam_beta1=0.9,\n",
    "            adam_beta2=0.99,\n",
    "            weight_decay=0.1,\n",
    "            warmup_ratio=0.1,\n",
    "            lr_scheduler_type='cosine',\n",
    "            logging_steps=1,\n",
    "            bf16=True,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=2,\n",
    "            num_generations=4,\n",
    "            max_prompt_length=256,\n",
    "            max_completion_length=512,\n",
    "            num_train_epochs=1,\n",
    "            save_steps=100,\n",
    "            max_grad_norm=0.1,\n",
    "            report_to=\"wandb\",\n",
    "            log_on_each_node=False,\n",
    "        )\n",
    "    \n",
    "    # Pass proof tree configuration (using depth_limit from the dataset).\n",
    "    proof_tree_config = {\"max_depth\": dataset[\"train\"][0][\"depth_limit\"]}\n",
    "    \n",
    "    # Initialize GRPOTrainer.\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=reward_funcs,\n",
    "        args=grpo_config,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        peft_config=peft_config,\n",
    "    )\n",
    "    \n",
    "    # Start training.\n",
    "    trainer.train()\n",
    "    trainer.save_model(grpo_config.output_dir)\n",
    "    \n",
    "    # Evaluate and log some generated outputs.\n",
    "    if eval_dataset is not None:\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(\"Evaluation results:\", eval_results)\n",
    "        wandb.log(eval_results)\n",
    "    \n",
    "    for i in range(3):\n",
    "        sample = train_dataset[i]\n",
    "        prompt = sample[\"prompt\"]\n",
    "        generated = run_generation(model, tokenizer, prompt)\n",
    "        print(f\"Sample {i} Prompt:\\n{prompt}\")\n",
    "        print(f\"Sample {i} Generated:\\n{generated}\\n{'-'*40}\")\n",
    "        wandb.log({f\"sample_{i}_prompt\": prompt, f\"sample_{i}_generated\": generated})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Set the required arguments here.\n",
    "sys.argv = [\n",
    "    \"dummy_script.py\",             # Script name (can be any valid string)\n",
    "    \"--output_dir\", \"outputs\",      # Required output directory\n",
    "    \"--seed\", \"42\",                 # Other required or optional arguments\n",
    "    \"--run_name\", \"my_run\",\n",
    "    \"--per_device_train_batch_size\", \"1\",\n",
    "    \"--num_train_epochs\", \"1\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbalaji-vir1997\u001b[0m (\u001b[33mbalaji-vir1997-stevens-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/balaji/Desktop/NeSy_T/wandb/run-20250203_121428-b8zvmkip</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/ProofGeneration/runs/b8zvmkip' target=\"_blank\">youthful-sea-22</a></strong> to <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/ProofGeneration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/ProofGeneration' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/ProofGeneration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/ProofGeneration/runs/b8zvmkip' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/ProofGeneration/runs/b8zvmkip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Success: False\n",
      "--- Complete proof:\n",
      "\n",
      "\n",
      "==== Success: False\n",
      "--- Complete proof:\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m parser \u001b[38;5;241m=\u001b[39m TrlParser((ProofTreeScriptArguments, GRPOConfig, ModelConfig))\n\u001b[1;32m      4\u001b[0m script_args, training_args, model_args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args_and_config()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 98\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(script_args, training_args, model_args)\u001b[0m\n\u001b[1;32m     87\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(\n\u001b[1;32m     88\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     89\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     peft_config\u001b[38;5;241m=\u001b[39mpeft_config,\n\u001b[1;32m     95\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Start training.\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(grpo_config\u001b[38;5;241m.\u001b[39moutput_dir)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Evaluate and log some generated outputs.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3681\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/trl/trainer/grpo_trainer.py:494\u001b[0m, in \u001b[0;36mGRPOTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[1;32m    492\u001b[0m                 \u001b[38;5;66;03m# Repeat each value in the column for `num_generations` times\u001b[39;00m\n\u001b[1;32m    493\u001b[0m                 reward_kwargs[key]\u001b[38;5;241m.\u001b[39mextend([example[key]] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_generations)\n\u001b[0;32m--> 494\u001b[0m         output_reward_func \u001b[38;5;241m=\u001b[39m \u001b[43mreward_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mreward_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m         rewards_per_func[:, i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(output_reward_func, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Sum the rewards from all reward functions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m, in \u001b[0;36mlocal_correctness_reward_wrapper\u001b[0;34m(prompts, completions, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo verifier provided to the reward function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Pass prompts, then completions, then verifier\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlocal_correctness_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mlocal_correctness_reward\u001b[0;34m(prompts, completions, verifier, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proof_text \u001b[38;5;129;01min\u001b[39;00m completions:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m         verified \u001b[38;5;241m=\u001b[39m \u001b[43mverifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproof_text\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m         rewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verified \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 77\u001b[0m, in \u001b[0;36mChecker.check\u001b[0;34m(self, statement_and_proof)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, statement_and_proof):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check the given proof.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     env\u001b[38;5;241m.\u001b[39minitialise()\n\u001b[1;32m     80\u001b[0m     theory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_theorem(statement_and_proof)\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mChecker._initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_initialize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the PISA environment.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialise_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43misa_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misa_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtheory_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheory_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworking_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworking_dir\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "File \u001b[0;32m~/Desktop/Portal-to-ISAbelle/src/main/python/pisa_client.py:26\u001b[0m, in \u001b[0;36minitialise_env\u001b[0;34m(port, isa_path, theory_file, working_directory, debug)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minitialise_env\u001b[39m(port, isa_path, theory_file, working_directory, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPisaEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misa_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43misa_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarter_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheory_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworking_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworking_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Portal-to-ISAbelle/src/main/python/pisa_client.py:46\u001b[0m, in \u001b[0;36mPisaEnv.__init__\u001b[0;34m(self, port, isa_path, starter_string, working_directory, debug)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuccessful_starting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Portal-to-ISAbelle/src/main/python/pisa_client.py:55\u001b[0m, in \u001b[0;36mPisaEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub\u001b[38;5;241m.\u001b[39mInitialiseIsabelle(server_pb2\u001b[38;5;241m.\u001b[39mIsaPath(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misa_path))\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m     54\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub\u001b[38;5;241m.\u001b[39mIsabelleWorkingDirectory(server_pb2\u001b[38;5;241m.\u001b[39mIsaPath(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworking_directory))\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m---> 55\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIsabelleContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIsaContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarter_string\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuccessful_starting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/grpc/_channel.py:1178\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1175\u001b[0m     (\n\u001b[1;32m   1176\u001b[0m         state,\n\u001b[1;32m   1177\u001b[0m         call,\n\u001b[0;32m-> 1178\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/grpc/_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[1;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from trl import TrlParser\n",
    "\n",
    "parser = TrlParser((ProofTreeScriptArguments, GRPOConfig, ModelConfig))\n",
    "script_args, training_args, model_args = parser.parse_args_and_config()\n",
    "main(script_args, training_args, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
