{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ArqgnsfGhgHI"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'latex2sympy2_extended'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 54\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlatex2sympy2_extended\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NormalizationConfig\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath_verify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LatexExtractionConfig, parse, verify\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'latex2sympy2_extended'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import os\n",
    "\n",
    "import textwrap\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable, Optional, Union\n",
    "from unittest.mock import patch\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import transformers\n",
    "from accelerate.utils import broadcast_object_list, gather_object\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    Trainer,\n",
    ")\n",
    "from trl.trainer import GRPOTrainer\n",
    "from trl.data_utils import (\n",
    "    apply_chat_template,\n",
    "    is_conversational,\n",
    "    maybe_apply_chat_template,\n",
    ")\n",
    "from trl.models import unwrap_model_for_generation\n",
    "from trl.trainer.grpo_config import GRPOConfig\n",
    "from trl.trainer.utils import pad\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import set_seed\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import subprocess\n",
    "from typing import TYPE_CHECKING, Dict, Union\n",
    "\n",
    "import logging\n",
    "import re\n",
    "from concurrent.futures import Future\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import re\n",
    "\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "\n",
    "import trl\n",
    "\n",
    "import subprocess\n",
    "from typing import List\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "from trl import ModelConfig, ScriptArguments, TrlParser, get_peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bz7whDjhk4jJ"
   },
   "outputs": [],
   "source": [
    "#from utils.callbacks import get_callbacks\n",
    "def get_callbacks(train_config, model_config) -> List[TrainerCallback]:\n",
    "    callbacks = []\n",
    "    for callback_name in train_config.callbacks:\n",
    "        if callback_name not in CALLBACKS:\n",
    "            raise ValueError(f\"Callback {callback_name} not found in CALLBACKS.\")\n",
    "        callbacks.append(CALLBACKS[callback_name](model_config))\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UNDdSjJl1vw"
   },
   "outputs": [],
   "source": [
    "#from grpo_trainer import GRPOTrainer\n",
    "\n",
    "\n",
    "RewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]\n",
    "\n",
    "\n",
    "class GRPOTrainer(GRPOTrainer):\n",
    "    # base trl GRPO_trainer\n",
    "    def compute_loss(\n",
    "        self, model, inputs, return_outputs=False, num_items_in_batch=None\n",
    "    ):\n",
    "        if return_outputs:\n",
    "            raise ValueError(\"The GRPOTrainer does not support returning outputs\")\n",
    "\n",
    "        device = self.accelerator.device\n",
    "        prompts = [x[\"prompt\"] for x in inputs]\n",
    "        prompts_text = [\n",
    "            maybe_apply_chat_template(example, self.processing_class)[\"prompt\"]\n",
    "            for example in inputs\n",
    "        ]\n",
    "        prompt_inputs = self.processing_class(\n",
    "            prompts_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            padding_side=\"left\",\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        prompt_inputs = super()._prepare_inputs(prompt_inputs)\n",
    "\n",
    "        if self.max_prompt_length is not None:\n",
    "            prompt_inputs[\"input_ids\"] = prompt_inputs[\"input_ids\"][\n",
    "                :, -self.max_prompt_length :\n",
    "            ]\n",
    "            prompt_inputs[\"attention_mask\"] = prompt_inputs[\"attention_mask\"][\n",
    "                :, -self.max_prompt_length :\n",
    "            ]\n",
    "\n",
    "        # Generate completions using either vLLM or regular generation\n",
    "        if self.args.use_vllm:\n",
    "            # First, have main process load weights if needed\n",
    "            if self.state.global_step != self._last_loaded_step:\n",
    "                with unwrap_model_for_generation(\n",
    "                    model, self.accelerator\n",
    "                ) as unwrapped_model:\n",
    "                    state_dict = unwrapped_model.state_dict()\n",
    "                if self.accelerator.is_main_process:\n",
    "                    llm_model = (\n",
    "                        self.llm.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "                    )\n",
    "                    llm_model.load_weights(state_dict.items())\n",
    "                self._last_loaded_step = self.state.global_step\n",
    "\n",
    "            # Generate completions using vLLM: gather all prompts and use them in a single call in the main process\n",
    "            all_prompts_text = gather_object(prompts_text)\n",
    "            if self.accelerator.is_main_process:\n",
    "                outputs = self.llm.generate(\n",
    "                    all_prompts_text,\n",
    "                    sampling_params=self.sampling_params,\n",
    "                    use_tqdm=False,\n",
    "                )\n",
    "                completion_ids = [\n",
    "                    out.token_ids\n",
    "                    for completions in outputs\n",
    "                    for out in completions.outputs\n",
    "                ]\n",
    "                for output in outputs:\n",
    "                    print(\"-\" * 100)\n",
    "                    print(\"\\n\\n\\n\")\n",
    "                    prompt = output.prompt\n",
    "                    for output_t in output.outputs:\n",
    "                        # print(completion_ids)\n",
    "                        print(\"=\" * 100)\n",
    "                        generated_text = output_t.text\n",
    "                        print(\"【USER】: \", prompt)\n",
    "                        print(\"\\n【ASSISTANT】:\", generated_text)\n",
    "            else:\n",
    "                completion_ids = [None] * len(all_prompts_text) * self.num_generations\n",
    "\n",
    "            # Broadcast the completions from the main process to all processes, ensuring each process receives its\n",
    "            # corresponding slice.\n",
    "            completion_ids = broadcast_object_list(completion_ids, from_process=0)\n",
    "            process_slice = slice(\n",
    "                self.accelerator.process_index * len(prompts) * self.num_generations,\n",
    "                (self.accelerator.process_index + 1)\n",
    "                * len(prompts)\n",
    "                * self.num_generations,\n",
    "            )\n",
    "            completion_ids = completion_ids[process_slice]\n",
    "\n",
    "            # Pad the completions, and concatenate them with the prompts\n",
    "            completion_ids = [\n",
    "                torch.tensor(ids, device=device) for ids in completion_ids\n",
    "            ]\n",
    "            completion_ids = pad(\n",
    "                completion_ids, padding_value=self.processing_class.pad_token_id\n",
    "            )\n",
    "            prompt_inputs_repeated = torch.repeat_interleave(\n",
    "                prompt_inputs[\"input_ids\"], self.num_generations, dim=0\n",
    "            ).to(device)\n",
    "            prompt_completion_ids = torch.cat(\n",
    "                [prompt_inputs_repeated, completion_ids], dim=1\n",
    "            )\n",
    "        else:\n",
    "            # Regular generation path\n",
    "            with unwrap_model_for_generation(\n",
    "                model, self.accelerator\n",
    "            ) as unwrapped_model:\n",
    "                prompt_inputs[\"input_ids\"] = prompt_inputs[\"input_ids\"].to(device)\n",
    "                prompt_inputs[\"attention_mask\"] = prompt_inputs[\"attention_mask\"].to(\n",
    "                    device\n",
    "                )\n",
    "\n",
    "                prompt_completion_ids = unwrapped_model.generate(\n",
    "                    **prompt_inputs, generation_config=self.generation_config\n",
    "                )\n",
    "\n",
    "        prompt_length = prompt_inputs[\"input_ids\"].size(1)\n",
    "        completion_ids = prompt_completion_ids[:, prompt_length:]\n",
    "\n",
    "        # Get the per-token log probabilities for the completions for the model and the reference model\n",
    "        def get_per_token_logps(model, input_ids, num_logits_to_keep):\n",
    "            # We add 1 to `num_logits_to_keep` because the last logits of the sequence is later excluded\n",
    "            logits = model(\n",
    "                input_ids, num_logits_to_keep=num_logits_to_keep + 1\n",
    "            ).logits  # (B, L, V)\n",
    "            logits = logits[\n",
    "                :, :-1, :\n",
    "            ]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred\n",
    "\n",
    "            # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\n",
    "            per_token_logps = []\n",
    "            for logits_row, input_ids_row in zip(\n",
    "                logits, input_ids[:, -num_logits_to_keep:]\n",
    "            ):\n",
    "                log_probs = logits_row.log_softmax(dim=-1)\n",
    "                token_log_prob = torch.gather(\n",
    "                    log_probs, dim=1, index=input_ids_row.unsqueeze(1)\n",
    "                ).squeeze(1)\n",
    "                per_token_logps.append(token_log_prob)\n",
    "            return torch.stack(per_token_logps)\n",
    "\n",
    "        num_logits_to_keep = completion_ids.size(\n",
    "            1\n",
    "        )  # we only need to compute the logits for the completion tokens\n",
    "        per_token_logps = get_per_token_logps(\n",
    "            model, prompt_completion_ids, num_logits_to_keep\n",
    "        )\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            if self.ref_model is not None:\n",
    "                ref_per_token_logps = get_per_token_logps(\n",
    "                    self.ref_model, prompt_completion_ids, num_logits_to_keep\n",
    "                )\n",
    "            else:\n",
    "                with self.accelerator.unwrap_model(model).disable_adapter():\n",
    "                    ref_per_token_logps = get_per_token_logps(\n",
    "                        model, prompt_completion_ids, num_logits_to_keep\n",
    "                    )\n",
    "\n",
    "        # Compute the KL divergence between the model and the reference model\n",
    "        per_token_kl = (\n",
    "            torch.exp(ref_per_token_logps - per_token_logps)\n",
    "            - (ref_per_token_logps - per_token_logps)\n",
    "            - 1\n",
    "        )\n",
    "\n",
    "        # Mask everything after the first EOS token\n",
    "        is_eos = completion_ids == self.processing_class.eos_token_id\n",
    "        eos_idx = torch.full(\n",
    "            (is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device\n",
    "        )\n",
    "        eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\n",
    "        sequence_indices = torch.arange(is_eos.size(1), device=device).expand(\n",
    "            is_eos.size(0), -1\n",
    "        )\n",
    "        completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "        # Decode the generated completions\n",
    "        completions = self.processing_class.batch_decode(\n",
    "            completion_ids, skip_special_tokens=True\n",
    "        )\n",
    "        if is_conversational(inputs[0]):\n",
    "            completions = [\n",
    "                [{\"role\": \"assistant\", \"content\": completion}]\n",
    "                for completion in completions\n",
    "            ]\n",
    "\n",
    "        # Compute the rewards\n",
    "        prompts = [prompt for prompt in prompts for _ in range(self.num_generations)]\n",
    "\n",
    "        rewards_per_func = torch.zeros(\n",
    "            len(prompts), len(self.reward_funcs), device=device\n",
    "        )\n",
    "        for i, (reward_func, reward_processing_class) in enumerate(\n",
    "            zip(self.reward_funcs, self.reward_processing_classes)\n",
    "        ):\n",
    "            if isinstance(reward_func, PreTrainedModel):\n",
    "                if is_conversational(inputs[0]):\n",
    "                    messages = [\n",
    "                        {\"messages\": p + c} for p, c in zip(prompts, completions)\n",
    "                    ]\n",
    "                    texts = [\n",
    "                        apply_chat_template(x, reward_processing_class)[\"text\"]\n",
    "                        for x in messages\n",
    "                    ]\n",
    "                else:\n",
    "                    texts = [p + c for p, c in zip(prompts, completions)]\n",
    "                reward_inputs = reward_processing_class(\n",
    "                    texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    padding_side=\"right\",\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "                reward_inputs = super()._prepare_inputs(reward_inputs)\n",
    "                with torch.inference_mode():\n",
    "                    rewards_per_func[:, i] = reward_func(**reward_inputs).logits[\n",
    "                        :, 0\n",
    "                    ]  # Shape (B*G,)\n",
    "            else:\n",
    "                # Repeat all input columns (but \"prompt\" and \"completion\") to match the number of generations\n",
    "                reward_kwargs = {\n",
    "                    key: []\n",
    "                    for key in inputs[0].keys()\n",
    "                    if key not in [\"prompt\", \"completion\"]\n",
    "                }\n",
    "                for key in reward_kwargs:\n",
    "                    for example in inputs:\n",
    "                        # Repeat each value in the column for `num_generations` times\n",
    "                        reward_kwargs[key].extend([example[key]] * self.num_generations)\n",
    "                output_reward_func = reward_func(\n",
    "                    prompts=prompts, completions=completions, **reward_kwargs\n",
    "                )\n",
    "                rewards_per_func[:, i] = torch.tensor(\n",
    "                    output_reward_func, dtype=torch.float32, device=device\n",
    "                )\n",
    "\n",
    "        # Sum the rewards from all reward functions\n",
    "        rewards = rewards_per_func.sum(dim=1)\n",
    "\n",
    "        # Compute grouped-wise rewards\n",
    "        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n",
    "        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n",
    "\n",
    "        # Normalize the rewards to compute the advantages\n",
    "        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(\n",
    "            self.num_generations, dim=0\n",
    "        )\n",
    "        std_grouped_rewards = std_grouped_rewards.repeat_interleave(\n",
    "            self.num_generations, dim=0\n",
    "        )\n",
    "        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n",
    "\n",
    "        # x - x.detach() allows for preserving gradients from x\n",
    "        per_token_loss = torch.exp(\n",
    "            per_token_logps - per_token_logps.detach()\n",
    "        ) * advantages.unsqueeze(1)\n",
    "        per_token_loss = -(per_token_loss - self.beta * per_token_kl)\n",
    "        loss = (\n",
    "            (per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n",
    "        ).mean()\n",
    "\n",
    "        # Log the metrics\n",
    "        completion_length = (\n",
    "            self.accelerator.gather_for_metrics(completion_mask.sum(1))\n",
    "            .float()\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        self._metrics[\"completion_length\"].append(completion_length)\n",
    "\n",
    "        reward_per_func = self.accelerator.gather_for_metrics(rewards_per_func).mean(0)\n",
    "        for i, reward_func in enumerate(self.reward_funcs):\n",
    "            if isinstance(reward_func, PreTrainedModel):\n",
    "                reward_func_name = reward_func.config._name_or_path.split(\"/\")[-1]\n",
    "            else:\n",
    "                reward_func_name = reward_func.__name__\n",
    "            self._metrics[f\"rewards/{reward_func_name}\"].append(\n",
    "                reward_per_func[i].item()\n",
    "            )\n",
    "\n",
    "        self._metrics[\"reward\"].append(\n",
    "            self.accelerator.gather_for_metrics(rewards).mean().item()\n",
    "        )\n",
    "\n",
    "        self._metrics[\"reward_std\"].append(\n",
    "            self.accelerator.gather_for_metrics(std_grouped_rewards).mean().item()\n",
    "        )\n",
    "\n",
    "        mean_kl = (\n",
    "            (per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n",
    "        ).mean()\n",
    "        self._metrics[\"kl\"].append(\n",
    "            self.accelerator.gather_for_metrics(mean_kl).mean().item()\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIsFSQYik_vJ"
   },
   "outputs": [],
   "source": [
    "#from .evaluation import run_benchmark_jobs\n",
    "def run_benchmark_jobs(training_args: Union[\"SFTConfig\", \"GRPOConfig\"], model_args: \"ModelConfig\") -> None:\n",
    "    benchmarks = training_args.benchmarks\n",
    "    if len(benchmarks) == 1 and benchmarks[0] == \"all\":\n",
    "        benchmarks = get_lighteval_tasks()\n",
    "        # Evaluate on all supported benchmarks. Later we may want to include a `chat` option\n",
    "        # that just evaluates on `ifeval` and `mt_bench` etc.\n",
    "\n",
    "    for benchmark in benchmarks:\n",
    "        print(f\"Launching benchmark `{benchmark}`\")\n",
    "        if benchmark in get_lighteval_tasks():\n",
    "            run_lighteval_job(benchmark, training_args, model_args)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown benchmark {benchmark}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGcuLNQyiTOz"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOConfig(trl.GRPOConfig):\n",
    "    \"\"\"\n",
    "    args for callbacks, benchmarks etc\n",
    "    \"\"\"\n",
    "\n",
    "    benchmarks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The benchmarks to run after training.\"},\n",
    "    )\n",
    "    callbacks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The callbacks to run during training.\"},\n",
    "    )\n",
    "    system_prompt: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The optional system prompt to use for benchmarking.\"},\n",
    "    )\n",
    "    hub_model_revision: Optional[str] = field(\n",
    "        default=\"main\", metadata={\"help\": \"The Hub model branch to push the model to.\"}\n",
    "    )\n",
    "    overwrite_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to overwrite the Hub revision.\"}\n",
    "    )\n",
    "    push_to_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to push to a Hub revision/branch.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SFTConfig(trl.SFTConfig):\n",
    "    \"\"\"\n",
    "    args for callbacks, benchmarks etc\n",
    "    \"\"\"\n",
    "\n",
    "    benchmarks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The benchmarks to run after training.\"},\n",
    "    )\n",
    "    callbacks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"The callbacks to run during training.\"},\n",
    "    )\n",
    "    system_prompt: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The optional system prompt to use for benchmarking.\"},\n",
    "    )\n",
    "    hub_model_revision: Optional[str] = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The Hub model branch to push the model to.\"},\n",
    "    )\n",
    "    overwrite_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to overwrite the Hub revision.\"}\n",
    "    )\n",
    "    push_to_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to push to a Hub revision/branch.\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4USTS_ADlIA1"
   },
   "outputs": [],
   "source": [
    "#from .hub import push_to_hub_revision\n",
    "def push_to_hub_revision(training_args: SFTConfig | GRPOConfig, extra_ignore_patterns=[]) -> Future:\n",
    "    \"\"\"Pushes the model to branch on a Hub repo.\"\"\"\n",
    "\n",
    "    # Create a repo if it doesn't exist yet\n",
    "    repo_url = create_repo(repo_id=training_args.hub_model_id, private=True, exist_ok=True)\n",
    "    # Get initial commit to branch from\n",
    "    initial_commit = list_repo_commits(training_args.hub_model_id)[-1]\n",
    "    # Now create the branch we'll be pushing to\n",
    "    create_branch(\n",
    "        repo_id=training_args.hub_model_id,\n",
    "        branch=training_args.hub_model_revision,\n",
    "        revision=initial_commit.commit_id,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "    logger.info(f\"Created target repo at {repo_url}\")\n",
    "    logger.info(f\"Pushing to the Hub revision {training_args.hub_model_revision}...\")\n",
    "    ignore_patterns = [\"checkpoint-*\", \"*.pth\"]\n",
    "    ignore_patterns.extend(extra_ignore_patterns)\n",
    "    future = upload_folder(\n",
    "        repo_id=training_args.hub_model_id,\n",
    "        folder_path=training_args.output_dir,\n",
    "        revision=training_args.hub_model_revision,\n",
    "        commit_message=f\"Add {training_args.hub_model_revision} checkpoint\",\n",
    "        ignore_patterns=ignore_patterns,\n",
    "        run_as_future=True,\n",
    "    )\n",
    "    logger.info(f\"Pushed to {repo_url} revision {training_args.hub_model_revision} successfully!\")\n",
    "\n",
    "    return future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRJND9mSkUbo"
   },
   "outputs": [],
   "source": [
    "#from rewards import REWARD_FUNCS_REGISTRY\n",
    "\n",
    "def accuracy_reward(completions, solution, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    for content, sol in zip(contents, solution):\n",
    "        gold_parsed = parse(\n",
    "            sol,\n",
    "            extraction_mode=\"first_match\",\n",
    "            extraction_config=[LatexExtractionConfig()],\n",
    "        )\n",
    "        if len(gold_parsed) != 0:\n",
    "            # print('latex gold parsed')\n",
    "            # We require the answer to be provided in correct latex (no malformed operators)\n",
    "            answer_parsed = parse(\n",
    "                content,\n",
    "                extraction_config=[\n",
    "                    LatexExtractionConfig(\n",
    "                        normalization_config=NormalizationConfig(\n",
    "                            nits=False,\n",
    "                            malformed_operators=False,\n",
    "                            basic_latex=True,\n",
    "                            equations=True,\n",
    "                            boxed=\"all\",\n",
    "                            units=True,\n",
    "                        ),\n",
    "                        # Ensures that boxed is tried first\n",
    "                        boxed_match_priority=0,\n",
    "                        try_extract_without_anchor=False,\n",
    "                    )\n",
    "                ],\n",
    "                extraction_mode=\"first_match\",\n",
    "            )\n",
    "            # Reward 1 if the content is the same as the ground truth, 0 otherwise\n",
    "            reward = float(verify(answer_parsed, gold_parsed))\n",
    "            # print('\\nprompt:', prompt)\n",
    "            print(\"-\" * 100)\n",
    "            print(\n",
    "                \"\\nanswer_parsed:\",\n",
    "                answer_parsed,\n",
    "                \"\\ngold_parsed:\",\n",
    "                gold_parsed,\n",
    "                \"\\nreward:\",\n",
    "                reward,\n",
    "            )\n",
    "        else:\n",
    "            reward = 1.0\n",
    "            print(\"Failed to parse gold solution: \", sol)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    print(\"\\naccuracy rewards:\", rewards)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, content) for content in completion_contents]\n",
    "\n",
    "    rewards = [1.0 if match else 0.0 for match in matches]\n",
    "    print(\"-\" * 100)\n",
    "    print(\"\\nformat rewards:\", rewards)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def reasoning_steps_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks for clear step-by-step reasoning.\n",
    "    Regex pattern:\n",
    "        Step \\d+: - matches \"Step 1:\", \"Step 2:\", etc.\n",
    "        ^\\d+\\. - matches numbered lists like \"1.\", \"2.\", etc. at start of line\n",
    "        \\n- - matches bullet points with hyphens\n",
    "        \\n\\* - matches bullet points with asterisks\n",
    "        First,|Second,|Next,|Finally, - matches transition words\n",
    "    \"\"\"\n",
    "    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [len(re.findall(pattern, content)) for content in completion_contents]\n",
    "\n",
    "    # Magic nubmer 3 to encourage 3 steps and more, otherwise partial reward\n",
    "    return [min(1.0, count / 3) for count in matches]\n",
    "\n",
    "\n",
    "REWARD_FUNCS_REGISTRY = {\n",
    "    \"accuracy\": accuracy_reward,\n",
    "    \"format\": format_reward,\n",
    "    \"reasoning_steps\": reasoning_steps_reward,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_isabelle_snippet(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts only the Isabelle snippet from the model's generated text,\n",
    "    for example lines between 'theory' and 'end'.\n",
    "    Adjust the regex to match your exact proof format.\n",
    "    \"\"\"\n",
    "    pattern = r\"(theory.*?end)\"\n",
    "    match = re.search(pattern, text, flags=re.DOTALL)\n",
    "    return match.group(1) if match else \"\"\n",
    "\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Checks if the model output has the form:\n",
    "       <think>...</think><answer>...</answer>\n",
    "    \"\"\"\n",
    "    pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    \n",
    "    # Print model generated outputs\n",
    "    for content in completion_contents:\n",
    "        #print(\"\\nMODEL GENERATED OUTPUT:\")\n",
    "        print(content)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    matches = [re.match(pattern, content) for content in completion_contents]\n",
    "    rewards = [1.0 if match else 0.0 for match in matches]\n",
    "    print(\"\\nFormat rewards:\", rewards)\n",
    "    return rewards\n",
    "\n",
    "def reasoning_steps_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Checks for multiple steps or structural markers:\n",
    "       - Step 1:, Step 2:\n",
    "       - Numbered lines (e.g., \"1.\", \"2.\" at start)\n",
    "       - Bullet points (\"-\",\"*\")\n",
    "       - Transition words (First, Second, Next, Finally)\n",
    "    \"\"\"\n",
    "    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    \n",
    "    # Print model generated outputs\n",
    "    for content in completion_contents:\n",
    "        #print(\"\\nMODEL GENERATED OUTPUT:\")\n",
    "        print(content)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    matches = [len(re.findall(pattern, content)) for content in completion_contents]\n",
    "    # Encourage at least 3 structural markers\n",
    "    rewards = [min(1.0, count / 3) for count in matches]\n",
    "    print(\"\\nReasoning-steps rewards:\", rewards)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def checker_reward(completions, checker, **kwargs):\n",
    "    \"\"\"\n",
    "    Uses the provided `checker` instance to verify model-generated proofs.\n",
    "    Prints out the model's completion text before checking.\n",
    "    Returns a simple binary reward (1.0 if success, 0.0 if failure).\n",
    "    \"\"\"\n",
    "    # Extract the model outputs from the completions\n",
    "\n",
    "    contents = [extract_isabelle_snippet(c[0][\"content\"]) for c in completions]\n",
    "    rewards = []\n",
    "\n",
    "    for content in contents:\n",
    "        # Print out the model-generated output\n",
    "        print(\"\\n[Model Output]:\")\n",
    "        print(content)\n",
    "\n",
    "        result = checker.check(content)\n",
    "\n",
    "        # If the checker indicates success, assign a reward of 1.0, otherwise 0.0\n",
    "        if result.get(\"success\", False):\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    print(\"\\nChecker rewards:\", rewards)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "REWARD_FUNCS_REGISTRY = {\n",
    "    \"format\": format_reward,\n",
    "    \"reasoning_steps\": reasoning_steps_reward,\n",
    "    \"isabelle_verification\": checker_reward,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-os0NYmhmP1"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOScriptArguments(ScriptArguments):\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"reasoning_steps\", \"format\", \"isabelle_verification\" ],\n",
    "        metadata={\n",
    "            \"help\": f\"List of reward functions. Possible values: {', '.join(REWARD_FUNCS_REGISTRY.keys())}\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1b8IlQxmYS1"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOScriptArguments(ScriptArguments):\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"reasoning_steps\", \"format\", \"isabelle_verification\" ],\n",
    "        metadata={\n",
    "            \"help\": f\"List of reward functions. Possible values: {', '.join(REWARD_FUNCS_REGISTRY.keys())}\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\"\"\"\n",
    "A conversation between User and Assistant. The user provides a mathematical statement, and the Assistant responds with a structured Isabelle proof including any necessary lemmas or sub-lemmas.\n",
    "\n",
    "Follow these rules and format constraints:\n",
    "\n",
    "1) **Chain of Thought**:  \n",
    "   - Enclose your internal reasoning steps in `<think>...</think>`. This represents the Assistant’s thought process or justification sequence.\n",
    "\n",
    "2) **Lemma or Sub-proof Invocation**:  \n",
    "   - When introducing or referencing additional lemmas or sub-lemmas, enclose them in `<invoke>...</invoke>`. For example, `<invoke>lemma helper_lemma</invoke>`.\n",
    "\n",
    "3) **Final Answer**:  \n",
    "   - Enclose the fully fleshed-out proof (in valid Isabelle syntax) in `<answer>...</answer>`. \n",
    "   - Make sure it follows a structure like:\n",
    "\n",
    "     ```isabelle\n",
    "     lemma <lemma_name>:\n",
    "       assumes \"<assumptions>\"\n",
    "       shows \"<goal>\"\n",
    "     proof -\n",
    "       ...\n",
    "     qed\n",
    "     ```\n",
    "\n",
    "4) **User Context**:  \n",
    "   - The user may provide partial solutions or additional context. Incorporate these if relevant, maintaining correctness and coherence.\n",
    "\n",
    "5) **Overall Structure**:  \n",
    "   - You may optionally include a high-level summary in `<reasoning>...</reasoning>`. \n",
    "   - **However**, you must include `<think>...</think>` for your chain-of-thought and `<answer>...</answer>` for your final formal proof. \n",
    "   - If you propose or reference a sub-proof, put it in `<invoke>...</invoke>` blocks.\n",
    "\n",
    "Example Output Skeleton:\n",
    "<reasoning>\n",
    "  [High-level or public explanation of the proof approach]\n",
    "</reasoning>\n",
    "<think>\n",
    "  [Detailed chain-of-thought or reasoning steps]\n",
    "</think>\n",
    "<invoke>\n",
    "  [Additional lemma or sub-proof details]\n",
    "</invoke>\n",
    "<answer>\n",
    "  [Final Isabelle theorem and proof]\n",
    "</answer>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIgFJH5Omegn"
   },
   "outputs": [],
   "source": [
    "def main(script_args, training_args, model_args):\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    ###############\n",
    "    # Setup logging\n",
    "    ###############\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process a small summary\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Model parameters {model_args}\")\n",
    "    logger.info(f\"Script parameters {script_args}\")\n",
    "    logger.info(f\"Data parameters {training_args}\")\n",
    "\n",
    "    # Check for last checkpoint\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir):\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint=}.\")\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\n",
    "\n",
    "    # Get reward functions\n",
    "    reward_funcs = [REWARD_FUNCS_REGISTRY[func] for func in script_args.reward_funcs]\n",
    "\n",
    "\n",
    "    # Format into conversation\n",
    "    def make_conversation(example):\n",
    "        return {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(make_conversation)\n",
    "    for split in dataset:\n",
    "        if \"messages\" in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].remove_columns(\"messages\")\n",
    "\n",
    "    logger.info(\"*** Initializing model kwargs ***\")\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "\n",
    "    training_args.gradient_checkpointing = True\n",
    "    model_kwargs = dict(\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        attn_implementation=model_args.attn_implementation,\n",
    "        torch_dtype=torch_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path, load_in_4bit=False, **model_kwargs\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        model_args.model_name_or_path,\n",
    "    )\n",
    "    #############################\n",
    "    # Initialize the GRPO trainer\n",
    "    #############################\n",
    "    trainer = GRPOTrainer(\n",
    "        # model=model_args.model_name_or_path,\n",
    "        model=model,\n",
    "        reward_funcs=reward_funcs,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[script_args.dataset_train_split],\n",
    "        eval_dataset=(\n",
    "            dataset[script_args.dataset_test_split]\n",
    "            if training_args.eval_strategy != \"no\"\n",
    "            else None\n",
    "        ),\n",
    "        callbacks=get_callbacks(training_args, model_args),\n",
    "    )\n",
    "\n",
    "    ###############\n",
    "    # Training loop\n",
    "    ###############\n",
    "    logger.info(\"*** Train ***\")\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    metrics = train_result.metrics\n",
    "    metrics[\"train_samples\"] = len(dataset[script_args.dataset_train_split])\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    ##################################\n",
    "    # Save model and create model card\n",
    "    ##################################\n",
    "    logger.info(\"*** Save model ***\")\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "    logger.info(f\"Model saved to {training_args.output_dir}\")\n",
    "\n",
    "    # Save everything else on main process\n",
    "    kwargs = {\n",
    "        \"dataset_name\": script_args.dataset_name,\n",
    "        \"tags\": [\"OvO-R1\"],\n",
    "    }\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        trainer.create_model_card(**kwargs)\n",
    "        # Restore k,v cache for fast inference\n",
    "        trainer.model.config.use_cache = True\n",
    "        trainer.model.config.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKC3z82fmlem"
   },
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    \"notebook\",  # sys.argv[0] is the script name in a real execution\n",
    "    \"--model_name_or_path\", \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"--model_revision\", \"main\",\n",
    "    \"--torch_dtype\", \"bfloat16\",\n",
    "    \"--attn_implementation\", \"eager\",\n",
    "\n",
    "    \"--dataset_name\", \"xiaodongguaAIGC/X-R1-750\",\n",
    "    #\"--dataset_configs\", \"train\",\n",
    "    #\"--num_processes\", \"3\",\n",
    "\n",
    "    \"--bf16\", \"true\",\n",
    "    \"--use_vllm\", \"false\",\n",
    "    #\"--vllm_device\", \"auto\",\n",
    "    #\"--vllm_gpu_memory_utilization\", \"0.7\",\n",
    "    \"--do_eval\", \"false\",\n",
    "    \"--eval_strategy\", \"no\",\n",
    "    \"--eval_steps\", \"10\",\n",
    "    \"--gradient_accumulation_steps\", \"4\",\n",
    "    \"--gradient_checkpointing\", \"true\",\n",
    "    \"--gradient_checkpointing_kwargs\", '{\"use_reentrant\": false}',\n",
    "    \"--hub_strategy\", \"every_save\",\n",
    "    \"--learning_rate\", \"3.0e-06\",\n",
    "    \"--log_level\", \"info\",\n",
    "    \"--logging_steps\", \"10\",\n",
    "    \"--logging_strategy\", \"steps\",\n",
    "    \"--lr_scheduler_type\", \"cosine\",\n",
    "    \"--max_prompt_length\", \"256\",\n",
    "    \"--num_generations\", \"2\",\n",
    "    \"--max_completion_length\", \"1024\",\n",
    "    \"--max_steps\", \"-1\",\n",
    "    \"--num_train_epochs\", \"3\",\n",
    "    \"--output_dir\", \"output/OvO-R1_instruct\",\n",
    "    \"--overwrite_output_dir\", \"true\",\n",
    "    \"--per_device_eval_batch_size\", \"1\",\n",
    "    \"--per_device_train_batch_size\", \"2\",\n",
    "    \"--push_to_hub\", \"false\",\n",
    "    \"--report_to\", \"wandb\",\n",
    "    \"--save_strategy\", \"epoch\",\n",
    "    \"--seed\", \"42\",\n",
    "    \"--warmup_ratio\", \"0.1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoymiaP6mkX6"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = TrlParser((GRPOScriptArguments, GRPOConfig, ModelConfig))\n",
    "    script_args, training_args, model_args = parser.parse_args_and_config()\n",
    "    main(script_args, training_args, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHeuieMa76FA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
