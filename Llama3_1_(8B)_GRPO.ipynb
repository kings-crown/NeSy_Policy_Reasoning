{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ5x6xlUqG4N"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxKlbfLhqG4P"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf7gq1YJqG4Q"
      },
      "source": [
        "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xpcb5j3qG4Q"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xnm_FUyqG4S"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1zyu9Ug2XEt"
      },
      "source": [
        "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59DIs5BMcvjN",
        "outputId": "a4b3de70-c99c-4e76-ee06-dab6a6505a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/siai/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-27 14:24:19,589\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8-SLRUB2gwM"
      },
      "source": [
        "Load up `Llama 3.1 8B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700,
          "referenced_widgets": [
            "d8d0dca36cfc47f0919924da07c231e8",
            "5f3d96b613e94e9984d4599ca9ca7b17",
            "66c3271554b1455eb56be55c9241e45e",
            "d36b61cf796c429080e93ea838a3759e",
            "94873c3c077e483790b34f95c421f484",
            "ea549fffa8c2469888d1668158bc105c",
            "98b432b98839428f85d91580c21e80e2",
            "fee4f852c9744a07b909e586e3615604",
            "3febcf8a8eca40c28aafc697f3ec8776",
            "b4e1eb8eeb064c88a2142e474fb8327f",
            "da10502506f9448c9de94f1ddd84d3b1",
            "e6cc388e78c14abfaa49d2be6fa1b5d9",
            "769bde36e2ba4434bddd78e7d5911be4",
            "3c522d78b1834068bd4b155d0f87a4d7",
            "a23afba19c2a4d3a90d771fc55f8d490",
            "6221f0be3b8d48e797c873565a216680",
            "1ac03aff5c314b00ac938c80eb7b2f8a",
            "88c63d94a05a42c49d5f8958a27987a6",
            "0ca67b0c4ca64eb788358a51308f6b97",
            "83c3c811923a4642aba156d1215b39d2",
            "e863bf099e064da7b482c21fe7b77de7",
            "697faad6643a43aca98015da4faef186"
          ]
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "514dea04-804e-47a8-b891-ed3f4a6fb530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-27 14:24:22 __init__.py:207] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading kings-crown/Isabelle_FVELer_SFT with actual GPU utilization = 79.55%\n",
            "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 47.54 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 288.\n",
            "Unsloth: vLLM's KV Cache can use up to 23.03 GB. Also swap space = 6 GB.\n",
            "INFO 02-27 14:24:27 config.py:549] This model supports multiple tasks: {'classify', 'embed', 'generate', 'score', 'reward'}. Defaulting to 'generate'.\n",
            "INFO 02-27 14:24:27 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='kings-crown/Isabelle_FVELer_SFT', speculative_config=None, tokenizer='kings-crown/Isabelle_FVELer_SFT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=kings-crown/Isabelle_FVELer_SFT, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":288}, use_cached_outputs=False, \n",
            "INFO 02-27 14:24:28 cuda.py:229] Using Flash Attention backend.\n",
            "INFO 02-27 14:24:28 model_runner.py:1110] Starting to load model kings-crown/Isabelle_FVELer_SFT...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W227 14:24:28.366179486 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-27 14:24:28 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
            "INFO 02-27 14:26:45 weight_utils.py:270] Time spent downloading weights for kings-crown/Isabelle_FVELer_SFT: 136.298619 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.33it/s]\n",
            "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  3.28it/s]\n",
            "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.58it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.14it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.32it/s]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-27 14:26:47 model_runner.py:1115] Loading model weights took 14.3854 GB\n",
            "INFO 02-27 14:26:47 punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 02-27 14:26:48 worker.py:267] Memory profiling takes 1.33 seconds\n",
            "INFO 02-27 14:26:48 worker.py:267] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.80) = 37.81GiB\n",
            "INFO 02-27 14:26:48 worker.py:267] model weights take 14.39GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 21.79GiB.\n",
            "INFO 02-27 14:26:49 executor_base.py:111] # cuda blocks: 25500, # CPU blocks: 7021\n",
            "INFO 02-27 14:26:49 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 199.22x\n",
            "INFO 02-27 14:26:53 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:21<00:00,  1.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-27 14:27:14 model_runner.py:1562] Graph capturing finished in 21 secs, took 2.12 GiB\n",
            "INFO 02-27 14:27:14 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 27.09 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Unsloth 2025.2.15 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "import time\n",
        "\n",
        "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
        "lora_rank = 128 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"kings-crown/Isabelle_FVELer_SFT\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.8, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_isabelle_snippet(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts Isabelle proof content from a string, covering different types of Isabelle snippets,\n",
        "    including multi-line proofs, lemmas, and structured blocks.\n",
        "    \"\"\"\n",
        "    # Ensure the text is a string and not a list\n",
        "    if isinstance(text, list):\n",
        "        text = text[0]  # Access the first element if it's a list\n",
        "    pattern = r\"```isabelle(.*?)```\"\n",
        "    matches = re.findall(pattern, text, flags=re.DOTALL)\n",
        "    return matches[0] if matches else \" NONE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Checker(object):\n",
        "    \"\"\"A modified version of the Draft, Sketch, Prove proof-checking client.\n",
        "    (https://github.com/albertqjiang/draft_sketch_prove/blob/main/autoformalization/checker.py)\n",
        "\n",
        "    This checker supports Isabelle2022 via the new version of PISA\n",
        "    (https://albertqjiang.github.io/Portal-to-ISAbelle/).\n",
        "\n",
        "    It supports checking a miniF2F-style proof via `check`.\n",
        "\n",
        "    Finally, it replaces `sledgehammer` with a call to `normalhammer`.\n",
        "    \"\"\"\n",
        "    def __init__(self, working_dir, isa_path, theory_file_path, port=9000):\n",
        "        sys.path.append(os.environ.get('PISA_PATH', ''))\n",
        "        try:\n",
        "            from pisa_client import initialise_env\n",
        "            self.initialise_env = initialise_env\n",
        "        except ImportError:\n",
        "            print(\"Set $PISA_PATH to /yourpath/to/Portal-to-ISAbelle/src/main/python\")\n",
        "\n",
        "        self.working_dir = working_dir\n",
        "        self.isa_path = isa_path\n",
        "        self.theory_file_path = theory_file_path\n",
        "        self.port = port\n",
        "\n",
        "    def _initialize(self):\n",
        "        \"\"\"Initialize the PISA environment.\"\"\"\n",
        "        env = self.initialise_env(\n",
        "            self.port,\n",
        "            isa_path=self.isa_path,\n",
        "            theory_file_path=self.theory_file_path,\n",
        "            working_directory=self.working_dir\n",
        "        )\n",
        "        return env\n",
        "\n",
        "    def _exit(self, env):\n",
        "        \"\"\"Exit the environment and clean up resources.\"\"\"\n",
        "        try:\n",
        "            env.post('exit')\n",
        "        except Exception:\n",
        "            pass\n",
        "        os.system(\"ps aux | grep Isabelle | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
        "        os.system(\"ps aux | grep poly | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
        "\n",
        "    def _parse_output(self, obs):\n",
        "        \"\"\"Parse the sledgehammer output, returning the relevant part.\"\"\"\n",
        "        return obs.split('<hammer>')[0] if '<hammer>' in obs else ''\n",
        "\n",
        "    def _run_step(self, step, i, tls_name, env):\n",
        "        \"\"\"Run a single proof step.\"\"\"\n",
        "        try:\n",
        "            obs, reward, done, metadata = env.step_to_top_level_state(\n",
        "                action=step,\n",
        "                tls_name=tls_name,\n",
        "                new_name=f'default_{i}'\n",
        "            )\n",
        "            return obs, reward, done, metadata, None\n",
        "        except Exception as e:\n",
        "            return '', 0, False, None, str(e)\n",
        "\n",
        "    def _run_sledgehammer(self, step, i, tls_name, env):\n",
        "        \"\"\"Run sledgehammer or fallback heuristics on a step.\"\"\"\n",
        "        heuristics = [\n",
        "            'by auto', 'by simp', 'by blast', 'by fastforce',\n",
        "            'by force', 'by eval', 'by presburger', 'by sos',\n",
        "            'by arith', 'by linarith', 'by (auto simp: field_simps)'\n",
        "        ]\n",
        "        for heuristic in heuristics:\n",
        "            step_ = step.replace('normalhammer', heuristic)\n",
        "            obs, reward, done, metadata, error = self._run_step(step_, i, tls_name, env)\n",
        "            if error is None:\n",
        "                obs = f'{heuristic} <hammer> {obs}'\n",
        "                return obs, reward, done, metadata, error\n",
        "        return self._run_step(step.replace(\"normalhammer\", \"sledgehammer\"), i, tls_name, env)\n",
        "\n",
        "    def check(self, statement_and_proof):\n",
        "        \"\"\"Check the given proof.\"\"\"\n",
        "        env = self._initialize()\n",
        "        env.initialise()\n",
        "\n",
        "        theory = self.wrap_theorem(statement_and_proof)\n",
        "        steps = self.get_parsed(env, theory)\n",
        "\n",
        "        result = self._check(env, steps)\n",
        "        self._exit(env)\n",
        "\n",
        "        # Output the result\n",
        "        #print(\"\\n==== Success: %s\" % result['success'])\n",
        "        #print(\"--- Complete proof:\\n%s\" % result['theorem_and_proof'])\n",
        "        return result\n",
        "\n",
        "    def _check(self, env, steps):\n",
        "        \"\"\"Run the proof steps and collect results.\"\"\"\n",
        "        success, reason, done = False, '', False\n",
        "        step_results = []\n",
        "        tls_name = 'default'\n",
        "\n",
        "        for i, step in enumerate(steps):\n",
        "            time0 = time.time()\n",
        "            if 'normalhammer' in step or 'sledgehammer' in step:\n",
        "                obs, reward, done, metadata, error = self._run_sledgehammer(step, i, tls_name, env)\n",
        "            else:\n",
        "                obs, reward, done, metadata, error = self._run_step(step, i, tls_name, env)\n",
        "\n",
        "            step_time = time.time() - time0\n",
        "            step_results.append({\n",
        "                'index': i, 'step': step, \n",
        "                'output': self._parse_output(obs), \n",
        "                'step_time': step_time\n",
        "            })\n",
        "\n",
        "            if error:\n",
        "                reason = error\n",
        "                break\n",
        "            tls_name = f'default_{i}'\n",
        "\n",
        "        success = done and reward == 1.0\n",
        "        return {\n",
        "            'success': success,\n",
        "            'reason': reason,\n",
        "            'num_steps': len(steps),\n",
        "            'last_step': len(step_results),\n",
        "            'step_results': step_results,\n",
        "            'theorem_and_proof': self.reconstruct(step_results) if success else ''\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def reconstruct(step_results):\n",
        "        \"\"\"Reconstruct the complete proof.\"\"\"\n",
        "        return '\\n'.join(\n",
        "            step_result['output'].strip() if step_result['output'] else step_result['step'].strip()\n",
        "            for step_result in step_results[1:]\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def wrap_theorem(theorem):\n",
        "        \"\"\"Wrap the theorem in a theory file.\"\"\"\n",
        "        return (\n",
        "            'theory Interactive imports HOL.HOL Complex_Main '\n",
        "            '\"HOL-Library.Code_Target_Numeral\" \"HOL-Library.Sum_of_Squares\" '\n",
        "            '\"Symmetric_Polynomials.Vieta\" \"HOL-Computational_Algebra.Computational_Algebra\" '\n",
        "            '\"HOL-Number_Theory.Number_Theory\" \\n begin\\n%s' % theorem\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def get_parsed(env, theory):\n",
        "        \"\"\"Parse the theory and extract proof steps.\"\"\"\n",
        "        raw_steps = env.post(f\"<parse text> ${theory}\")\n",
        "        steps = [s.strip() for s in raw_steps.split('<SEP>') if s.strip() and s != '$']\n",
        "        processed_steps = []\n",
        "        for i, step in enumerate(steps):\n",
        "            if step.lower() == \"then\" and (i == 0 or steps[i - 1].startswith(\"proof\")):\n",
        "                continue\n",
        "            processed_steps.append(step)\n",
        "        return processed_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('../')\n",
        "os.environ['PISA_PATH'] = '/home/siai/Portal-to-ISAbelle/src/main/python'\n",
        "\n",
        "#import dsp_utils\n",
        "\n",
        "checker = Checker(\n",
        "    working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
        "    isa_path='/home/siai/Isabelle2022',\n",
        "    theory_file_path='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
        "    port=9000\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cXk993X6C2ZZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sys\n",
        "import os\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Load and prep dataset\n",
        "SYSTEM_PROMPT = \"\"\"Write a proof in Isabelle that appropriately proves the given statement in natural language.\n",
        "Make sure to wrap the proof within ``isabelle and ``` tags inside answer.\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "``isabelle \n",
        "(Proof)\n",
        " ``` \n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "\n",
        "def extract_isabelle_snippet(text: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Extracts Isabelle proof content from text. Handles both Markdown code blocks \n",
        "    and inline structured proofs by detecting `lemma`, `proof`, and `qed`.\n",
        "    \"\"\"\n",
        "    if not isinstance(text,str) or text.strip() == \"\":\n",
        "        return None\n",
        "    \n",
        "    if \":\" in text and \"```isabelle\" in text:\n",
        "        text = text.split(\"```isabelle\",1)[1]\n",
        "        text = \"```isabelle\" + text\n",
        "        \n",
        "    code_pattern = r\"```isabelle(.*?)qed```\"\n",
        "    matches = re.findall(code_pattern, text, flags=re.DOTALL | re.IGNORECASE)\n",
        "    \n",
        "    if matches:\n",
        "        return matches[0].strip() \n",
        "    inline_pattern = r\"(lemma.*?proof.*?qed)\"\n",
        "    matches = re.findall(inline_pattern, text, flags=re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    if matches:\n",
        "        return matches[0].strip() \n",
        "    return None\n",
        "\n",
        "\n",
        "# uncomment middle messages for 1-shot prompting\n",
        "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
        "    data = load_dataset('kings-crown/FVELer_PISA_Proven', 'default')[split] # type: ignore\n",
        "    data = data.map(lambda x: { # type: ignore\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['natural_language_statement']}\n",
        "        ],\n",
        "        'answer': extract_isabelle_snippet(x['formal_proof'])\n",
        "    }) # type: ignore\n",
        "    return data # type: ignore\n",
        "\n",
        "dataset = get_gsm8k_questions()\n",
        "\n",
        "# Reward functions\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_isabelle_snippet(r) for r in responses]\n",
        "    #print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "def checker_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_snippets = [extract_isabelle_snippet(r) for r in responses]\n",
        "\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_snippets[0]}\")\n",
        "\n",
        "    for content in extracted_snippets:\n",
        "        checker = Checker(\n",
        "            working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
        "            isa_path='/home/siai/Isabelle2022',\n",
        "            theory_file_path='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
        "            port=9000\n",
        "        )\n",
        "        #result = checker.check(content)\n",
        "        rewards = [2.0 if checker.check(content).get(\"success\", False) else 0.0 for content in extracted_snippets]\n",
        "    return rewards\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'natural_language_statement': 'The lemma states that if set \\\\( S \\\\) is a subset of set \\\\( T \\\\), and there exists an element \\\\( x \\\\) such that \\\\( S \\\\) is the singleton set containing \\\\( x \\\\), and there exists an element \\\\( y \\\\) such that \\\\( T \\\\) is the singleton set containing \\\\( y \\\\), then \\\\( S \\\\) is equal to \\\\( T \\\\).',\n",
              " 'formal_proof': 'To translate the informal proof into a structured Isabelle proof, we will follow the steps outlined in the informal reasoning and use `sledgehammer` to assist in finding any necessary lemmas or theorems. Here\\'s how the structured proof in Isabelle might look:\\n\\n```isabelle\\nlemma eq:\\n  assumes \"S \\\\<subseteq> T\"\\n    and \"\\\\<exists>x. S = {x}\"\\n    and \"\\\\<exists>y. T = {y}\"\\n  shows \"S = T\"\\nproof -\\n  from `\\\\<exists>x. S = {x}` obtain x where \"S = {x}\" by auto\\n  from `\\\\<exists>y. T = {y}` obtain y where \"T = {y}\" by auto\\n  have \"x \\\\<in> T\" using `S \\\\<subseteq> T` `S = {x}` by auto\\n  then have \"x = y\" using `T = {y}` by auto\\n  then have \"S = T\" using `S = {x}` `T = {y}` by simp\\n  thus ?thesis by simp\\nqed\\n```\\n\\n### Explanation:\\n\\n1. **Assumptions**: We start by assuming the conditions given in the lemma: `S \\\\<subseteq> T`, `\\\\<exists>x. S = {x}`, and `\\\\<exists>y. T = {y}`.\\n\\n2. **Existential Elimination**: We use `obtain` to extract the elements `x` and `y` such that `S = {x}` and `T = {y}`.\\n\\n3. **Subset Condition**: From the subset condition `S \\\\<subseteq> T` and the fact that `S = {x}`, we deduce that `x` must be an element of `T`.\\n\\n4. **Singleton Property**: Since `T = {y}`, the only element in `T` is `y`, so `x = y`.\\n\\n5. **Set Equality**: With `x = y`, we conclude that `S = {x} = {y} = T`.\\n\\n6. **Conclusion**: The proof concludes with `S = T`, as required by the lemma.\\n\\nThis structured proof closely follows the informal reasoning and uses basic set properties and logical deductions to reach the conclusion.',\n",
              " 'prompt': [{'content': 'Write a proof in Isabelle that appropriately proves the given statement in natural language.\\nMake sure to wrap the proof within ``isabelle and ``` tags inside answer.\\nRespond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n``isabelle \\n(Proof)\\n ``` \\n</answer>\\n',\n",
              "   'role': 'system'},\n",
              "  {'content': 'The lemma states that if set \\\\( S \\\\) is a subset of set \\\\( T \\\\), and there exists an element \\\\( x \\\\) such that \\\\( S \\\\) is the singleton set containing \\\\( x \\\\), and there exists an element \\\\( y \\\\) such that \\\\( T \\\\) is the singleton set containing \\\\( y \\\\), then \\\\( S \\\\) is equal to \\\\( T \\\\).',\n",
              "   'role': 'user'}],\n",
              " 'answer': 'lemma eq:\\n  assumes \"S \\\\<subseteq> T\"\\n    and \"\\\\<exists>x. S = {x}\"\\n    and \"\\\\<exists>y. T = {y}\"\\n  shows \"S = T\"\\nproof -\\n  from `\\\\<exists>x. S = {x}` obtain x where \"S = {x}\" by auto\\n  from `\\\\<exists>y. T = {y}` obtain y where \"T = {y}\" by auto\\n  have \"x \\\\<in> T\" using `S \\\\<subseteq> T` `S = {x}` by auto\\n  then have \"x = y\" using `T = {y}` by auto\\n  then have \"S = T\" using `S = {x}` `T = {y}` by simp\\n  thus ?thesis by simp\\nqed'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptqkXK2D4d6p",
        "outputId": "9d5551f4-0276-47ca-e4ca-e96c846cc976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 2 to the `num_generations` of 6\n"
          ]
        }
      ],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True, # use vLLM for fast inference!\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
        "    num_generations = 6, # Decrease if out of memory\n",
        "    max_prompt_length = 256,\n",
        "    max_completion_length = 2048,\n",
        "    num_train_epochs = 3, # Set to 1 for a full training run\n",
        "    max_steps = 300,\n",
        "    save_steps = 50,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"wandb\", # Can use Weights & Biases\n",
        "    output_dir = \"output_RL\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vzOuSVCL_GA9",
        "outputId": "a71824d4-afd8-47ac-f0ea-eae276927e19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 1,138 | Num Epochs = 2\n",
            "O^O/ \\_/ \\    Batch size per device = 6 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 24 | Total steps = 300\n",
            " \"-____-\"     Number of trainable parameters = 161,480,704\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbalaji-vir1997\u001b[0m (\u001b[33mbalaji-vir1997-stevens-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/siai/Documents/GitHub/NeSy_Policy_Reasoning/wandb/run-20250227_142820-e1gscejl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/e1gscejl' target=\"_blank\">output_RL</a></strong> to <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/e1gscejl' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/e1gscejl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------- Question:\n",
            "The lemma states that for any natural numbers \\( a \\) and \\( b \\), the product of the integer division of \\( a \\) by \\( b \\) and \\( b \\) is less than or equal to \\( a \\). \n",
            "Answer:\n",
            "lemma div_mult_le:\n",
            "  \"a div b * b \\<le> a\" for a b :: nat\n",
            "proof -\n",
            "  have \"a = (a div b) * b + (a mod b)\"\n",
            "    by (simp add: div_mult_mod_eq)\n",
            "  then have \"(a div b) * b \\<le> a\"\n",
            "    by simp\n",
            "  thus ?thesis .\n",
            "qed \n",
            "Response:\n",
            "Okay, so I need to prove that for any natural numbers \\( a \\) and \\( b \\), the product of the integer division of \\( a \\) by \\( b \\) and \\( b \\) is less than or equal to \\( a \\). Hmm, let me break this down.\n",
            "\n",
            "First, I remember that in integer division, when you divide \\( a \\) by \\( b \\), you get a quotient \\( q \\) and a remainder \\( r \\) such that \\( a = q \\cdot b + r \\) and \\( 0 \\leq r < b \\). So, the quotient \\( q \\) is essentially the integer division of \\( a \\) by \\( b \\), which is what we're dealing with here.\n",
            "\n",
            "Now, the statement we need to prove is \\( (a \\div b) \\cdot b \\leq a \\). Substituting the quotient from the division algorithm, \\( q \\), this becomes \\( q \\cdot b \\leq (q \\cdot b) + r \\). Simplify the right side: \\( (q \\cdot b) + r \\) minus \\( q \\cdot b \\) is just \\( r \\). Since \\( r \\) is between 0 (inclusive) and \\( b \\) (exclusive), subtracting \\( q \\cdot b \\) from both sides gives \\( 0 \\leq r \\).\n",
            "\n",
            "This inequality is always true because \\( r \\) is non-negative. Therefore, \\( q \\cdot b \\) is always less than or equal to \\( a \\), which means \\( (a \\div b) \\cdot b \\leq a \\) holds for all natural numbers \\( a \\) and \\( b \\).\n",
            "\n",
            "Wait, I should make sure I didn't make any assumptions here. Does this hold for all natural numbers, including when \\( b \\) is zero? Oh, right, in the context of natural numbers, \\( b \\) shouldn't be zero because division by zero isn't defined. So the statement holds for \\( b \\neq 0 \\), which is typically the case in these proofs.\n",
            "\n",
            "Also, considering edge cases: if \\( a \\) is zero, then \\( (0 \\div b) \\cdot b = 0 \\cdot b = 0 \\), which is equal to \\( a \\). If \\( a \\) is a multiple of \\( b \\), then the remainder \\( r \\) is zero, so again the inequality becomes an equality. If \\( a \\) is not a multiple of \\( b \\), then \\( r \\) is positive, making \\( q \\cdot b < a \\).\n",
            "\n",
            "This makes sense. So the proof seems solid. I think I've covered all the necessary points. Let's formalize this now.\n",
            "</think>\n",
            "\n",
            "<reasoning>\n",
            "The statement to prove is that for any natural numbers \\( a \\) and \\( b \\), the product of the integer division of \\( a \\) by \\( b \\) and \\( b \\) is less than or equal to \\( a \\). \n",
            "\n",
            "Using the division algorithm, any natural number \\( a \\) can be expressed as \\( a = (a \\div b) \\cdot b + r \\), where \\( r \\) is the remainder and \\( 0 \\leq r < b \\). When we multiply the quotient \\( (a \\div b) \\) by \\( b \\), we have \\( (a \\div b) \\cdot b \\). Adding the remainder \\( r \\) to this product gives us the original number \\( a \\). Since \\( r \\geq 0 \\), it follows that \\( (a \\div b) \\cdot b \\leq a \\).\n",
            "</reasoning>\n",
            "\n",
            "```isabelle\n",
            "lemma div_mul_self_le: \"a ‚â§ b * (a div b)\"\n",
            "  by (cases \"b\" rule: ne_sinic) simp_all [Nat.div_self]\n",
            "``` \n",
            "Extracted:\n",
            "None\n",
            "-------------------- Question:\n",
            "The lemma `if_evrv` can be translated into the following natural language statement:\n",
            "\n",
            "Suppose we have two conditions:\n",
            "1. If the boolean condition `b` is true, then the function `f` satisfies the property `equiv_valid_rv_inv` with respect to the invariant `I`, the set of actions `A`, the relation `R`, and the precondition `P`.\n",
            "2. If the boolean condition `b` is false, then the function `g` satisfies the property `equiv_valid_rv_inv` with respect to the invariant `I`, the set of actions `A`, the relation `R`, and the precondition `Q`.\n",
            "\n",
            "Under these assumptions, the function that chooses `f` if `b` is true and `g` if `b` is false satisfies the property `equiv_valid_rv_inv` with respect to the invariant `I`, the set of actions `A`, the relation `R`, and the combined precondition that requires `P` to hold when `b` is true and `Q` to hold when `b` is false. \n",
            "Answer:\n",
            "lemma if_evrv:\n",
            "  assumes \"b \\<Longrightarrow> equiv_valid_rv_inv I A R P f\"\n",
            "  assumes \"\\<not> b \\<Longrightarrow> equiv_valid_rv_inv I A R Q g\"\n",
            "  shows \"equiv_valid_rv_inv I A R (\\<lambda>s. (b \\<longrightarrow> P s) \\<and> (\\<not>b \\<longrightarrow> Q s)) (if b then f else g)\"\n",
            "proof (cases b)\n",
            "  case True\n",
            "  then have \"equiv_valid_rv_inv I A R P f\" using assms(1) by simp\n",
            "  moreover have \"(\\<lambda>s. (True \\<longrightarrow> P s) \\<and> (False \\<longrightarrow> Q s)) = P\" by auto\n",
            "  ultimately show ?thesis by simp\n",
            "next\n",
            "  case False\n",
            "  then have \"equiv_valid_rv_inv I A R Q g\" using assms(2) by simp\n",
            "  moreover have \"(\\<lambda>s. (False \\<longrightarrow> P s) \\<and> (True \\<longrightarrow> Q s)) = Q\" by auto\n",
            "  ultimately show ?thesis by simp\n",
            "qed \n",
            "Response:\n",
            "Okay, so I've been given this lemma called `if_evrv` that I need to prove using Isabelle. The lemma talks about functions `f` and `g` satisfying a property called `equiv_valid_rv_inv` under certain conditions when a boolean variable `b` is either true or false. I need to make sure I understand all the concepts involved before diving into the proof.\n",
            "\n",
            "First, let's break down what `equiv_valid_rv_inv` means. From the context, it seems this is a property that relates to equivalence and validity with respect to some invariant `I`, a set of actions `A`, a relation `R`, and a precondition. The exact semantics might depend on the specific definitions of these terms, which I don't have here, but I can assume that `equiv_valid_rv_inv` is some kind of equivalence property that holds under certain conditions.\n",
            "\n",
            "Now, the lemma is about choosing between two functions, `f` and `g`, based on the boolean value of `b`. The functions `f` and `g` each satisfy `equiv_valid_rv_inv` under different conditions: when `b` is true, `f` satisfies it with certain preconditions, and when `b` is false, `g` does. The goal is to show that a function that selects `f` or `g` based on `b`'s value will also satisfy `equiv_valid_rv_inv` with combined preconditions.\n",
            "\n",
            "I think the key idea here is to use the fact that `b` can only be either true or false and ensure that the combined function behaves as required in both cases. Since `b` is a boolean, the function that chooses between `f` and `g` is essentially a conditional function.\n",
            "\n",
            "To approach the proof, I might need to consider both cases: when `b` is true and when `b` is false. In each case, show that the combined function satisfies `equiv_valid_rv_inv` with the respective preconditions. Then, use the tautology that since `b` must be either true or false, the combined function behaves correctly in all scenarios.\n",
            "\n",
            "I should also consider how to structure the proof in Isabelle. It might involve introducing the assumptions and then using them to prove the main goal. Since Isabelle is a proof assistant, the proof will be formal and rely on logical steps rather than natural language explanation.\n",
            "\n",
            "I might need to use tactics like `case` to handle the two cases of `b`, and then apply the assumptions about `f` and `g` to each case. Also, I should ensure that the combined precondition is correctly represented in the logic, combining `P` for `b` being true and `Q` for `b` being false.\n",
            "\n",
            "I'm a bit unsure about the exact syntax and how to properly use the lemmas and definitions related to `equiv_valid_rv_inv`, but I can work that out as I go along. I'll need to make sure that each step in the proof is logically sound and that I correctly reference the necessary definitions and rules in Isabelle.\n",
            "\n",
            "Overall, the strategy is to use case analysis on `b`, apply the corresponding function for each case, and then show that the combined function satisfies the desired property. This should logically follow from the assumptions that `f` and `g` satisfy `equiv_valid_rv_inv` under the respective conditions.\n",
            "</think>\n",
            "\n",
            "<reasoning>\n",
            "The `if_evrv` lemma establishes that for a boolean condition `b`, if `f` satisfies `equiv_valid_rv_inv` under `P` when `b` is true and `g` satisfies `equiv_valid_rv_inv` under `Q` when `b` is false, then the conditional function `Œªb. if b then f else g` satisfies `equiv_valid_rv_inv` under a combined precondition. The proof proceeds by case analysis on `b`:\n",
            "1. **Case `b = true`:** The conditional function reduces to `f`, which by assumption satisfies `equiv_valid_rv_inv` with precondition `P`. Thus, the function satisfies `equiv_valid_rv_inv` under the combined precondition.\n",
            "2. **Case `b = false`:** The conditional function reduces to `g`, which by assumption satisfies `equiv_valid_rv_inv` with precondition `Q`. Thus, the function satisfies `equiv_valid_rv_inv` under the combined precondition.\n",
            "\n",
            "Since both cases exhaust all possibilities for the boolean `b`, the conditional function satisfies `equiv_valid_rv_inv` under the combined precondition combining `P` and `Q`.\n",
            "</answer>\n",
            "``isabelle \n",
            "lemma if_evrv:\n",
            "  assumes h1: \"b = true ‚üπ f satisfies (I, A, R, P)\"\n",
            "  assumes h2: \"b = false ‚üπ g satisfies (I, A, R, Q)\"\n",
            "  shows \"Œªb. if b then f else g satisfies (I, A, R, (Œªb. P b ‚àß Q b))\"\n",
            "  using h1 h2 by (cases b) simp_all\n",
            "``` \n",
            "Extracted:\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "sys.path.append('../')\n",
        "os.environ['PISA_PATH'] = '/home/siai/Portal-to-ISAbelle/src/main/python'\n",
        "\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "        checker_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "qtcz_lpbVC92",
        "outputId": "9b12655a-7905-42a8-d6f0-210ff74a6d73"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "zf_OY5WMVOxF",
        "outputId": "c34d81a7-192d-427d-81f0-cbca7009b7d1"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDgFfhFYIAS"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n33Lwz_eqG4Z"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ca67b0c4ca64eb788358a51308f6b97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac03aff5c314b00ac938c80eb7b2f8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c522d78b1834068bd4b155d0f87a4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca67b0c4ca64eb788358a51308f6b97",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83c3c811923a4642aba156d1215b39d2",
            "value": 1
          }
        },
        "3febcf8a8eca40c28aafc697f3ec8776": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f3d96b613e94e9984d4599ca9ca7b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea549fffa8c2469888d1668158bc105c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_98b432b98839428f85d91580c21e80e2",
            "value": ""
          }
        },
        "6221f0be3b8d48e797c873565a216680": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66c3271554b1455eb56be55c9241e45e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fee4f852c9744a07b909e586e3615604",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3febcf8a8eca40c28aafc697f3ec8776",
            "value": 1
          }
        },
        "697faad6643a43aca98015da4faef186": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "769bde36e2ba4434bddd78e7d5911be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ac03aff5c314b00ac938c80eb7b2f8a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_88c63d94a05a42c49d5f8958a27987a6",
            "value": ""
          }
        },
        "83c3c811923a4642aba156d1215b39d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88c63d94a05a42c49d5f8958a27987a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94873c3c077e483790b34f95c421f484": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98b432b98839428f85d91580c21e80e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a23afba19c2a4d3a90d771fc55f8d490": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e863bf099e064da7b482c21fe7b77de7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_697faad6643a43aca98015da4faef186",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:16&lt;00:00,‚Äá16.13s/it]\n"
          }
        },
        "b4e1eb8eeb064c88a2142e474fb8327f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d36b61cf796c429080e93ea838a3759e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4e1eb8eeb064c88a2142e474fb8327f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_da10502506f9448c9de94f1ddd84d3b1",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:27&lt;00:00,‚Äá27.50s/it]\n"
          }
        },
        "d8d0dca36cfc47f0919924da07c231e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f3d96b613e94e9984d4599ca9ca7b17",
              "IPY_MODEL_66c3271554b1455eb56be55c9241e45e",
              "IPY_MODEL_d36b61cf796c429080e93ea838a3759e"
            ],
            "layout": "IPY_MODEL_94873c3c077e483790b34f95c421f484"
          }
        },
        "da10502506f9448c9de94f1ddd84d3b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6cc388e78c14abfaa49d2be6fa1b5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_769bde36e2ba4434bddd78e7d5911be4",
              "IPY_MODEL_3c522d78b1834068bd4b155d0f87a4d7",
              "IPY_MODEL_a23afba19c2a4d3a90d771fc55f8d490"
            ],
            "layout": "IPY_MODEL_6221f0be3b8d48e797c873565a216680"
          }
        },
        "e863bf099e064da7b482c21fe7b77de7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea549fffa8c2469888d1668158bc105c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fee4f852c9744a07b909e586e3615604": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
