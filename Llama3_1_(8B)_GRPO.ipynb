{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ5x6xlUqG4N"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxKlbfLhqG4P"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf7gq1YJqG4Q"
      },
      "source": [
        "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xpcb5j3qG4Q"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xnm_FUyqG4S"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1zyu9Ug2XEt"
      },
      "source": [
        "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59DIs5BMcvjN",
        "outputId": "a4b3de70-c99c-4e76-ee06-dab6a6505a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/siai/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-27 14:44:27,398\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8-SLRUB2gwM"
      },
      "source": [
        "Load up `Llama 3.1 8B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700,
          "referenced_widgets": [
            "d8d0dca36cfc47f0919924da07c231e8",
            "5f3d96b613e94e9984d4599ca9ca7b17",
            "66c3271554b1455eb56be55c9241e45e",
            "d36b61cf796c429080e93ea838a3759e",
            "94873c3c077e483790b34f95c421f484",
            "ea549fffa8c2469888d1668158bc105c",
            "98b432b98839428f85d91580c21e80e2",
            "fee4f852c9744a07b909e586e3615604",
            "3febcf8a8eca40c28aafc697f3ec8776",
            "b4e1eb8eeb064c88a2142e474fb8327f",
            "da10502506f9448c9de94f1ddd84d3b1",
            "e6cc388e78c14abfaa49d2be6fa1b5d9",
            "769bde36e2ba4434bddd78e7d5911be4",
            "3c522d78b1834068bd4b155d0f87a4d7",
            "a23afba19c2a4d3a90d771fc55f8d490",
            "6221f0be3b8d48e797c873565a216680",
            "1ac03aff5c314b00ac938c80eb7b2f8a",
            "88c63d94a05a42c49d5f8958a27987a6",
            "0ca67b0c4ca64eb788358a51308f6b97",
            "83c3c811923a4642aba156d1215b39d2",
            "e863bf099e064da7b482c21fe7b77de7",
            "697faad6643a43aca98015da4faef186"
          ]
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "514dea04-804e-47a8-b891-ed3f4a6fb530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-27 14:44:28 __init__.py:207] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading kings-crown/Isabelle_FVELer_SFT with actual GPU utilization = 79.47%\n",
            "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 47.54 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 288.\n",
            "Unsloth: vLLM's KV Cache can use up to 22.99 GB. Also swap space = 6 GB.\n",
            "INFO 02-27 14:44:33 config.py:549] This model supports multiple tasks: {'embed', 'generate', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\n",
            "INFO 02-27 14:44:33 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='kings-crown/Isabelle_FVELer_SFT', speculative_config=None, tokenizer='kings-crown/Isabelle_FVELer_SFT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=kings-crown/Isabelle_FVELer_SFT, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":288}, use_cached_outputs=False, \n",
            "INFO 02-27 14:44:34 cuda.py:229] Using Flash Attention backend.\n",
            "INFO 02-27 14:44:34 model_runner.py:1110] Starting to load model kings-crown/Isabelle_FVELer_SFT...\n",
            "INFO 02-27 14:44:34 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W227 14:44:34.127945147 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.47it/s]\n",
            "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  3.40it/s]\n",
            "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.61it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.15it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.35it/s]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-27 14:44:36 model_runner.py:1115] Loading model weights took 14.3854 GB\n",
            "INFO 02-27 14:44:36 punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 02-27 14:44:38 worker.py:267] Memory profiling takes 1.32 seconds\n",
            "INFO 02-27 14:44:38 worker.py:267] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.79) = 37.78GiB\n",
            "INFO 02-27 14:44:38 worker.py:267] model weights take 14.39GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 21.76GiB.\n",
            "INFO 02-27 14:44:38 executor_base.py:111] # cuda blocks: 25460, # CPU blocks: 7021\n",
            "INFO 02-27 14:44:38 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 198.91x\n",
            "INFO 02-27 14:44:42 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:21<00:00,  1.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-27 14:45:03 model_runner.py:1562] Graph capturing finished in 21 secs, took 2.12 GiB\n",
            "INFO 02-27 14:45:03 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 27.04 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Unsloth 2025.2.15 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "import time\n",
        "\n",
        "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
        "lora_rank = 128 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"kings-crown/Isabelle_FVELer_SFT\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.8, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Checker(object):\n",
        "    \"\"\"A modified version of the Draft, Sketch, Prove proof-checking client.\n",
        "    (https://github.com/albertqjiang/draft_sketch_prove/blob/main/autoformalization/checker.py)\n",
        "\n",
        "    This checker supports Isabelle2022 via the new version of PISA\n",
        "    (https://albertqjiang.github.io/Portal-to-ISAbelle/).\n",
        "\n",
        "    It supports checking a miniF2F-style proof via `check`.\n",
        "\n",
        "    Finally, it replaces `sledgehammer` with a call to `normalhammer`.\n",
        "    \"\"\"\n",
        "    def __init__(self, working_dir, isa_path, theory_file_path, port=9000):\n",
        "        sys.path.append(os.environ.get('PISA_PATH', ''))\n",
        "        try:\n",
        "            from pisa_client import initialise_env\n",
        "            self.initialise_env = initialise_env\n",
        "        except ImportError:\n",
        "            print(\"Set $PISA_PATH to /yourpath/to/Portal-to-ISAbelle/src/main/python\")\n",
        "\n",
        "        self.working_dir = working_dir\n",
        "        self.isa_path = isa_path\n",
        "        self.theory_file_path = theory_file_path\n",
        "        self.port = port\n",
        "\n",
        "    def _initialize(self):\n",
        "        \"\"\"Initialize the PISA environment.\"\"\"\n",
        "        env = self.initialise_env(\n",
        "            self.port,\n",
        "            isa_path=self.isa_path,\n",
        "            theory_file_path=self.theory_file_path,\n",
        "            working_directory=self.working_dir\n",
        "        )\n",
        "        return env\n",
        "\n",
        "    def _exit(self, env):\n",
        "        \"\"\"Exit the environment and clean up resources.\"\"\"\n",
        "        try:\n",
        "            env.post('exit')\n",
        "        except Exception:\n",
        "            pass\n",
        "        os.system(\"ps aux | grep Isabelle | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
        "        os.system(\"ps aux | grep poly | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
        "\n",
        "    def _parse_output(self, obs):\n",
        "        \"\"\"Parse the sledgehammer output, returning the relevant part.\"\"\"\n",
        "        return obs.split('<hammer>')[0] if '<hammer>' in obs else ''\n",
        "\n",
        "    def _run_step(self, step, i, tls_name, env):\n",
        "        \"\"\"Run a single proof step.\"\"\"\n",
        "        try:\n",
        "            obs, reward, done, metadata = env.step_to_top_level_state(\n",
        "                action=step,\n",
        "                tls_name=tls_name,\n",
        "                new_name=f'default_{i}'\n",
        "            )\n",
        "            return obs, reward, done, metadata, None\n",
        "        except Exception as e:\n",
        "            return '', 0, False, None, str(e)\n",
        "\n",
        "    def _run_sledgehammer(self, step, i, tls_name, env):\n",
        "        \"\"\"Run sledgehammer or fallback heuristics on a step.\"\"\"\n",
        "        heuristics = [\n",
        "            'by auto', 'by simp', 'by blast', 'by fastforce',\n",
        "            'by force', 'by eval', 'by presburger', 'by sos',\n",
        "            'by arith', 'by linarith', 'by (auto simp: field_simps)'\n",
        "        ]\n",
        "        for heuristic in heuristics:\n",
        "            step_ = step.replace('normalhammer', heuristic)\n",
        "            obs, reward, done, metadata, error = self._run_step(step_, i, tls_name, env)\n",
        "            if error is None:\n",
        "                obs = f'{heuristic} <hammer> {obs}'\n",
        "                return obs, reward, done, metadata, error\n",
        "        return self._run_step(step.replace(\"normalhammer\", \"sledgehammer\"), i, tls_name, env)\n",
        "\n",
        "    def check(self, statement_and_proof):\n",
        "        \"\"\"Check the given proof.\"\"\"\n",
        "        env = self._initialize()\n",
        "        env.initialise()\n",
        "\n",
        "        theory = self.wrap_theorem(statement_and_proof)\n",
        "        steps = self.get_parsed(env, theory)\n",
        "\n",
        "        result = self._check(env, steps)\n",
        "        self._exit(env)\n",
        "\n",
        "        # Output the result\n",
        "        #print(\"\\n==== Success: %s\" % result['success'])\n",
        "        #print(\"--- Complete proof:\\n%s\" % result['theorem_and_proof'])\n",
        "        return result\n",
        "\n",
        "    def _check(self, env, steps):\n",
        "        \"\"\"Run the proof steps and collect results.\"\"\"\n",
        "        success, reason, done = False, '', False\n",
        "        step_results = []\n",
        "        tls_name = 'default'\n",
        "\n",
        "        for i, step in enumerate(steps):\n",
        "            time0 = time.time()\n",
        "            if 'normalhammer' in step or 'sledgehammer' in step:\n",
        "                obs, reward, done, metadata, error = self._run_sledgehammer(step, i, tls_name, env)\n",
        "            else:\n",
        "                obs, reward, done, metadata, error = self._run_step(step, i, tls_name, env)\n",
        "\n",
        "            step_time = time.time() - time0\n",
        "            step_results.append({\n",
        "                'index': i, 'step': step, \n",
        "                'output': self._parse_output(obs), \n",
        "                'step_time': step_time\n",
        "            })\n",
        "\n",
        "            if error:\n",
        "                reason = error\n",
        "                break\n",
        "            tls_name = f'default_{i}'\n",
        "\n",
        "        success = done and reward == 1.0\n",
        "        return {\n",
        "            'success': success,\n",
        "            'reason': reason,\n",
        "            'num_steps': len(steps),\n",
        "            'last_step': len(step_results),\n",
        "            'step_results': step_results,\n",
        "            'theorem_and_proof': self.reconstruct(step_results) if success else ''\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def reconstruct(step_results):\n",
        "        \"\"\"Reconstruct the complete proof.\"\"\"\n",
        "        return '\\n'.join(\n",
        "            step_result['output'].strip() if step_result['output'] else step_result['step'].strip()\n",
        "            for step_result in step_results[1:]\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def wrap_theorem(theorem):\n",
        "        \"\"\"Wrap the theorem in a theory file.\"\"\"\n",
        "        return (\n",
        "            'theory Interactive imports HOL.HOL Complex_Main '\n",
        "            '\"HOL-Library.Code_Target_Numeral\" \"HOL-Library.Sum_of_Squares\" '\n",
        "            '\"Symmetric_Polynomials.Vieta\" \"HOL-Computational_Algebra.Computational_Algebra\" '\n",
        "            '\"HOL-Number_Theory.Number_Theory\" \\n begin\\n%s' % theorem\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def get_parsed(env, theory):\n",
        "        \"\"\"Parse the theory and extract proof steps.\"\"\"\n",
        "        raw_steps = env.post(f\"<parse text> ${theory}\")\n",
        "        steps = [s.strip() for s in raw_steps.split('<SEP>') if s.strip() and s != '$']\n",
        "        processed_steps = []\n",
        "        for i, step in enumerate(steps):\n",
        "            if step.lower() == \"then\" and (i == 0 or steps[i - 1].startswith(\"proof\")):\n",
        "                continue\n",
        "            processed_steps.append(step)\n",
        "        return processed_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('../')\n",
        "os.environ['PISA_PATH'] = '/home/siai/Portal-to-ISAbelle/src/main/python'\n",
        "\n",
        "#import dsp_utils\n",
        "\n",
        "checker = Checker(\n",
        "    working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
        "    isa_path='/home/siai/Isabelle2022',\n",
        "    theory_file_path='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
        "    port=9000\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXk993X6C2ZZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1138/1138 [00:00<00:00, 14085.10 examples/s]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import sys\n",
        "import os\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Load and prep dataset\n",
        "SYSTEM_PROMPT = \"\"\"Write a proof in Isabelle that appropriately proves the given statement in natural language.\n",
        "Make sure to wrap the proof within ``isabelle and ``` tags inside answer.\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "[Your explanation or chain of thought]\n",
        "</reasoning>\n",
        "<answer>\n",
        "``` \n",
        "isabelle \n",
        "[Your formal Isabelle code]\n",
        "``` \n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "\n",
        "def extract_isabelle_snippet(text: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Extracts Isabelle proof content from text. Handles both Markdown code blocks \n",
        "    and inline structured proofs by detecting `lemma`, `proof`, and `qed`.\n",
        "    \"\"\"\n",
        "    if not isinstance(text,str) or text.strip() == \"\":\n",
        "        return None\n",
        "    \n",
        "    if \":\" in text and \"```isabelle\" in text:\n",
        "        text = text.split(\"```isabelle\",1)[1]\n",
        "        text = \"```isabelle\" + text\n",
        "        \n",
        "    code_pattern = r\"```isabelle\\s*(.+?)\\s*```\"\n",
        "    matches = re.findall(code_pattern, text, flags=re.DOTALL | re.IGNORECASE)\n",
        "    \n",
        "    if matches:\n",
        "        return matches[0].strip() \n",
        "    inline_pattern = r\"(lemma.*?proof.*?qed)\"\n",
        "    matches = re.findall(inline_pattern, text, flags=re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    if matches:\n",
        "        return matches[0].strip() \n",
        "    return None\n",
        "\n",
        "\n",
        "# uncomment middle messages for 1-shot prompting\n",
        "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
        "    data = load_dataset('kings-crown/FVELer_PISA_Proven', 'default')[split] # type: ignore\n",
        "    data = data.map(lambda x: { # type: ignore\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['natural_language_statement']}\n",
        "        ],\n",
        "        'answer': extract_isabelle_snippet(x['formal_proof'])\n",
        "    }) # type: ignore\n",
        "    return data # type: ignore\n",
        "\n",
        "dataset = get_gsm8k_questions()\n",
        "\n",
        "# Reward functions\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_isabelle_snippet(r) for r in responses]\n",
        "    #print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "def checker_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_snippets = [extract_isabelle_snippet(r) for r in responses]\n",
        "\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_snippets[0]}\")\n",
        "\n",
        "    for content in extracted_snippets:\n",
        "        checker = Checker(\n",
        "            working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
        "            isa_path='/home/siai/Isabelle2022',\n",
        "            theory_file_path='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
        "            port=9000\n",
        "        )\n",
        "        #result = checker.check(content)\n",
        "        rewards = [2.0 if checker.check(content).get(\"success\", False) else 0.0 for content in extracted_snippets]\n",
        "    return rewards\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'natural_language_statement': 'The lemma states that if set \\\\( S \\\\) is a subset of set \\\\( T \\\\), and there exists an element \\\\( x \\\\) such that \\\\( S \\\\) is the singleton set containing \\\\( x \\\\), and there exists an element \\\\( y \\\\) such that \\\\( T \\\\) is the singleton set containing \\\\( y \\\\), then \\\\( S \\\\) is equal to \\\\( T \\\\).',\n",
              " 'formal_proof': 'To translate the informal proof into a structured Isabelle proof, we will follow the steps outlined in the informal reasoning and use `sledgehammer` to assist in finding any necessary lemmas or theorems. Here\\'s how the structured proof in Isabelle might look:\\n\\n```isabelle\\nlemma eq:\\n  assumes \"S \\\\<subseteq> T\"\\n    and \"\\\\<exists>x. S = {x}\"\\n    and \"\\\\<exists>y. T = {y}\"\\n  shows \"S = T\"\\nproof -\\n  from `\\\\<exists>x. S = {x}` obtain x where \"S = {x}\" by auto\\n  from `\\\\<exists>y. T = {y}` obtain y where \"T = {y}\" by auto\\n  have \"x \\\\<in> T\" using `S \\\\<subseteq> T` `S = {x}` by auto\\n  then have \"x = y\" using `T = {y}` by auto\\n  then have \"S = T\" using `S = {x}` `T = {y}` by simp\\n  thus ?thesis by simp\\nqed\\n```\\n\\n### Explanation:\\n\\n1. **Assumptions**: We start by assuming the conditions given in the lemma: `S \\\\<subseteq> T`, `\\\\<exists>x. S = {x}`, and `\\\\<exists>y. T = {y}`.\\n\\n2. **Existential Elimination**: We use `obtain` to extract the elements `x` and `y` such that `S = {x}` and `T = {y}`.\\n\\n3. **Subset Condition**: From the subset condition `S \\\\<subseteq> T` and the fact that `S = {x}`, we deduce that `x` must be an element of `T`.\\n\\n4. **Singleton Property**: Since `T = {y}`, the only element in `T` is `y`, so `x = y`.\\n\\n5. **Set Equality**: With `x = y`, we conclude that `S = {x} = {y} = T`.\\n\\n6. **Conclusion**: The proof concludes with `S = T`, as required by the lemma.\\n\\nThis structured proof closely follows the informal reasoning and uses basic set properties and logical deductions to reach the conclusion.',\n",
              " 'prompt': [{'content': 'Write a proof in Isabelle that appropriately proves the given statement in natural language.\\nMake sure to wrap the proof within ``isabelle and ``` tags inside answer.\\nRespond in the following format:\\n<reasoning>\\n[Your explanation or chain of thought]\\n</reasoning>\\n<answer>\\n``` \\nisabelle \\n[Your formal Isabelle code]\\n``` \\n</answer>\\n',\n",
              "   'role': 'system'},\n",
              "  {'content': 'The lemma states that if set \\\\( S \\\\) is a subset of set \\\\( T \\\\), and there exists an element \\\\( x \\\\) such that \\\\( S \\\\) is the singleton set containing \\\\( x \\\\), and there exists an element \\\\( y \\\\) such that \\\\( T \\\\) is the singleton set containing \\\\( y \\\\), then \\\\( S \\\\) is equal to \\\\( T \\\\).',\n",
              "   'role': 'user'}],\n",
              " 'answer': 'lemma eq:\\n  assumes \"S \\\\<subseteq> T\"\\n    and \"\\\\<exists>x. S = {x}\"\\n    and \"\\\\<exists>y. T = {y}\"\\n  shows \"S = T\"\\nproof -\\n  from `\\\\<exists>x. S = {x}` obtain x where \"S = {x}\" by auto\\n  from `\\\\<exists>y. T = {y}` obtain y where \"T = {y}\" by auto\\n  have \"x \\\\<in> T\" using `S \\\\<subseteq> T` `S = {x}` by auto\\n  then have \"x = y\" using `T = {y}` by auto\\n  then have \"S = T\" using `S = {x}` `T = {y}` by simp\\n  thus ?thesis by simp\\nqed'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptqkXK2D4d6p",
        "outputId": "9d5551f4-0276-47ca-e4ca-e96c846cc976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 2 to the `num_generations` of 6\n"
          ]
        }
      ],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True, # use vLLM for fast inference!\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
        "    num_generations = 6, # Decrease if out of memory\n",
        "    max_prompt_length = 256,\n",
        "    max_completion_length = 3000,\n",
        "    num_train_epochs = 3, # Set to 1 for a full training run\n",
        "    max_steps = 500,\n",
        "    save_steps = 50,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"wandb\", # Can use Weights & Biases\n",
        "    output_dir = \"output_RL\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vzOuSVCL_GA9",
        "outputId": "a71824d4-afd8-47ac-f0ea-eae276927e19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 1,138 | Num Epochs = 2\n",
            "O^O/ \\_/ \\    Batch size per device = 6 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 24 | Total steps = 500\n",
            " \"-____-\"     Number of trainable parameters = 161,480,704\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbalaji-vir1997\u001b[0m (\u001b[33mbalaji-vir1997-stevens-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/siai/Documents/GitHub/NeSy_Policy_Reasoning/wandb/run-20250227_144619-fza0e0f3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/fza0e0f3' target=\"_blank\">output_RL</a></strong> to <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/fza0e0f3' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/fza0e0f3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------- Question:\n",
            "The lemma states that for any natural numbers \\( a \\) and \\( b \\), the product of the integer division of \\( a \\) by \\( b \\) and \\( b \\) is less than or equal to \\( a \\). \n",
            "Answer:\n",
            "lemma div_mult_le:\n",
            "  \"a div b * b \\<le> a\" for a b :: nat\n",
            "proof -\n",
            "  have \"a = (a div b) * b + (a mod b)\"\n",
            "    by (simp add: div_mult_mod_eq)\n",
            "  then have \"(a div b) * b \\<le> a\"\n",
            "    by simp\n",
            "  thus ?thesis .\n",
            "qed \n",
            "Response:\n",
            "Okay, so I need to prove that for any natural numbers \\( a \\) and \\( b \\), the product of the integer division of \\( a \\) by \\( b \\) and \\( b \\) is less than or equal to \\( a \\). Hmm, let me break this down step by step.\n",
            "\n",
            "First, what does integer division mean? I think it's when you divide two numbers and take the floor of the result. So, \\( a \\div b \\) would give a quotient which is the largest integer less than or equal to the actual division result of \\( a \\) by \\( b \\). For example, if \\( a = 10 \\) and \\( b = 3 \\), then \\( 10 \\div 3 = 3 \\) because 3 times 3 is 9, which is less than 10, and the next multiple would be 12, which is more than 10.\n",
            "\n",
            "So, the statement is saying that when you take \\( a \\div b \\) and multiply it by \\( b \\), the result should be less than or equal to \\( a \\). Using my previous example, \\( 3 \\times 3 = 9 \\) which is indeed less than or equal to 10.\n",
            "\n",
            "I guess this makes sense because when you divide \\( a \\) by \\( b \\), you're essentially seeing how many times \\( b \\) fits completely into \\( a \\). Then, when you multiply that quotient back by \\( b \\), you're getting the largest multiple of \\( b \\) that doesn't exceed \\( a \\). So, naturally, this product should be less than or equal to \\( a \\).\n",
            "\n",
            "But wait, what if \\( b \\) is 0? Division by zero is undefined, so \\( b \\) must be a positive natural number, right? Or maybe \\( b \\) can't be zero because division by zero isn't allowed. So, in the context of natural numbers, \\( b \\) should be at least 1.\n",
            "\n",
            "Let me think about another example to test this. If \\( a = 7 \\) and \\( b = 2 \\), then \\( 7 \\div 2 = 3 \\), and \\( 3 \\times 2 = 6 \\), which is less than 7. Another example: if \\( a = 5 \\) and \\( b = 5 \\), then \\( 5 \\div 5 = 1 \\), and \\( 1 \\times 5 = 5 \\), which is equal to \\( a \\). So, it still holds.\n",
            "\n",
            "Now, how do I formally prove this? Maybe I can use the definition of integer division. So, \\( a \\div b \\) is the largest integer \\( q \\) such that \\( q \\times b \\leq a \\). So, by definition, \\( q = a \\div b \\) implies that \\( q \\times b \\leq a < (q + 1) \\times b \\). Therefore, \\( a \\div b \\times b \\leq a \\) directly follows from the definition.\n",
            "\n",
            "Wait, is that enough? Maybe I should elaborate more. Perhaps I can use the properties of natural numbers and division. Also, maybe consider different cases, like whether \\( a \\) is a multiple of \\( b \\) or not. If \\( a \\) is a multiple of \\( b \\), then \\( a \\div b \\times b = a \\), so equality holds. If not, \\( a \\div b \\times b \\) is strictly less than \\( a \\).\n",
            "\n",
            "Maybe I should also think about how this relates to the remainder. If \\( a = q \\times b + r \\), where \\( 0 \\leq r < b \\), then \\( a \\div b = q \\), so \\( q \\times b = a - r \\), which is less than or equal to \\( a \\) since \\( r \\geq 0 \\).\n",
            "\n",
            "I think that covers it. Using the definition of division with quotient and remainder, we can show that \\( q \\times b \\) is indeed less than or equal to \\( a \\).\n",
            "\n",
            "So, putting it all together, the proof would start by defining what integer division is, then using that definition or the properties of division to show the desired inequality holds.\n",
            "</think>\n",
            "\n",
            "<reasoning>\n",
            "To prove that for any natural numbers \\( a \\) and \\( b \\) (with \\( b > 0 \\)), \\( a \\div b \\times b \\leq a \\):\n",
            "\n",
            "1. **Understand Integer Division**: \\( a \\div b \\) gives the quotient \\( q \\), which is the largest integer such that \\( q \\times b \\leq a \\).\n",
            "\n",
            "2. **Express \\( a \\) in Terms of \\( q \\) and \\( b \\)**: \\( a = q \\times b + r \\), where \\( 0 \\leq r < b \\) is the remainder.\n",
            "\n",
            "3. **Multiply Quotient by \\( b \\)**: \\( q \\times b = a - r \\).\n",
            "\n",
            "4. **Establish the Inequality**: Since \\( r \\geq 0 \\), \\( a - r \\leq a \\).\n",
            "\n",
            "5. **Conclude**: \\( q \\times b \\leq a \\).\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "``` \n",
            "isabelle \n",
            "lemma \"‚àÄ a b::nat. b > 0 ‚ü∂ (a div b) * b ‚â§ a\" \n",
            "by (induction b) \n",
            " (auto intro: le_add_comm le_of_lt)\n",
            "``` \n",
            "</answer>\n",
            "``` \n",
            "Extracted:\n",
            "None\n",
            "-------------------- Question:\n",
            "The lemma `if_evrv` can be translated into the following natural language statement:\n",
            "\n",
            "Suppose we have two conditions:\n",
            "1. If the boolean condition `b` is true, then the function `f` satisfies the property `equiv_valid_rv_inv` with respect to the invariant `I`, the set of actions `A`, the relation `R`, and the precondition `P`.\n",
            "2. If the boolean condition `b` is false, then the function `g` satisfies the property `equiv_valid_rv_inv` with respect to the invariant `I`, the set of actions `A`, the relation `R`, and the precondition `Q`.\n",
            "\n",
            "Under these assumptions, the function that chooses `f` if `b` is true and `g` if `b` is false satisfies the property `equiv_valid_rv_inv` with respect to the invariant `I`, the set of actions `A`, the relation `R`, and the combined precondition that requires `P` to hold when `b` is true and `Q` to hold when `b` is false. \n",
            "Answer:\n",
            "lemma if_evrv:\n",
            "  assumes \"b \\<Longrightarrow> equiv_valid_rv_inv I A R P f\"\n",
            "  assumes \"\\<not> b \\<Longrightarrow> equiv_valid_rv_inv I A R Q g\"\n",
            "  shows \"equiv_valid_rv_inv I A R (\\<lambda>s. (b \\<longrightarrow> P s) \\<and> (\\<not>b \\<longrightarrow> Q s)) (if b then f else g)\"\n",
            "proof (cases b)\n",
            "  case True\n",
            "  then have \"equiv_valid_rv_inv I A R P f\" using assms(1) by simp\n",
            "  moreover have \"(\\<lambda>s. (True \\<longrightarrow> P s) \\<and> (False \\<longrightarrow> Q s)) = P\" by auto\n",
            "  ultimately show ?thesis by simp\n",
            "next\n",
            "  case False\n",
            "  then have \"equiv_valid_rv_inv I A R Q g\" using assms(2) by simp\n",
            "  moreover have \"(\\<lambda>s. (False \\<longrightarrow> P s) \\<and> (True \\<longrightarrow> Q s)) = Q\" by auto\n",
            "  ultimately show ?thesis by simp\n",
            "qed \n",
            "Response:\n",
            "Okay, so I have this lemma called `if_evrv` that I need to prove using Isabelle. I'm a bit new to this, but I'll try to figure it out step by step.\n",
            "\n",
            "First, I need to understand what the lemma is about. From the code snippet I see that it involves boolean conditions and functions that satisfy certain properties. It looks like the lemma is about choosing between two functions, f and g, based on the value of a boolean condition b. If b is true, it uses f; if b is false, it uses g. I need to show that the resulting function still satisfies some kind of equivalence or validity property.\n",
            "\n",
            "In the code, there are two assumptions: `h1` and `h2`. h1 says that if b is true, then f satisfies `equiv_valid_rv_inv` with some arguments. Similarly, h2 says that if b is false, then g satisfies the same property. I need to combine these to show that the function that selects f or g based on b also satisfies `equiv_valid_rv_inv`.\n",
            "\n",
            "I think the key here is to use the logical structure of the boolean condition b. Since b can only be true or false, the function's behavior is determined by which function we pick in each case. So, if I can show that in both scenarios (b true and b false), the resulting function behaves correctly, then overall it should hold.\n",
            "\n",
            "Maybe I can use case analysis on b. If b is true, then the function is f, and I apply h1 directly. If b is false, the function is g, and I apply h2. Since in both cases, the properties hold, combining them should give the desired result.\n",
            "\n",
            "I also notice that `equiv_valid_rv_inv` is being applied with the same setup except for the precondition. So, when b is true, the precondition is P, and when it's false, it's Q. The combined precondition makes sure both are satisfied.\n",
            "\n",
            "I need to make sure that I correctly combine these properties. Maybe using logical disjunction because the conditions are based on b. If b is true, P holds, and if b is false, Q holds. So, the combined precondition is (P ‚àß b) ‚à® (Q ‚àß ¬¨b).\n",
            "\n",
            "Putting it all together, I think the proof structure should involve assuming b and proving the property using h1 or h2 depending on the case. Then, since in both cases the property holds, the theorem is proven.\n",
            "\n",
            "I should also check if there are any implicit assumptions or if I need to handle any other aspects in Isabelle, like closing assumptions or using specific tactics. Maybe I need to use `cases` to handle the boolean b and then apply the respective assumptions.\n",
            "\n",
            "Overall, the approach seems to be to use case analysis on b, apply the corresponding function that satisfies the property in each case, and then combine the results. I need to structure this carefully in Isabelle to make sure the proof is correct and covers all scenarios.\n",
            "</think>\n",
            "\n",
            "<reasoning>\n",
            "To prove the `if_evrv` lemma, we perform a case analysis on the boolean condition `b`. If `b` is `true`, the function `f` must satisfy the `equiv_valid_rv_inv` property. If `b` is `false`, the function `g` must satisfy the property. By ensuring that the combined function (which selects `f` or `g` based on `b`) satisfies `equiv_valid_rv_inv` under both cases, we establish the lemma's validity.\n",
            "</reasoning>\n",
            "\n",
            "```isabelle\n",
            "lemma \"if_evrv P b f Q g <-> (b --> P) & (~b --> Q)\" \n",
            "  (is \"_ <-> ?R\")\n",
            "  apply Iff.intro \n",
            "  apply simp_all\n",
            "  apply rule\n",
            "  apply intro\n",
            "  apply IffD1\n",
            "  apply conjI\n",
            "-- Case analysis on the boolean condition b\n",
            "  apply (case b in <;> simp_all)\n",
            "  ¬∑ apply IffD1\n",
            "    apply conjI\n",
            "    ¬∑ apply IffD1\n",
            "      apply (drule h1, simp_all)\n",
            "    ¬∑ apply IffD1\n",
            "      apply (drule h2, simp_all)\n",
            "  apply IffD2\n",
            "  apply conjI\n",
            "  ¬∑ apply IffD1\n",
            "    apply (drule h1, simp_all)\n",
            "  ¬∑ apply IffD1\n",
            "    apply (drule h2, simp_all)\n",
            "``` \n",
            "Extracted:\n",
            "lemma \"if_evrv P b f Q g <-> (b --> P) & (~b --> Q)\" \n",
            "  (is \"_ <-> ?R\")\n",
            "  apply Iff.intro \n",
            "  apply simp_all\n",
            "  apply rule\n",
            "  apply intro\n",
            "  apply IffD1\n",
            "  apply conjI\n",
            "-- Case analysis on the boolean condition b\n",
            "  apply (case b in <;> simp_all)\n",
            "  ¬∑ apply IffD1\n",
            "    apply conjI\n",
            "    ¬∑ apply IffD1\n",
            "      apply (drule h1, simp_all)\n",
            "    ¬∑ apply IffD1\n",
            "      apply (drule h2, simp_all)\n",
            "  apply IffD2\n",
            "  apply conjI\n",
            "  ¬∑ apply IffD1\n",
            "    apply (drule h1, simp_all)\n",
            "  ¬∑ apply IffD1\n",
            "    apply (drule h2, simp_all)\n"
          ]
        }
      ],
      "source": [
        "sys.path.append('../')\n",
        "os.environ['PISA_PATH'] = '/home/siai/Portal-to-ISAbelle/src/main/python'\n",
        "\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "        checker_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "qtcz_lpbVC92",
        "outputId": "9b12655a-7905-42a8-d6f0-210ff74a6d73"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "zf_OY5WMVOxF",
        "outputId": "c34d81a7-192d-427d-81f0-cbca7009b7d1"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDgFfhFYIAS"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n33Lwz_eqG4Z"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ca67b0c4ca64eb788358a51308f6b97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac03aff5c314b00ac938c80eb7b2f8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c522d78b1834068bd4b155d0f87a4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca67b0c4ca64eb788358a51308f6b97",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83c3c811923a4642aba156d1215b39d2",
            "value": 1
          }
        },
        "3febcf8a8eca40c28aafc697f3ec8776": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f3d96b613e94e9984d4599ca9ca7b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea549fffa8c2469888d1668158bc105c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_98b432b98839428f85d91580c21e80e2",
            "value": ""
          }
        },
        "6221f0be3b8d48e797c873565a216680": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66c3271554b1455eb56be55c9241e45e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fee4f852c9744a07b909e586e3615604",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3febcf8a8eca40c28aafc697f3ec8776",
            "value": 1
          }
        },
        "697faad6643a43aca98015da4faef186": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "769bde36e2ba4434bddd78e7d5911be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ac03aff5c314b00ac938c80eb7b2f8a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_88c63d94a05a42c49d5f8958a27987a6",
            "value": ""
          }
        },
        "83c3c811923a4642aba156d1215b39d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88c63d94a05a42c49d5f8958a27987a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94873c3c077e483790b34f95c421f484": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98b432b98839428f85d91580c21e80e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a23afba19c2a4d3a90d771fc55f8d490": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e863bf099e064da7b482c21fe7b77de7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_697faad6643a43aca98015da4faef186",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:16&lt;00:00,‚Äá16.13s/it]\n"
          }
        },
        "b4e1eb8eeb064c88a2142e474fb8327f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d36b61cf796c429080e93ea838a3759e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4e1eb8eeb064c88a2142e474fb8327f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_da10502506f9448c9de94f1ddd84d3b1",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:27&lt;00:00,‚Äá27.50s/it]\n"
          }
        },
        "d8d0dca36cfc47f0919924da07c231e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f3d96b613e94e9984d4599ca9ca7b17",
              "IPY_MODEL_66c3271554b1455eb56be55c9241e45e",
              "IPY_MODEL_d36b61cf796c429080e93ea838a3759e"
            ],
            "layout": "IPY_MODEL_94873c3c077e483790b34f95c421f484"
          }
        },
        "da10502506f9448c9de94f1ddd84d3b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6cc388e78c14abfaa49d2be6fa1b5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_769bde36e2ba4434bddd78e7d5911be4",
              "IPY_MODEL_3c522d78b1834068bd4b155d0f87a4d7",
              "IPY_MODEL_a23afba19c2a4d3a90d771fc55f8d490"
            ],
            "layout": "IPY_MODEL_6221f0be3b8d48e797c873565a216680"
          }
        },
        "e863bf099e064da7b482c21fe7b77de7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea549fffa8c2469888d1668158bc105c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fee4f852c9744a07b909e586e3615604": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
