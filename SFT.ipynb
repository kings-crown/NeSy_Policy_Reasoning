{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f42729-fa07-4295-9ff2-4947cd0fdade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import torch.distributed\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "EOT_TOKEN = \"<|EOT|>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5816640b-3c21-4574-9031-f612ca4ee24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_instruction_prompt(instruction: str):\n",
    "    return \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant used for generating proofs in Isabelle to prove the provided natural language statements.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\".format(\n",
    "        instruction.strip()\n",
    "    ).lstrip()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: List[str] = field(\n",
    "        default_factory=list, metadata={\"help\": \"Paths to the training data.\"}\n",
    "    )\n",
    "    instruction_field: str = field(\n",
    "        default=\"instruction\", metadata={\"help\": \"The field name for the instruction\"}\n",
    "    )\n",
    "    output_field: str = field(\n",
    "        default=\"output\", metadata={\"help\": \"The field name for the output\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(  # NOTE: ignore this\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b637bb-927e-45b3-bc67-37fb663d0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(\n",
    "    strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer\n",
    ") -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            # max_length=tokenizer.model_max_length,\n",
    "            # truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "        for tokenized in tokenized_list\n",
    "    ]\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [\n",
    "        _tokenize_fn(strings, tokenizer) for strings in (examples, sources)\n",
    "    ]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple(\n",
    "            [instance[key] for instance in instances] for key in (\"input_ids\", \"labels\")\n",
    "        )\n",
    "        input_ids = [torch.tensor(x) for x in input_ids]\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = [torch.tensor(x) for x in labels]\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=IGNORE_INDEX\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d747cd71-316d-4935-8920-dcd056ce161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # Initialize distributed training only if running in distributed mode\n",
    "    if training_args.local_rank != -1 and os.getenv(\"RANK\") is not None:\n",
    "        if torch.distributed.is_available() and not torch.distributed.is_initialized():\n",
    "            torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "\n",
    "    if training_args.local_rank == 0:\n",
    "        print(\"=\" * 100)\n",
    "        print(training_args)\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token \n",
    "    def train_tokenize_function(examples, tokenizer):\n",
    "        sources = [\n",
    "            build_instruction_prompt(instruction)\n",
    "            for instruction in examples[data_args.instruction_field]\n",
    "        ]\n",
    "        targets = [\n",
    "            f\"{output.strip()}{tokenizer.eos_token}\"\n",
    "            for output in examples[data_args.output_field]\n",
    "        ]\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "        return data_dict\n",
    "\n",
    "    print(\"PAD Token:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "    print(\"BOS Token\", tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "    print(\"EOS Token\", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "\n",
    "    if training_args.local_rank == 0:\n",
    "        print(\"Load tokenizer from {} over.\".format(model_args.model_name_or_path))\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path, torch_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    if training_args.local_rank == 0:\n",
    "        print(\"Load model from {} over.\".format(model_args.model_name_or_path))\n",
    "\n",
    "    raw_train_datasets = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data_args.data_path,\n",
    "        split=\"train\",\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "    \n",
    "    # Use a safe barrier\n",
    "    if training_args.local_rank > 0 and torch.distributed.is_initialized():\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    train_dataset = raw_train_datasets.map(\n",
    "        train_tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=3000,\n",
    "        num_proc=32,\n",
    "        remove_columns=raw_train_datasets.column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running Encoding\",\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    )\n",
    "\n",
    "    if training_args.local_rank == 0 and torch.distributed.is_initialized():\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    if training_args.local_rank == 0:\n",
    "        print(\"Training dataset samples:\", len(train_dataset))\n",
    "        for index in random.sample(range(len(train_dataset)), 3):\n",
    "            print(\n",
    "                f\"Sample {index} of the training set: {train_dataset[index]['input_ids']}, {train_dataset[index]['labels']}.\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Sample {index} of the training set: {tokenizer.decode(list(train_dataset[index]['input_ids']))}.\"\n",
    "            )\n",
    "\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    data_module = dict(\n",
    "        train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, tokenizer=tokenizer, args=training_args, **data_module\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_state()\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68da4bef-ab03-4206-8054-7193db19fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "critic_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "data_path = \"train.jsonl\"\n",
    "output_path = \"Distill-Qwen-1.5B\"\n",
    "\n",
    "sys.argv = [\n",
    "    \"notebook\",\n",
    "    \"--model_name_or_path\", critic_model,\n",
    "    \"--data_path\", data_path,\n",
    "    \"--output_dir\", output_path,\n",
    "    \"--instruction_field\", \"natural_language_statement\",\n",
    "    \"--output_field\", \"formal_proof\",\n",
    "    \"--num_train_epochs\", \"1\",\n",
    "    \"--model_max_length\", \"1024\",\n",
    "    \"--per_device_train_batch_size\", \"4\",\n",
    "    \"--per_device_eval_batch_size\", \"1\",\n",
    "    \"--gradient_accumulation_steps\", \"8\",\n",
    "    \"--eval_strategy\", \"no\",\n",
    "    \"--save_strategy\", \"steps\",\n",
    "    \"--save_steps\", \"10\",\n",
    "    \"--save_total_limit\", \"5\",\n",
    "    \"--learning_rate\", \"1e-5\",\n",
    "    \"--warmup_steps\", \"0\",\n",
    "    \"--logging_steps\", \"1\",\n",
    "    \"--lr_scheduler_type\", \"cosine\",\n",
    "    \"--gradient_checkpointing\", \"True\",\n",
    "    \"--report_to\", \"wandb\",\n",
    "    \"--bf16\", \"True\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10016b8e-e838-4df0-90c2-33465666d621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "cache_dir=None,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=Distill-Qwen-1.5B/runs/Feb24_15-50-03_ssegpt,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "model_max_length=1024,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=Distill-Qwen-1.5B,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=Distill-Qwen-1.5B,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=10,\n",
      "save_strategy=steps,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "PAD Token: <｜end▁of▁sentence｜> 100001\n",
      "BOS Token <｜begin▁of▁sentence｜> 100000\n",
      "EOS Token <｜end▁of▁sentence｜> 100001\n",
      "Load tokenizer from deepseek-ai/DeepSeek-Prover-V1.5-Base over.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607c0e94e9fc4c2d8560207a80394307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from deepseek-ai/DeepSeek-Prover-V1.5-Base over.\n",
      "Training dataset samples: 200\n",
      "Sample 134 of the training set: [100000, 27, 91, 309, 62, 4789, 66325, 6713, 185, 2054, 418, 1551, 20881, 11, 4015, 457, 92632, 15895, 13, 1257, 418, 245, 9394, 20308, 1222, 327, 17209, 28489, 279, 98907, 276, 6650, 254, 4286, 3892, 4706, 12838, 17790, 91, 309, 62, 409, 66325, 185, 27, 91, 309, 62, 4789, 66325, 3631, 185, 549, 19407, 440, 29280, 17, 1, 481, 330, 23351, 881, 3892, 4706, 372, 4446, 25, 1273, 12837, 252, 40616, 317, 5929, 276, 12837, 244, 357, 654, 12837, 2644, 40616, 317, 5929, 276, 12837, 353, 357, 654, 285, 254, 31583, 12837, 375, 40616, 7432, 327, 12837, 252, 40616, 285, 12837, 2644, 357, 654, 937, 254, 31583, 12837, 375, 40616, 839, 7432, 327, 12837, 244, 40616, 285, 12837, 353, 357, 633, 27, 91, 309, 62, 409, 66325, 185, 27, 91, 309, 62, 4789, 66325, 81038, 185, 1898, 15255, 254, 24937, 3418, 881, 245, 26932, 98907, 5637, 11, 395, 543, 1181, 254, 5610, 28526, 279, 254, 24937, 5637, 285, 938, 98907, 6, 82, 5637, 4786, 11, 2847, 2030, 82, 3761, 53473, 12181, 276, 47521, 4373, 280, 254, 5637, 1066, 13762, 13, 4462, 317, 254, 26932, 98907, 5637, 327, 254, 19407, 2030, 29280, 17, 33244, 34083, 262, 1039, 275, 19407, 4896, 17, 25, 12287, 27, 75, 5272, 27825, 29, 82, 403, 244, 26, 2644, 403, 353, 26, 375, 252, 2644, 59, 27, 81, 5272, 27825, 29, 97970, 91147, 29, 375, 244, 353, 1, 5637, 570, 6049, 16577, 16, 25, 440, 82, 403, 244, 1, 6049, 16577, 17, 25, 440, 84, 403, 353, 1, 6049, 3789, 25, 440, 47, 252, 2644, 1, 473, 16577, 16, 463, 440, 83, 403, 252, 1, 457, 334, 8928, 4300, 8, 473, 16577, 17, 463, 440, 85, 403, 2644, 1, 457, 334, 8928, 4300, 8, 473, 437, 285, 3789, 463, 440, 47, 252, 353, 1, 457, 334, 8928, 4896, 8, 473, 2030, 83, 403, 252, 63, 285, 2030, 47, 252, 353, 63, 463, 440, 47, 244, 353, 1, 457, 334, 8928, 4896, 8, 4117, 440, 47, 244, 353, 1, 1021, 4662, 271, 34083, 49757, 2078, 43769, 25, 207, 16, 13, 9217, 5630, 64447, 59668, 1003, 1330, 457, 13613, 254, 30281, 280, 254, 19407, 25, 570, 2030, 1187, 16, 25, 252, 403, 244, 63, 570, 2030, 1187, 17, 25, 2644, 403, 353, 63, 570, 2030, 14756, 25, 375, 252, 2644, 63, 207, 17, 13, 9217, 50, 22053, 280, 81306, 59668, 1003, 938, 254, 12754, 280, 17940, 276, 29948, 2030, 82, 403, 244, 63, 372, 2030, 83, 403, 252, 63, 285, 2030, 84, 403, 353, 63, 372, 2030, 85, 403, 2644, 13557, 1002, 317, 2368, 1244, 254, 2030, 16778, 63, 6290, 13, 207, 18, 13, 9217, 42006, 6157, 59668, 570, 5904, 11, 395, 22463, 2030, 85, 63, 327, 2030, 84, 63, 279, 254, 31583, 2030, 47, 252, 2644, 63, 276, 752, 2030, 47, 252, 353, 13557, 1002, 317, 2368, 1244, 254, 2030, 29280, 63, 6290, 13, 570, 2928, 11, 395, 22463, 2030, 83, 63, 327, 2030, 82, 63, 279, 2030, 47, 252, 353, 63, 276, 752, 2030, 47, 244, 353, 13557, 207, 19, 13, 9217, 30179, 59668, 12226, 11, 395, 14193, 344, 2030, 47, 244, 353, 63, 7432, 11, 23241, 254, 5637, 13, 1002, 26932, 5637, 11489, 4446, 254, 24937, 22834, 285, 5131, 98907, 6, 82, 5637, 30127, 276, 6428, 254, 89326, 13, 429, 938, 280, 2030, 16778, 63, 285, 2030, 29280, 63, 6543, 4723, 12777, 276, 254, 14502, 5610, 280, 53873, 5929, 3769, 13, 100001], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1898, 15255, 254, 24937, 3418, 881, 245, 26932, 98907, 5637, 11, 395, 543, 1181, 254, 5610, 28526, 279, 254, 24937, 5637, 285, 938, 98907, 6, 82, 5637, 4786, 11, 2847, 2030, 82, 3761, 53473, 12181, 276, 47521, 4373, 280, 254, 5637, 1066, 13762, 13, 4462, 317, 254, 26932, 98907, 5637, 327, 254, 19407, 2030, 29280, 17, 33244, 34083, 262, 1039, 275, 19407, 4896, 17, 25, 12287, 27, 75, 5272, 27825, 29, 82, 403, 244, 26, 2644, 403, 353, 26, 375, 252, 2644, 59, 27, 81, 5272, 27825, 29, 97970, 91147, 29, 375, 244, 353, 1, 5637, 570, 6049, 16577, 16, 25, 440, 82, 403, 244, 1, 6049, 16577, 17, 25, 440, 84, 403, 353, 1, 6049, 3789, 25, 440, 47, 252, 2644, 1, 473, 16577, 16, 463, 440, 83, 403, 252, 1, 457, 334, 8928, 4300, 8, 473, 16577, 17, 463, 440, 85, 403, 2644, 1, 457, 334, 8928, 4300, 8, 473, 437, 285, 3789, 463, 440, 47, 252, 353, 1, 457, 334, 8928, 4896, 8, 473, 2030, 83, 403, 252, 63, 285, 2030, 47, 252, 353, 63, 463, 440, 47, 244, 353, 1, 457, 334, 8928, 4896, 8, 4117, 440, 47, 244, 353, 1, 1021, 4662, 271, 34083, 49757, 2078, 43769, 25, 207, 16, 13, 9217, 5630, 64447, 59668, 1003, 1330, 457, 13613, 254, 30281, 280, 254, 19407, 25, 570, 2030, 1187, 16, 25, 252, 403, 244, 63, 570, 2030, 1187, 17, 25, 2644, 403, 353, 63, 570, 2030, 14756, 25, 375, 252, 2644, 63, 207, 17, 13, 9217, 50, 22053, 280, 81306, 59668, 1003, 938, 254, 12754, 280, 17940, 276, 29948, 2030, 82, 403, 244, 63, 372, 2030, 83, 403, 252, 63, 285, 2030, 84, 403, 353, 63, 372, 2030, 85, 403, 2644, 13557, 1002, 317, 2368, 1244, 254, 2030, 16778, 63, 6290, 13, 207, 18, 13, 9217, 42006, 6157, 59668, 570, 5904, 11, 395, 22463, 2030, 85, 63, 327, 2030, 84, 63, 279, 254, 31583, 2030, 47, 252, 2644, 63, 276, 752, 2030, 47, 252, 353, 13557, 1002, 317, 2368, 1244, 254, 2030, 29280, 63, 6290, 13, 570, 2928, 11, 395, 22463, 2030, 83, 63, 327, 2030, 82, 63, 279, 2030, 47, 252, 353, 63, 276, 752, 2030, 47, 244, 353, 13557, 207, 19, 13, 9217, 30179, 59668, 12226, 11, 395, 14193, 344, 2030, 47, 244, 353, 63, 7432, 11, 23241, 254, 5637, 13, 1002, 26932, 5637, 11489, 4446, 254, 24937, 22834, 285, 5131, 98907, 6, 82, 5637, 30127, 276, 6428, 254, 89326, 13, 429, 938, 280, 2030, 16778, 63, 285, 2030, 29280, 63, 6543, 4723, 12777, 276, 254, 14502, 5610, 280, 53873, 5929, 3769, 13, 100001].\n",
      "Sample 134 of the training set: <｜begin▁of▁sentence｜><|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant used for generating proofs in Isabelle to prove the provided natural language statements.<|im_end|>\n",
      "<|im_start|>user\n",
      "The lemma \"subst2\" can be translated into natural language as follows: If \\( s \\) is equal to \\( t \\), \\( u \\) is equal to \\( v \\), and the predicate \\( P \\) holds for \\( s \\) and \\( u \\), then the predicate \\( P \\) also holds for \\( t \\) and \\( v \\).<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To translate the informal solution into a structured Isabelle proof, we will follow the steps outlined in the informal proof and use Isabelle's proof methods, including `sledgehammer`, to automate parts of the proof where applicable. Here is the structured Isabelle proof for the lemma `subst2`: ```isabelle lemma subst2: \"\\<lbrakk>s = t; u = v; P s u\\<rbrakk> \\<Longrightarrow> P t v\" proof - assume eq1: \"s = t\" assume eq2: \"u = v\" assume pred: \"P s u\" from eq1 have \"t = s\" by (rule sym) from eq2 have \"v = u\" by (rule sym) from this and pred have \"P s v\" by (rule subst) from `t = s` and `P s v` have \"P t v\" by (rule subst) thus \"P t v\" . qed ``` ### Explanation: 1. **Assumptions**: We start by assuming the premises of the lemma: - `eq1: s = t` - `eq2: u = v` - `pred: P s u` 2. **Symmetry of Equality**: We use the symmetry of equality to rewrite `s = t` as `t = s` and `u = v` as `v = u`. This is done using the `sym` rule. 3. **Substitution**: - First, we substitute `v` for `u` in the predicate `P s u` to get `P s v`. This is done using the `subst` rule. - Then, we substitute `t` for `s` in `P s v` to get `P t v`. 4. **Conclusion**: Finally, we conclude that `P t v` holds, completing the proof. This structured proof closely follows the informal reasoning and uses Isabelle's proof automation to handle the substitutions. The use of `sym` and `subst` rules directly corresponds to the logical steps of substituting equal terms.<｜end▁of▁sentence｜>.\n",
      "Sample 154 of the training set: [100000, 27, 91, 309, 62, 4789, 66325, 6713, 185, 2054, 418, 1551, 20881, 11, 4015, 457, 92632, 15895, 13, 1257, 418, 245, 9394, 20308, 1222, 327, 17209, 28489, 279, 98907, 276, 6650, 254, 4286, 3892, 4706, 12838, 17790, 91, 309, 62, 409, 66325, 185, 27, 91, 309, 62, 4789, 66325, 3631, 185, 549, 19407, 2030, 15478, 62, 12866, 62, 4092, 63, 4605, 344, 565, 340, 1529, 254, 1022, 2030, 4092, 61069, 63, 4899, 473, 254, 1525, 2030, 886, 63, 285, 937, 18276, 359, 366, 254, 1525, 2030, 17048, 12181, 254, 1230, 317, 254, 1246, 372, 1677, 12490, 2030, 17048, 63, 4723, 366, 2030, 886, 13557, 685, 20666, 3769, 11, 754, 1677, 12490, 984, 11971, 11, 565, 340, 5462, 344, 254, 1864, 1525, 317, 430, 2754, 372, 1234, 372, 254, 1022, 457, 3950, 889, 372, 1313, 4899, 372, 254, 1022, 1525, 643, 11, 254, 1677, 12490, 6225, 543, 6915, 254, 1246, 1230, 372, 565, 340, 661, 1677, 10157, 254, 984, 11971, 1673, 21814, 17790, 91, 309, 62, 409, 66325, 185, 27, 91, 309, 62, 4789, 66325, 81038, 185, 1898, 15255, 254, 24937, 3418, 881, 245, 26932, 98907, 5637, 11, 395, 543, 1181, 254, 22834, 4286, 285, 938, 98907, 6, 82, 5637, 6164, 11, 2847, 2030, 82, 3761, 53473, 12181, 276, 4750, 279, 7526, 254, 4374, 5610, 13, 4462, 6, 82, 946, 340, 481, 4327, 254, 5637, 25, 34083, 262, 1039, 275, 19407, 18276, 62, 12866, 62, 4092, 25, 440, 15478, 61069, 334, 12866, 334, 4092, 61069, 8, 320, 82, 8, 403, 18276, 61069, 320, 82, 1, 5637, 334, 515, 13023, 61069, 12954, 25, 320, 82, 8, 1460, 47767, 937, 1296, 3025, 5879, 457, 1115, 79, 2112, 1460, 334, 8458, 1376, 61069, 8, 937, 1296, 3025, 5879, 5637, 334, 11995, 320, 82, 8, 1460, 47767, 937, 1296, 3025, 672, 16243, 457, 1115, 79, 2112, 1460, 334, 8458, 320, 320, 82, 2519, 937, 463, 440, 15478, 334, 87, 1501, 61069, 8, 334, 12866, 334, 4092, 334, 87, 1501, 61069, 1509, 334, 88, 1501, 320, 82, 14201, 403, 334, 87, 11, 320, 8, 1501, 18276, 61069, 334, 12866, 334, 4092, 61069, 8, 320, 82, 85622, 457, 1115, 79, 839, 463, 42202, 403, 334, 87, 11, 320, 8, 1501, 18276, 61069, 334, 88, 1501, 320, 82, 85622, 1244, 4926, 13, 84805, 457, 1115, 79, 6334, 1296, 3025, 672, 16243, 457, 1115, 79, 4662, 271, 4662, 271, 34083, 49757, 2078, 43769, 25, 207, 16, 13, 9217, 3296, 13023, 331, 2030, 17048, 63, 59668, 1003, 2111, 22614, 331, 254, 1525, 2030, 17048, 13557, 1002, 317, 245, 3064, 11996, 327, 1525, 12, 10652, 28489, 279, 98907, 13, 207, 17, 13, 9217, 7182, 15256, 21968, 51250, 25584, 59668, 2473, 2030, 17048, 63, 317, 7137, 21968, 51250, 44047, 1572, 9226, 280, 254, 6337, 418, 2030, 4818, 12181, 558, 254, 5637, 317, 17197, 1244, 2030, 3392, 79, 13557, 207, 18, 13, 9217, 3296, 53020, 10925, 21968, 8458, 25584, 59668, 1494, 254, 38865, 3458, 11, 395, 6049, 254, 19407, 7432, 327, 2030, 17048, 63, 285, 6650, 359, 327, 2030, 87, 1501, 61069, 13557, 207, 19, 13, 9217, 9688, 19347, 331, 2030, 886, 63, 59668, 1003, 2111, 245, 1460, 4751, 331, 2030, 886, 63, 276, 6428, 254, 5437, 1066, 2030, 886, 63, 1667, 330, 7137, 410, 2170, 12, 10506, 13, 570, 9217, 9688, 2030, 886, 403, 47767, 63, 59668, 1273, 2030, 886, 63, 317, 7137, 11, 1572, 9226, 280, 254, 6337, 418, 2030, 4818, 12181, 558, 254, 5637, 317, 17236, 1244, 2030, 3392, 79, 13557, 570, 9217, 9688, 2030, 886, 403, 4926, 320, 320, 82, 6, 63, 59668, 1273, 2030, 886, 63, 317, 2170, 12, 10506, 11, 395, 27734, 1572, 9226, 280, 254, 6337, 13, 429, 1022, 3699, 2030, 7, 87, 11, 320, 64166, 317, 31170, 11, 285, 395, 4359, 254, 22614, 18852, 276, 254, 1610, 280, 254, 11971, 13, 207, 20, 13, 9217, 9136, 280, 2030, 82, 3761, 53473, 63, 59668, 685, 437, 5637, 11, 2030, 82, 3761, 53473, 63, 481, 330, 1222, 276, 3128, 254, 938, 280, 2030, 3392, 79, 63, 285, 750, 32513, 11, 4398, 279, 254, 38865, 3458, 1066, 7667, 8445, 418, 4067, 13, 1002, 26932, 5637, 4446, 254, 24937, 22834, 11489, 11, 19888, 344, 1319, 3458, 317, 29313, 285, 3662, 13, 100001], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1898, 15255, 254, 24937, 3418, 881, 245, 26932, 98907, 5637, 11, 395, 543, 1181, 254, 22834, 4286, 285, 938, 98907, 6, 82, 5637, 6164, 11, 2847, 2030, 82, 3761, 53473, 12181, 276, 4750, 279, 7526, 254, 4374, 5610, 13, 4462, 6, 82, 946, 340, 481, 4327, 254, 5637, 25, 34083, 262, 1039, 275, 19407, 18276, 62, 12866, 62, 4092, 25, 440, 15478, 61069, 334, 12866, 334, 4092, 61069, 8, 320, 82, 8, 403, 18276, 61069, 320, 82, 1, 5637, 334, 515, 13023, 61069, 12954, 25, 320, 82, 8, 1460, 47767, 937, 1296, 3025, 5879, 457, 1115, 79, 2112, 1460, 334, 8458, 1376, 61069, 8, 937, 1296, 3025, 5879, 5637, 334, 11995, 320, 82, 8, 1460, 47767, 937, 1296, 3025, 672, 16243, 457, 1115, 79, 2112, 1460, 334, 8458, 320, 320, 82, 2519, 937, 463, 440, 15478, 334, 87, 1501, 61069, 8, 334, 12866, 334, 4092, 334, 87, 1501, 61069, 1509, 334, 88, 1501, 320, 82, 14201, 403, 334, 87, 11, 320, 8, 1501, 18276, 61069, 334, 12866, 334, 4092, 61069, 8, 320, 82, 85622, 457, 1115, 79, 839, 463, 42202, 403, 334, 87, 11, 320, 8, 1501, 18276, 61069, 334, 88, 1501, 320, 82, 85622, 1244, 4926, 13, 84805, 457, 1115, 79, 6334, 1296, 3025, 672, 16243, 457, 1115, 79, 4662, 271, 4662, 271, 34083, 49757, 2078, 43769, 25, 207, 16, 13, 9217, 3296, 13023, 331, 2030, 17048, 63, 59668, 1003, 2111, 22614, 331, 254, 1525, 2030, 17048, 13557, 1002, 317, 245, 3064, 11996, 327, 1525, 12, 10652, 28489, 279, 98907, 13, 207, 17, 13, 9217, 7182, 15256, 21968, 51250, 25584, 59668, 2473, 2030, 17048, 63, 317, 7137, 21968, 51250, 44047, 1572, 9226, 280, 254, 6337, 418, 2030, 4818, 12181, 558, 254, 5637, 317, 17197, 1244, 2030, 3392, 79, 13557, 207, 18, 13, 9217, 3296, 53020, 10925, 21968, 8458, 25584, 59668, 1494, 254, 38865, 3458, 11, 395, 6049, 254, 19407, 7432, 327, 2030, 17048, 63, 285, 6650, 359, 327, 2030, 87, 1501, 61069, 13557, 207, 19, 13, 9217, 9688, 19347, 331, 2030, 886, 63, 59668, 1003, 2111, 245, 1460, 4751, 331, 2030, 886, 63, 276, 6428, 254, 5437, 1066, 2030, 886, 63, 1667, 330, 7137, 410, 2170, 12, 10506, 13, 570, 9217, 9688, 2030, 886, 403, 47767, 63, 59668, 1273, 2030, 886, 63, 317, 7137, 11, 1572, 9226, 280, 254, 6337, 418, 2030, 4818, 12181, 558, 254, 5637, 317, 17236, 1244, 2030, 3392, 79, 13557, 570, 9217, 9688, 2030, 886, 403, 4926, 320, 320, 82, 6, 63, 59668, 1273, 2030, 886, 63, 317, 2170, 12, 10506, 11, 395, 27734, 1572, 9226, 280, 254, 6337, 13, 429, 1022, 3699, 2030, 7, 87, 11, 320, 64166, 317, 31170, 11, 285, 395, 4359, 254, 22614, 18852, 276, 254, 1610, 280, 254, 11971, 13, 207, 20, 13, 9217, 9136, 280, 2030, 82, 3761, 53473, 63, 59668, 685, 437, 5637, 11, 2030, 82, 3761, 53473, 63, 481, 330, 1222, 276, 3128, 254, 938, 280, 2030, 3392, 79, 63, 285, 750, 32513, 11, 4398, 279, 254, 38865, 3458, 1066, 7667, 8445, 418, 4067, 13, 1002, 26932, 5637, 4446, 254, 24937, 22834, 11489, 11, 19888, 344, 1319, 3458, 317, 29313, 285, 3662, 13, 100001].\n",
      "Sample 154 of the training set: <｜begin▁of▁sentence｜><|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant used for generating proofs in Isabelle to prove the provided natural language statements.<|im_end|>\n",
      "<|im_start|>user\n",
      "The lemma `zip_take_length` states that if you take the first `length xs` elements from the list `ys` and then zip it with the list `xs`, the result is the same as zipping `xs` directly with `ys`. In simpler terms, when zipping two lists, if you ensure that the second list is at least as long as the first by taking only as many elements as the first list has, the zipping operation will yield the same result as if you had zipped the two lists without modification.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To translate the informal solution into a structured Isabelle proof, we will follow the reasoning provided and use Isabelle's proof tools, including `sledgehammer`, to assist in finding the necessary steps. Here's how you can structure the proof: ```isabelle lemma zip_take_length: \"zip xs (take (length xs) ys) = zip xs ys\" proof (induction xs arbitrary: ys) case Nil then show ?case by simp next case (Cons x xs) then show ?case proof (cases ys) case Nil then show ?thesis by simp next case (Cons y ys') then have \"zip (x # xs) (take (length (x # xs)) (y # ys')) = (x, y) # zip xs (take (length xs) ys')\" by simp also have \"... = (x, y) # zip xs (y # ys')\" using Cons.IH by simp finally show ?thesis by simp qed qed ``` ### Explanation: 1. **Induction on `xs`**: We perform induction on the list `xs`. This is a common technique for list-related proofs in Isabelle. 2. **Base Case (`Nil`)**: When `xs` is empty (`Nil`), both sides of the equation are `[]`, so the proof is trivial using `simp`. 3. **Inductive Step (`Cons`)**: For the inductive step, we assume the lemma holds for `xs` and prove it for `x # xs`. 4. **Case Analysis on `ys`**: We perform a case analysis on `ys` to handle the situation where `ys` might be empty or non-empty. - **Case `ys = Nil`**: If `ys` is empty, both sides of the equation are `[]`, so the proof is straightforward using `simp`. - **Case `ys = Cons y ys'`**: If `ys` is non-empty, we simplify both sides of the equation. The first element `(x, y)` is paired, and we apply the induction hypothesis to the rest of the lists. 5. **Use of `sledgehammer`**: In this proof, `sledgehammer` can be used to suggest the use of `simp` and other tactics, especially in the inductive step where simplifications are needed. This structured proof follows the informal reasoning closely, ensuring that each step is justified and clear.<｜end▁of▁sentence｜>.\n",
      "Sample 182 of the training set: [100000, 27, 91, 309, 62, 4789, 66325, 6713, 185, 2054, 418, 1551, 20881, 11, 4015, 457, 92632, 15895, 13, 1257, 418, 245, 9394, 20308, 1222, 327, 17209, 28489, 279, 98907, 276, 6650, 254, 4286, 3892, 4706, 12838, 17790, 91, 309, 62, 409, 66325, 185, 27, 91, 309, 62, 4789, 66325, 3631, 185, 549, 19407, 2030, 2502, 62, 436, 17, 62, 30785, 63, 481, 330, 23351, 881, 3892, 4706, 372, 4446, 25, 16199, 984, 11971, 2030, 281, 63, 285, 2030, 5508, 12181, 565, 327, 1131, 5939, 5696, 280, 4899, 2030, 7, 64, 11, 258, 64166, 473, 1069, 11971, 11, 254, 31583, 2030, 47, 63, 7432, 21968, 2502, 62, 436, 17, 375, 372, 32144, 44047, 285, 254, 31583, 2030, 48, 63, 839, 7432, 21968, 2502, 62, 436, 17, 1551, 372, 32144, 44047, 937, 327, 1131, 5939, 5696, 280, 4899, 2030, 7, 64, 11, 258, 8, 12181, 254, 23201, 280, 71863, 2030, 47, 63, 285, 2030, 48, 63, 7432, 21968, 2502, 62, 436, 17, 2549, 27, 2229, 29, 64, 270, 13, 375, 245, 270, 97970, 384, 29, 1551, 245, 270, 8, 372, 32144, 63, 633, 27, 91, 309, 62, 409, 66325, 185, 27, 91, 309, 62, 4789, 66325, 81038, 185, 1898, 15255, 254, 24937, 3418, 881, 245, 26932, 98907, 5637, 11, 395, 543, 1181, 254, 5610, 28526, 279, 254, 24937, 5637, 285, 938, 98907, 6, 82, 5637, 4786, 276, 8970, 945, 359, 13, 1003, 543, 839, 938, 2030, 82, 3761, 53473, 63, 276, 4750, 279, 7526, 254, 4374, 69289, 410, 10870, 565, 4067, 13, 4462, 6, 82, 946, 254, 26932, 98907, 5637, 1667, 1068, 25, 34083, 262, 1039, 275, 19407, 1525, 62, 436, 17, 62, 30785, 25, 25107, 284, 436, 16, 25, 440, 2502, 62, 436, 17, 375, 372, 32144, 1, 285, 284, 436, 17, 25, 440, 2502, 62, 436, 17, 1551, 372, 32144, 1, 3535, 440, 2502, 62, 436, 17, 2549, 27, 2229, 29, 64, 270, 13, 375, 245, 270, 97970, 384, 29, 1551, 245, 270, 8, 372, 32144, 1, 5637, 570, 473, 284, 436, 16, 284, 436, 17, 1296, 3025, 672, 16243, 5637, 334, 515, 13023, 372, 12954, 25, 32144, 8, 1460, 47767, 937, 1296, 3025, 5879, 457, 1115, 79, 2112, 1460, 334, 8458, 245, 372, 8, 937, 2903, 258, 32144, 6, 1066, 32144, 25, 440, 5508, 403, 258, 1501, 32144, 24451, 285, 375, 25, 440, 47, 245, 258, 1, 285, 1551, 25, 440, 48, 245, 258, 1, 285, 304, 39, 25, 440, 2502, 62, 436, 17, 375, 372, 32144, 24451, 440, 2502, 62, 436, 17, 1551, 372, 32144, 24451, 457, 334, 10860, 1115, 79, 25, 1525, 62, 436, 17, 62, 8458, 16, 8, 937, 463, 440, 2502, 62, 436, 17, 2549, 27, 2229, 29, 64, 270, 13, 375, 245, 270, 97970, 384, 29, 1551, 245, 270, 8, 372, 32144, 24451, 1244, 4926, 13, 84805, 457, 28884, 366, 375, 1551, 32144, 1296, 3025, 5879, 457, 334, 3392, 79, 962, 25, 1525, 62, 436, 17, 62, 8458, 16, 8, 4662, 271, 4662, 271, 34083, 49757, 2078, 43769, 25, 207, 16, 13, 9217, 7182, 15256, 21968, 51250, 63, 1780, 746, 570, 1273, 1572, 11971, 2030, 281, 63, 285, 2030, 5508, 63, 418, 7137, 11, 937, 2030, 2502, 62, 436, 17, 63, 60010, 7432, 327, 688, 31583, 11, 2847, 254, 23201, 280, 2030, 47, 63, 285, 2030, 48, 13557, 1002, 317, 3707, 1244, 2030, 3392, 79, 13557, 207, 17, 13, 9217, 3296, 53020, 10925, 21968, 8458, 63, 1780, 746, 570, 30037, 254, 3587, 7432, 327, 11971, 2030, 281, 63, 285, 2030, 5508, 6, 63, 334, 515, 53020, 18852, 633, 570, 1003, 933, 276, 1296, 359, 7432, 327, 2030, 64, 1501, 372, 63, 285, 2030, 66, 1501, 32144, 6, 13557, 570, 4810, 254, 17325, 2030, 75, 436, 16, 63, 285, 2030, 75, 436, 17, 12181, 395, 1006, 2030, 47, 245, 258, 63, 285, 2030, 48, 245, 258, 63, 2785, 327, 254, 11991, 280, 254, 11971, 13, 570, 3563, 254, 38865, 18852, 11, 2030, 2502, 62, 436, 17, 2549, 27, 2229, 29, 64, 270, 13, 375, 245, 270, 97970, 384, 29, 1551, 245, 270, 8, 372, 32144, 6, 63, 7432, 13, 570, 6587, 11, 2030, 2502, 62, 436, 17, 2549, 27, 2229, 29, 64, 270, 13, 375, 245, 270, 97970, 384, 29, 1551, 245, 270, 8, 334, 64, 1501, 372, 8, 334, 66, 1501, 32144, 2519, 63, 7432, 457, 22381, 254, 1917, 285, 254, 38865, 18852, 13, 207, 18, 13, 9217, 11037, 2030, 82, 3761, 53473, 33244, 746, 570, 685, 437, 5637, 11, 2030, 82, 3761, 53473, 63, 481, 330, 1222, 276, 3128, 254, 938, 280, 2030, 2502, 62, 436, 17, 62, 8458, 16, 63, 285, 750, 7667, 8445, 11, 548, 254, 5637, 317, 17236, 2527, 344, 10118, 22834, 32600, 13, 1002, 26932, 5637, 4446, 254, 24937, 22834, 11489, 285, 5131, 22614, 276, 6428, 254, 1525, 4327, 13, 100001], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1898, 15255, 254, 24937, 3418, 881, 245, 26932, 98907, 5637, 11, 395, 543, 1181, 254, 5610, 28526, 279, 254, 24937, 5637, 285, 938, 98907, 6, 82, 5637, 4786, 276, 8970, 945, 359, 13, 1003, 543, 839, 938, 2030, 82, 3761, 53473, 63, 276, 4750, 279, 7526, 254, 4374, 69289, 410, 10870, 565, 4067, 13, 4462, 6, 82, 946, 254, 26932, 98907, 5637, 1667, 1068, 25, 34083, 262, 1039, 275, 19407, 1525, 62, 436, 17, 62, 30785, 25, 25107, 284, 436, 16, 25, 440, 2502, 62, 436, 17, 375, 372, 32144, 1, 285, 284, 436, 17, 25, 440, 2502, 62, 436, 17, 1551, 372, 32144, 1, 3535, 440, 2502, 62, 436, 17, 2549, 27, 2229, 29, 64, 270, 13, 375, 245, 270, 97970, 384, 29, 1551, 245, 270, 8, 372, 32144, 1, 5637, 570, 473, 284, 436, 16, 284, 436, 17, 1296, 3025, 672, 16243, 5637, 334, 515, 13023, 372, 12954, 25, 32144, 8, 1460, 47767, 937, 1296, 3025, 5879, 457, 1115, 79, 2112, 1460, 334, 8458, 245, 372, 8, 937, 2903, 258, 32144, 6, 1066, 32144, 25, 440, 5508, 403, 258, 1501, 32144, 24451, 285, 375, 25, 440, 47, 245, 258, 1, 285, 1551, 25, 440, 48, 245, 258, 1, 285, 304, 39, 25, 440, 2502, 62, 436, 17, 375, 372, 32144, 24451, 440, 2502, 62, 436, 17, 1551, 372, 32144, 24451, 457, 334, 10860, 1115, 79, 25, 1525, 62, 436, 17, 62, 8458, 16, 8, 937, 463, 440, 2502, 62, 436, 17, 2549, 27, 2229, 29, 64, 270, 13, 375, 245, 270, 97970, 384, 29, 1551, 245, 270, 8, 372, 32144, 24451, 1244, 4926, 13, 84805, 457, 28884, 366, 375, 1551, 32144, 1296, 3025, 5879, 457, 334, 3392, 79, 962, 25, 1525, 62, 436, 17, 62, 8458, 16, 8, 4662, 271, 4662, 271, 34083, 49757, 2078, 43769, 25, 207, 16, 13, 9217, 7182, 15256, 21968, 51250, 63, 1780, 746, 570, 1273, 1572, 11971, 2030, 281, 63, 285, 2030, 5508, 63, 418, 7137, 11, 937, 2030, 2502, 62, 436, 17, 63, 60010, 7432, 327, 688, 31583, 11, 2847, 254, 23201, 280, 2030, 47, 63, 285, 2030, 48, 13557, 1002, 317, 3707, 1244, 2030, 3392, 79, 13557, 207, 17, 13, 9217, 3296, 53020, 10925, 21968, 8458, 63, 1780, 746, 570, 30037, 254, 3587, 7432, 327, 11971, 2030, 281, 63, 285, 2030, 5508, 6, 63, 334, 515, 53020, 18852, 633, 570, 1003, 933, 276, 1296, 359, 7432, 327, 2030, 64, 1501, 372, 63, 285, 2030, 66, 1501, 32144, 6, 13557, 570, 4810, 254, 17325, 2030, 75, 436, 16, 63, 285, 2030, 75, 436, 17, 12181, 395, 1006, 2030, 47, 245, 258, 63, 285, 2030, 48, 245, 258, 63, 2785, 327, 254, 11991, 280, 254, 11971, 13, 570, 3563, 254, 38865, 18852, 11, 2030, 2502, 62, 436, 17, 2549, 27, 2229, 29, 64, 270, 13, 375, 245, 270, 97970, 384, 29, 1551, 245, 270, 8, 372, 32144, 6, 63, 7432, 13, 570, 6587, 11, 2030, 2502, 62, 436, 17, 2549, 27, 2229, 29, 64, 270, 13, 375, 245, 270, 97970, 384, 29, 1551, 245, 270, 8, 334, 64, 1501, 372, 8, 334, 66, 1501, 32144, 2519, 63, 7432, 457, 22381, 254, 1917, 285, 254, 38865, 18852, 13, 207, 18, 13, 9217, 11037, 2030, 82, 3761, 53473, 33244, 746, 570, 685, 437, 5637, 11, 2030, 82, 3761, 53473, 63, 481, 330, 1222, 276, 3128, 254, 938, 280, 2030, 2502, 62, 436, 17, 62, 8458, 16, 63, 285, 750, 7667, 8445, 11, 548, 254, 5637, 317, 17236, 2527, 344, 10118, 22834, 32600, 13, 1002, 26932, 5637, 4446, 254, 24937, 22834, 11489, 285, 5131, 22614, 276, 6428, 254, 1525, 4327, 13, 100001].\n",
      "Sample 182 of the training set: <｜begin▁of▁sentence｜><|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant used for generating proofs in Isabelle to prove the provided natural language statements.<|im_end|>\n",
      "<|im_start|>user\n",
      "The lemma `list_all2_conj` can be translated into natural language as follows: Given two lists `as` and `cs`, if for every corresponding pair of elements `(a, c)` from these lists, the predicate `P` holds (`list_all2 P as cs`), and the predicate `Q` also holds (`list_all2 Q as cs`), then for every corresponding pair of elements `(a, c)`, the conjunction of predicates `P` and `Q` holds (`list_all2 (\\<lambda>a b. P a b \\<and> Q a b) as cs`).<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To translate the informal solution into a structured Isabelle proof, we will follow the steps outlined in the informal proof and use Isabelle's proof methods to formalize it. We will also use `sledgehammer` to assist in finding the necessary lemmas or facts if needed. Here's how the structured Isabelle proof might look: ```isabelle lemma list_all2_conj: assumes lall1: \"list_all2 P as cs\" and lall2: \"list_all2 Q as cs\" shows \"list_all2 (\\<lambda>a b. P a b \\<and> Q a b) as cs\" proof - from lall1 lall2 show ?thesis proof (induction as arbitrary: cs) case Nil then show ?case by simp next case (Cons a as) then obtain c cs' where cs: \"cs = c # cs'\" and P: \"P a c\" and Q: \"Q a c\" and IH: \"list_all2 P as cs'\" \"list_all2 Q as cs'\" by (auto simp: list_all2_Cons1) then have \"list_all2 (\\<lambda>a b. P a b \\<and> Q a b) as cs'\" using Cons.IH by blast with P Q cs show ?case by (simp add: list_all2_Cons1) qed qed ``` ### Explanation: 1. **Base Case (`Nil`):** - If both lists `as` and `cs` are empty, then `list_all2` trivially holds for any predicate, including the conjunction of `P` and `Q`. This is shown using `simp`. 2. **Inductive Step (`Cons`):** - Assume the property holds for lists `as` and `cs'` (inductive hypothesis). - We need to show it holds for `a # as` and `c # cs'`. - From the assumptions `lall1` and `lall2`, we know `P a c` and `Q a c` hold for the heads of the lists. - By the inductive hypothesis, `list_all2 (\\<lambda>a b. P a b \\<and> Q a b) as cs'` holds. - Thus, `list_all2 (\\<lambda>a b. P a b \\<and> Q a b) (a # as) (c # cs')` holds by combining the head and the inductive hypothesis. 3. **Using `sledgehammer`:** - In this proof, `sledgehammer` can be used to suggest the use of `list_all2_Cons1` and other simplifications, but the proof is straightforward enough that manual reasoning suffices. This structured proof follows the informal reasoning closely and uses induction to handle the list structure.<｜end▁of▁sentence｜>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2226779/547032888.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-24 15:50:07,025] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -lcufile\n",
      "collect2: error: ld returned 1 exit status\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbalaji-vir1997\u001b[0m (\u001b[33mbalaji-vir1997-stevens-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/balaji/Desktop/Theorem_Proving/wandb/run-20250224_155010-cror1drv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/cror1drv' target=\"_blank\">Distill-Qwen-1.5B</a></strong> to <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/cror1drv' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/cror1drv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/balaji/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 47.74 GiB of which 54.25 MiB is free. Including non-PyTorch memory, this process has 44.01 GiB memory in use. Of the allocated memory 39.42 GiB is allocated by PyTorch, and 3.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 95\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m data_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     88\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data_collator\u001b[38;5;241m=\u001b[39mdata_collator\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     91\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     92\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, args\u001b[38;5;241m=\u001b[39mtraining_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_module\n\u001b[1;32m     93\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_state()\n\u001b[1;32m     97\u001b[0m safe_save_model_for_hf_trainer(trainer\u001b[38;5;241m=\u001b[39mtrainer, output_dir\u001b[38;5;241m=\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39moutput_dir)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2599\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2595\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped:\n\u001b[1;32m   2604\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/optimizer.py:178\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adamw.py:216\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    213\u001b[0m     amsgrad: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     adamw(\n\u001b[1;32m    228\u001b[0m         params_with_grad,\n\u001b[1;32m    229\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adamw.py:159\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    155\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    156\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    165\u001b[0m         p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    166\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 47.74 GiB of which 54.25 MiB is free. Including non-PyTorch memory, this process has 44.01 GiB memory in use. Of the allocated memory 39.42 GiB is allocated by PyTorch, and 3.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8154b6-ef74-4668-9555-fb4a5cfb9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d9d13-f177-46eb-8211-7f7d91771b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
