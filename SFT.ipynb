{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f42729-fa07-4295-9ff2-4947cd0fdade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import torch.distributed\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "EOT_TOKEN = \"<|EOT|>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5816640b-3c21-4574-9031-f612ca4ee24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_instruction_prompt(instruction: str):\n",
    "    return \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\".format(\n",
    "        instruction.strip()\n",
    "    ).lstrip()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"Qwen/Qwen2.5-Coder-1.5B-Instruct\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: List[str] = field(\n",
    "        default_factory=list, metadata={\"help\": \"Paths to the training data.\"}\n",
    "    )\n",
    "    instruction_field: str = field(\n",
    "        default=\"instruction\", metadata={\"help\": \"The field name for the instruction\"}\n",
    "    )\n",
    "    output_field: str = field(\n",
    "        default=\"output\", metadata={\"help\": \"The field name for the output\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(  # NOTE: ignore this\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b637bb-927e-45b3-bc67-37fb663d0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(\n",
    "    strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer\n",
    ") -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            # max_length=tokenizer.model_max_length,\n",
    "            # truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "        for tokenized in tokenized_list\n",
    "    ]\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [\n",
    "        _tokenize_fn(strings, tokenizer) for strings in (examples, sources)\n",
    "    ]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple(\n",
    "            [instance[key] for instance in instances] for key in (\"input_ids\", \"labels\")\n",
    "        )\n",
    "        input_ids = [torch.tensor(x) for x in input_ids]\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = [torch.tensor(x) for x in labels]\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=IGNORE_INDEX\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d747cd71-316d-4935-8920-dcd056ce161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # Initialize distributed training only if running in distributed mode\n",
    "    if training_args.local_rank != -1 and os.getenv(\"RANK\") is not None:\n",
    "        if torch.distributed.is_available() and not torch.distributed.is_initialized():\n",
    "            torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "\n",
    "    if training_args.local_rank == 0:\n",
    "        print(\"=\" * 100)\n",
    "        print(training_args)\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    def train_tokenize_function(examples, tokenizer):\n",
    "        sources = [\n",
    "            build_instruction_prompt(instruction)\n",
    "            for instruction in examples[data_args.instruction_field]\n",
    "        ]\n",
    "        targets = [\n",
    "            f\"{output.strip()}{tokenizer.eos_token}\"\n",
    "            for output in examples[data_args.output_field]\n",
    "        ]\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "        return data_dict\n",
    "\n",
    "    print(\"PAD Token:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "    print(\"BOS Token\", tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "    print(\"EOS Token\", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "\n",
    "    if training_args.local_rank == 0:\n",
    "        print(\"Load tokenizer from {} over.\".format(model_args.model_name_or_path))\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path, torch_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    if training_args.local_rank == 0:\n",
    "        print(\"Load model from {} over.\".format(model_args.model_name_or_path))\n",
    "\n",
    "    raw_train_datasets = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data_args.data_path,\n",
    "        split=\"train\",\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "    \n",
    "    # Use a safe barrier\n",
    "    if training_args.local_rank > 0 and torch.distributed.is_initialized():\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    train_dataset = raw_train_datasets.map(\n",
    "        train_tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=3000,\n",
    "        num_proc=32,\n",
    "        remove_columns=raw_train_datasets.column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running Encoding\",\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    )\n",
    "\n",
    "    if training_args.local_rank == 0 and torch.distributed.is_initialized():\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    if training_args.local_rank == 0:\n",
    "        print(\"Training dataset samples:\", len(train_dataset))\n",
    "        for index in random.sample(range(len(train_dataset)), 3):\n",
    "            print(\n",
    "                f\"Sample {index} of the training set: {train_dataset[index]['input_ids']}, {train_dataset[index]['labels']}.\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Sample {index} of the training set: {tokenizer.decode(list(train_dataset[index]['input_ids']))}.\"\n",
    "            )\n",
    "\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    data_module = dict(\n",
    "        train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, tokenizer=tokenizer, args=training_args, **data_module\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_state()\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68da4bef-ab03-4206-8054-7193db19fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "critic_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "data_path = \"train.jsonl\"\n",
    "output_path = \"Distill-Qwen-1.5B\"\n",
    "\n",
    "sys.argv = [\n",
    "    \"notebook\",\n",
    "    \"--model_name_or_path\", critic_model,\n",
    "    \"--data_path\", data_path,\n",
    "    \"--output_dir\", output_path,\n",
    "    \"--instruction_field\", \"natural_language_statement\",\n",
    "    \"--output_field\", \"formal_proof\",\n",
    "    \"--num_train_epochs\", \"1\",\n",
    "    \"--model_max_length\", \"2048\",\n",
    "    \"--per_device_train_batch_size\", \"4\",\n",
    "    \"--per_device_eval_batch_size\", \"1\",\n",
    "    \"--gradient_accumulation_steps\", \"8\",\n",
    "    \"--eval_strategy\", \"no\",\n",
    "    \"--save_strategy\", \"steps\",\n",
    "    \"--save_steps\", \"20\",\n",
    "    \"--save_total_limit\", \"10\",\n",
    "    \"--learning_rate\", \"2e-5\",\n",
    "    \"--warmup_steps\", \"0\",\n",
    "    \"--logging_steps\", \"1\",\n",
    "    \"--lr_scheduler_type\", \"cosine\",\n",
    "    \"--gradient_checkpointing\", \"True\",\n",
    "    \"--report_to\", \"wandb\",\n",
    "    \"--bf16\", \"True\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10016b8e-e838-4df0-90c2-33465666d621",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m model_args, data_args, training_args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args_into_dataclasses()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize distributed training only if running in distributed mode\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mlocal_rank \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRANK\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[1;32m     10\u001b[0m         torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39minit_process_group(backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnccl\u001b[39m\u001b[38;5;124m\"\u001b[39m, init_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv://\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8154b6-ef74-4668-9555-fb4a5cfb9849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
