{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7895ad4e-0703-4a4f-9ab9-e421fb09ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import asdict, dataclass\n",
    "from autoformalism_with_llms import prompt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from autoformalism_with_llms.dataset import MiniF2FMATH\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "\n",
    "from autoformalism_with_llms.dataset import MathQuestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efeb42f4-2557-4fd1-9848-9a3ebae1030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"meta-llama/Meta-Llama-3-70B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc99cbc7-4fbf-4961-9563-f33fc5d37d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    name: str\n",
    "    model: str = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    temperature: float = 0.1\n",
    "    max_tokens: int = 2048\n",
    "    top_p: float = 1.0\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FEWSHOTIDS:\n",
    "    \"\"\"IDs of the few-shot learning examples used in the paper\"\"\"\n",
    "    algebra: tuple[str, ...] = (\n",
    "        \"245\",\n",
    "        \"76\",\n",
    "        \"478\",\n",
    "        \"338\",\n",
    "        \"422\",\n",
    "        \"43\",\n",
    "        \"756\",\n",
    "        \"149\",\n",
    "        \"48\",\n",
    "        \"410\",\n",
    "    )\n",
    "\n",
    "    numbertheory: tuple[str, ...] = (\n",
    "        \"709\",\n",
    "        \"461\",\n",
    "        \"466\",\n",
    "        \"257\",\n",
    "        \"34\",\n",
    "        \"780\",\n",
    "        \"233\",\n",
    "        \"764\",\n",
    "        \"345\",\n",
    "        \"227\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7a9f17-4884-4792-abe4-80ba39f1b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"Loads the model and tokenizer.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acab4f4c-ef6e-4a13-8eae-faaa6f6cf0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_response(prompt_str, model, tokenizer, **kwargs):\n",
    "    \"\"\"Generates a response for a given prompt.\"\"\"\n",
    "    temperature = kwargs.get(\"temperature\", 0.7)\n",
    "    max_tokens = kwargs.get(\"max_tokens\", 512)\n",
    "    top_p = kwargs.get(\"top_p\", 1.0)\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt_str, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9d94b8-2255-4a47-9b1b-9979e0bd73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fewshot_prompt(dataset, question_ids):\n",
    "    \"\"\"Creates a few-shot prompt using the dataset and question IDs.\"\"\"\n",
    "    questions = [dataset.get_question(qid) for qid in question_ids]\n",
    "    messages = [system_message()]\n",
    "    messages.extend(prompt.informal_to_formal_messages(questions))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71712b3-19d1-4592-a925-3ae39a278aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_message():\n",
    "    \"\"\"Returns the system message for the prompt.\"\"\"\n",
    "    return {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"Translate the following natural language math problem to the \"\n",
    "            \"Isabelle theorem proving language. Do not provide a proof of the \"\n",
    "            \"statement. Use diligence when translating the problem and make \"\n",
    "            \"certain you capture all the necessary assumptions as hypotheses.\"\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d19721-a8a7-4312-9fe1-dca604359c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_messages_to_llama3(messages: list[dict]) -> str:\n",
    "    \"\"\"Convert a list of messages to a llama3 string.\n",
    "\n",
    "    See:\n",
    "        https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "\n",
    "    Args:\n",
    "        messages (list[dict]): A list of messages.\n",
    "\n",
    "    Returns:\n",
    "        str: The llama3 string.\n",
    "    \"\"\"\n",
    "    HEADER_START = \"<|start_header_id|>\"\n",
    "    HEADER_END = \"<|end_header_id|>\"\n",
    "    role_template = HEADER_START + \"{role}\" + HEADER_END + \"\\n\\n\"\n",
    "    llama3 = []\n",
    "    llama3.append(\"<|begin_of_text|>\")\n",
    "    for message in messages:\n",
    "        msg = role_template.format(role=message[\"role\"])\n",
    "        msg += message[\"content\"]\n",
    "        msg += \"<|eot_id|>\"\n",
    "        llama3.append(msg)\n",
    "\n",
    "    llama3.append(role_template.format(role=\"assistant\"))\n",
    "    return \"\".join(llama3)\n",
    "\n",
    "\n",
    "def informal_to_formal_messages(questions: list[MathQuestion]) -> list[dict]:\n",
    "    \"\"\"Convert the list of MathQuestions to a message string.\"\"\"\n",
    "    messages = []\n",
    "    for question in questions:\n",
    "        example = make_example(question)\n",
    "        messages.append(get_natural_language_message(question))\n",
    "        messages.append(get_formal_language_message(question))\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_natural_language_message(question: MathQuestion, role: str = \"user\") -> dict:\n",
    "    \"\"\"Convert a MathQuestion object to an OpenAI message dictionary.\n",
    "\n",
    "    This message is the natural language message, i.e. the informal statement of a\n",
    "    math problem.\n",
    "\n",
    "    Args:\n",
    "        question (MathQuestion): The MathQuestion object which contains informal\n",
    "            and formal statements of a question.\n",
    "        role (str, optional): The role of the speaker. Defaults to \"user\".\n",
    "\n",
    "    Returns:\n",
    "        dict: The message dictionary.\n",
    "    \"\"\"\n",
    "    example = make_example(question)\n",
    "    return {\"role\": role, \"content\": example[\"natural_question\"]}\n",
    "\n",
    "\n",
    "def get_formal_language_message(\n",
    "    question: MathQuestion, role: str = \"assistant\"\n",
    ") -> dict:\n",
    "    \"\"\"Convert a MathQuestion object to an OpenAI message dictionary.\n",
    "\n",
    "    This message is the formal language message, i.e. the formal statement of a\n",
    "    math problem in a theorem prover language.\n",
    "\n",
    "    Args:\n",
    "        question (MathQuestion): The MathQuestion object which contains informal\n",
    "            and formal statements of a question.\n",
    "        role (str, optional): The role of the speaker. Defaults to \"assistant\".\n",
    "\n",
    "    Returns:\n",
    "        dict: The message dictionary.\n",
    "    \"\"\"\n",
    "    example = make_example(question)\n",
    "    return {\"role\": role, \"content\": example[\"formal_question\"]}\n",
    "\n",
    "\n",
    "def make_example(question: MathQuestion) -> dict[str, str]:\n",
    "    \"\"\"Convert a MathQuestion object to a single example for translation.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        question (MathQuestion): The MathQuestion object which contains informal\n",
    "            and formal statements of a question.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, str]: A dictionary containing the natural language question and\n",
    "            the formal question. This can be used to contrust few shot learning\n",
    "            examples for the translation task.\n",
    "    \"\"\"\n",
    "    question_prompt = question_with_answer_prompt(\n",
    "        question.informal_statement, question.informal_solution\n",
    "    )\n",
    "    theorem_prompt = remove_content_after_theorem_shows(question.formal_statement)\n",
    "    theorem_prompt = remove_content_before_theorem(theorem_prompt)\n",
    "    theorem_prompt = remove_theorem_name(theorem_prompt)\n",
    "    theorem_prompt = theorem_prompt.strip()\n",
    "\n",
    "    return {\n",
    "        \"natural_question\": question_prompt,\n",
    "        \"formal_question\": theorem_prompt,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_question(question: MathQuestion) -> str:\n",
    "    \"\"\"Convert a MathQuestion object to a question string.\"\"\"\n",
    "    return make_example(question)[\"natural_question\"]\n",
    "\n",
    "\n",
    "def question_with_answer_prompt(question: str, solution: str) -> str:\n",
    "    r\"\"\"Convert the question and solution strings to a natural language string.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question string.\n",
    "        solution (str): The solution string.\n",
    "\n",
    "    Returns:\n",
    "        str: The natural language string.\n",
    "\n",
    "    \"\"\"\n",
    "    final_answer = get_boxed_answer(solution)\n",
    "    return f\"{question} The final answer is ${final_answer}$.\"\n",
    "\n",
    "\n",
    "def remove_content_after_theorem_shows(formal_statement: str) -> str:\n",
    "    \"\"\"Remove the content after the shows statement in the theorem.\n",
    "\n",
    "    Note:\n",
    "        This is not applicable to metamath or hollight datasets.\n",
    "    \"\"\"\n",
    "    for line_number, line in enumerate(formal_statement.splitlines()):\n",
    "        if re.search(r\"^\\s*shows\", line):\n",
    "            return \"\\n\".join(formal_statement.splitlines()[: line_number + 1])\n",
    "    return formal_statement\n",
    "\n",
    "\n",
    "def remove_content_before_theorem(formal_statement: str) -> str:\n",
    "    \"\"\"Removes all the content before the theorem statement.\"\"\"\n",
    "    for line_number, line in enumerate(formal_statement.splitlines()):\n",
    "        if re.search(r\"^\\s*theorem\", line):\n",
    "            return \"\\n\".join(formal_statement.splitlines()[line_number:])\n",
    "    return formal_statement\n",
    "\n",
    "\n",
    "def remove_theorem_name(formal_statement: str) -> str:\n",
    "    \"\"\"Removes the theorem name from the formal statement.\"\"\"\n",
    "    return re.sub(r\"(.*theorem).*(?:|$)\", r\"\\1\", formal_statement, re.M)\n",
    "\n",
    "\n",
    "def get_boxed_answer(question: str) -> str | None:\n",
    "    r\"\"\"Extract the boxed answer from the string.\n",
    "\n",
    "    We assume the question has a latex boxed answer in the form `\\boxed{answer}`.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question string.\n",
    "\n",
    "    Returns:\n",
    "        str: The boxed answer string.\n",
    "\n",
    "    \"\"\"\n",
    "    phrase = r\"\\boxed{\"\n",
    "    try:\n",
    "        index = question.index(phrase) + len(phrase)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    open_count = 1  # since we start after \\boxed{ we have one open brace\n",
    "    close_count = 0\n",
    "    end_index = None\n",
    "    for i, c in enumerate(question[index:]):\n",
    "        if c == \"{\":\n",
    "            open_count += 1\n",
    "        elif c == \"}\":\n",
    "            close_count += 1\n",
    "        if open_count == close_count:\n",
    "            end_index = i\n",
    "            break\n",
    "    if end_index is None:\n",
    "        return None\n",
    "    return question[index : index + end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22754d05-383d-4165-8c03-6b50063a6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset, fewshot_ids, log_dir, model, tokenizer, **kwargs):\n",
    "    \"\"\"Runs the experiment on the dataset.\"\"\"\n",
    "    messages = make_fewshot_prompt(dataset, fewshot_ids)\n",
    "\n",
    "    for question in dataset:\n",
    "        if question.question_number in fewshot_ids:\n",
    "            continue\n",
    "\n",
    "        fname = Path(log_dir) / f\"{question.question_number}.json\"\n",
    "        if fname.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            prompt_str = get_prompt_str(question, messages)\n",
    "            response = get_question_response(prompt_str, model, tokenizer, **kwargs)\n",
    "\n",
    "            # Extract only the last assistant's response\n",
    "            if \"assistant<|end_header_id|>\" in response:\n",
    "                response = response.split(\"assistant<|end_header_id|>\")[-1].strip()\n",
    "\n",
    "            data = {\n",
    "                \"response\": response,  # Save only the isolated assistant response\n",
    "                \"metadata\": asdict(question),\n",
    "                \"prompt\": prompt_str,\n",
    "            }\n",
    "            with open(fname, \"w\") as f:\n",
    "                json.dump(data, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {question.question_number}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt_str(question, messages):\n",
    "    \"\"\"Formats the full prompt string.\"\"\"\n",
    "    _messages = messages + [prompt.get_natural_language_message(question)]\n",
    "    prompt_str = convert_messages_to_llama3(_messages)\n",
    "    return prompt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "667894bc-20b4-4e58-97e9-75b6ecb637b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = Args(name=\"llama3_8b_baseline\")\n",
    "\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load_model_and_tokenizer(args.model)\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = MiniF2FMATH()\n",
    "    algebra = dataset.get_subject(\"algebra\")\n",
    "    algebra_ids = FEWSHOTIDS.algebra\n",
    "    numtheory = dataset.get_subject(\"numbertheory\")\n",
    "    numtheory_ids = FEWSHOTIDS.numbertheory\n",
    "\n",
    "    algebra_data = (\"algebra\", algebra, algebra_ids)\n",
    "    numtheory_data = (\"numbertheory\", numtheory, numtheory_ids)\n",
    "\n",
    "    for data in (algebra_data, numtheory_data):\n",
    "        dataset_name, dataset, ids = data\n",
    "        log_dir = Path(\"artifacts\") / args.name / dataset_name\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        params = asdict(args)\n",
    "\n",
    "        # Save parameters\n",
    "        with open(log_dir / \"params.json\", \"w\") as f:\n",
    "            json.dump(params, f)\n",
    "\n",
    "        # Remove \"model\" from params to avoid conflict\n",
    "        params.pop(\"name\", None)  # Optional: Remove name too if not needed\n",
    "        params.pop(\"model\", None)\n",
    "\n",
    "        # Pass the remaining params\n",
    "        run_experiment(dataset, ids, log_dir, model, tokenizer, **params)\n",
    "\n",
    "    print(\"Experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7486d75c-82d2-4d1c-9f3a-156cb817d641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947327f8b1724c19ab91eea186263485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2901942/3832242952.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2901942/3507463912.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Pass the remaining params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Experiment complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2901942/2387043826.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(dataset, fewshot_ids, log_dir, model, tokenizer, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprompt_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prompt_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_question_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Extract only the last assistant's response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2901942/2165966028.py\u001b[0m in \u001b[0;36mget_question_response\u001b[0;34m(prompt_str, model, tokenizer, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Generate response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     output = model.generate(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3257\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    832\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m                 )\n\u001b[1;32m    588\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    590\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36meager_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, scaling, dropout, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m ):\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeat_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeat_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_key_value_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_key_value_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf2167-efc7-4867-a281-b6c7d7894719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
