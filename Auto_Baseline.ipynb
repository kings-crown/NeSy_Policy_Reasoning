{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7895ad4e-0703-4a4f-9ab9-e421fb09ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import asdict, dataclass\n",
    "from autoformalism_with_llms import prompt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from autoformalism_with_llms.dataset import MiniF2FMATH\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "\n",
    "from autoformalism_with_llms.dataset import MathQuestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efeb42f4-2557-4fd1-9848-9a3ebae1030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"meta-llama/Meta-Llama-3-70B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc99cbc7-4fbf-4961-9563-f33fc5d37d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    name: str\n",
    "    model: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"\n",
    "    temperature: float = 0.1\n",
    "    max_tokens: int = 2048\n",
    "    top_p: float = 1.0\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FEWSHOTIDS:\n",
    "    \"\"\"IDs of the few-shot learning examples used in the paper\"\"\"\n",
    "    algebra: tuple[str, ...] = (\n",
    "        \"245\",\n",
    "        \"76\",\n",
    "        \"478\",\n",
    "        \"338\",\n",
    "        \"422\",\n",
    "        \"43\",\n",
    "        \"756\",\n",
    "        \"149\",\n",
    "        \"48\",\n",
    "        \"410\",\n",
    "    )\n",
    "\n",
    "    numbertheory: tuple[str, ...] = (\n",
    "        \"709\",\n",
    "        \"461\",\n",
    "        \"466\",\n",
    "        \"257\",\n",
    "        \"34\",\n",
    "        \"780\",\n",
    "        \"233\",\n",
    "        \"764\",\n",
    "        \"345\",\n",
    "        \"227\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7a9f17-4884-4792-abe4-80ba39f1b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"Loads the model and tokenizer.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acab4f4c-ef6e-4a13-8eae-faaa6f6cf0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_response(prompt_str, model, tokenizer, **kwargs):\n",
    "    \"\"\"Generates a response for a given prompt.\"\"\"\n",
    "    temperature = kwargs.get(\"temperature\", 0.2)\n",
    "    max_tokens = kwargs.get(\"max_tokens\", 512)\n",
    "    top_p = kwargs.get(\"top_p\", 1.0)\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt_str, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9d94b8-2255-4a47-9b1b-9979e0bd73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fewshot_prompt(dataset, question_ids):\n",
    "    \"\"\"Creates a few-shot prompt using the dataset and question IDs.\"\"\"\n",
    "    questions = [dataset.get_question(qid) for qid in question_ids]\n",
    "    messages = [system_message()]\n",
    "    messages.extend(prompt.informal_to_formal_messages(questions))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71712b3-19d1-4592-a925-3ae39a278aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_message():\n",
    "    \"\"\"Returns the system message for the prompt.\"\"\"\n",
    "    return {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"Translate the following natural language math problem to the \"\n",
    "            \"Isabelle theorem proving language. Do not provide a proof of the \"\n",
    "            \"statement. Use diligence when translating the problem and make \"\n",
    "            \"certain you capture all the necessary assumptions as hypotheses.\"\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d19721-a8a7-4312-9fe1-dca604359c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_messages_to_llama3(messages: list[dict]) -> str:\n",
    "    \"\"\"Convert a list of messages to a llama3 string.\n",
    "\n",
    "    See:\n",
    "        https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "\n",
    "    Args:\n",
    "        messages (list[dict]): A list of messages.\n",
    "\n",
    "    Returns:\n",
    "        str: The llama3 string.\n",
    "    \"\"\"\n",
    "    HEADER_START = \"<|start_header_id|>\"\n",
    "    HEADER_END = \"<|end_header_id|>\"\n",
    "    role_template = HEADER_START + \"{role}\" + HEADER_END + \"\\n\\n\"\n",
    "    llama3 = []\n",
    "    llama3.append(\"<|begin_of_text|>\")\n",
    "    for message in messages:\n",
    "        msg = role_template.format(role=message[\"role\"])\n",
    "        msg += message[\"content\"]\n",
    "        msg += \"<|eot_id|>\"\n",
    "        llama3.append(msg)\n",
    "\n",
    "    llama3.append(role_template.format(role=\"assistant\"))\n",
    "    return \"\".join(llama3)\n",
    "\n",
    "\n",
    "def informal_to_formal_messages(questions: list[MathQuestion]) -> list[dict]:\n",
    "    \"\"\"Convert the list of MathQuestions to a message string.\"\"\"\n",
    "    messages = []\n",
    "    for question in questions:\n",
    "        example = make_example(question)\n",
    "        messages.append(get_natural_language_message(question))\n",
    "        messages.append(get_formal_language_message(question))\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_natural_language_message(question: MathQuestion, role: str = \"user\") -> dict:\n",
    "    \"\"\"Convert a MathQuestion object to an OpenAI message dictionary.\n",
    "\n",
    "    This message is the natural language message, i.e. the informal statement of a\n",
    "    math problem.\n",
    "\n",
    "    Args:\n",
    "        question (MathQuestion): The MathQuestion object which contains informal\n",
    "            and formal statements of a question.\n",
    "        role (str, optional): The role of the speaker. Defaults to \"user\".\n",
    "\n",
    "    Returns:\n",
    "        dict: The message dictionary.\n",
    "    \"\"\"\n",
    "    example = make_example(question)\n",
    "    return {\"role\": role, \"content\": example[\"natural_question\"]}\n",
    "\n",
    "\n",
    "def get_formal_language_message(\n",
    "    question: MathQuestion, role: str = \"assistant\"\n",
    ") -> dict:\n",
    "    \"\"\"Convert a MathQuestion object to an OpenAI message dictionary.\n",
    "\n",
    "    This message is the formal language message, i.e. the formal statement of a\n",
    "    math problem in a theorem prover language.\n",
    "\n",
    "    Args:\n",
    "        question (MathQuestion): The MathQuestion object which contains informal\n",
    "            and formal statements of a question.\n",
    "        role (str, optional): The role of the speaker. Defaults to \"assistant\".\n",
    "\n",
    "    Returns:\n",
    "        dict: The message dictionary.\n",
    "    \"\"\"\n",
    "    example = make_example(question)\n",
    "    return {\"role\": role, \"content\": example[\"formal_question\"]}\n",
    "\n",
    "\n",
    "def make_example(question: MathQuestion) -> dict[str, str]:\n",
    "    \"\"\"Convert a MathQuestion object to a single example for translation.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        question (MathQuestion): The MathQuestion object which contains informal\n",
    "            and formal statements of a question.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, str]: A dictionary containing the natural language question and\n",
    "            the formal question. This can be used to contrust few shot learning\n",
    "            examples for the translation task.\n",
    "    \"\"\"\n",
    "    question_prompt = question_with_answer_prompt(\n",
    "        question.informal_statement, question.informal_solution\n",
    "    )\n",
    "    theorem_prompt = remove_content_after_theorem_shows(question.formal_statement)\n",
    "    theorem_prompt = remove_content_before_theorem(theorem_prompt)\n",
    "    theorem_prompt = remove_theorem_name(theorem_prompt)\n",
    "    theorem_prompt = theorem_prompt.strip()\n",
    "\n",
    "    return {\n",
    "        \"natural_question\": question_prompt,\n",
    "        \"formal_question\": theorem_prompt,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_question(question: MathQuestion) -> str:\n",
    "    \"\"\"Convert a MathQuestion object to a question string.\"\"\"\n",
    "    return make_example(question)[\"natural_question\"]\n",
    "\n",
    "\n",
    "def question_with_answer_prompt(question: str, solution: str) -> str:\n",
    "    r\"\"\"Convert the question and solution strings to a natural language string.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question string.\n",
    "        solution (str): The solution string.\n",
    "\n",
    "    Returns:\n",
    "        str: The natural language string.\n",
    "\n",
    "    \"\"\"\n",
    "    final_answer = get_boxed_answer(solution)\n",
    "    return f\"{question} The final answer is ${final_answer}$.\"\n",
    "\n",
    "\n",
    "def remove_content_after_theorem_shows(formal_statement: str) -> str:\n",
    "    \"\"\"Remove the content after the shows statement in the theorem.\n",
    "\n",
    "    Note:\n",
    "        This is not applicable to metamath or hollight datasets.\n",
    "    \"\"\"\n",
    "    for line_number, line in enumerate(formal_statement.splitlines()):\n",
    "        if re.search(r\"^\\s*shows\", line):\n",
    "            return \"\\n\".join(formal_statement.splitlines()[: line_number + 1])\n",
    "    return formal_statement\n",
    "\n",
    "\n",
    "def remove_content_before_theorem(formal_statement: str) -> str:\n",
    "    \"\"\"Removes all the content before the theorem statement.\"\"\"\n",
    "    for line_number, line in enumerate(formal_statement.splitlines()):\n",
    "        if re.search(r\"^\\s*theorem\", line):\n",
    "            return \"\\n\".join(formal_statement.splitlines()[line_number:])\n",
    "    return formal_statement\n",
    "\n",
    "\n",
    "def remove_theorem_name(formal_statement: str) -> str:\n",
    "    \"\"\"Removes the theorem name from the formal statement.\"\"\"\n",
    "    return re.sub(r\"(.*theorem).*(?:|$)\", r\"\\1\", formal_statement, re.M)\n",
    "\n",
    "\n",
    "def get_boxed_answer(question: str) -> str | None:\n",
    "    r\"\"\"Extract the boxed answer from the string.\n",
    "\n",
    "    We assume the question has a latex boxed answer in the form `\\boxed{answer}`.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question string.\n",
    "\n",
    "    Returns:\n",
    "        str: The boxed answer string.\n",
    "\n",
    "    \"\"\"\n",
    "    phrase = r\"\\boxed{\"\n",
    "    try:\n",
    "        index = question.index(phrase) + len(phrase)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    open_count = 1  # since we start after \\boxed{ we have one open brace\n",
    "    close_count = 0\n",
    "    end_index = None\n",
    "    for i, c in enumerate(question[index:]):\n",
    "        if c == \"{\":\n",
    "            open_count += 1\n",
    "        elif c == \"}\":\n",
    "            close_count += 1\n",
    "        if open_count == close_count:\n",
    "            end_index = i\n",
    "            break\n",
    "    if end_index is None:\n",
    "        return None\n",
    "    return question[index : index + end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22754d05-383d-4165-8c03-6b50063a6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset, fewshot_ids, log_dir, model, tokenizer, **kwargs):\n",
    "    \"\"\"Runs the experiment on the dataset.\"\"\"\n",
    "    messages = make_fewshot_prompt(dataset, fewshot_ids)\n",
    "\n",
    "    for question in dataset:\n",
    "        if question.question_number in fewshot_ids:\n",
    "            continue\n",
    "\n",
    "        fname = Path(log_dir) / f\"{question.question_number}.json\"\n",
    "        if fname.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            prompt_str = get_prompt_str(question, messages)\n",
    "            response = get_question_response(prompt_str, model, tokenizer, **kwargs)\n",
    "\n",
    "            # Extract only the last assistant's response\n",
    "            if \"assistant<|end_header_id|>\" in response:\n",
    "                response = response.split(\"assistant<|end_header_id|>\")[-1].strip()\n",
    "\n",
    "            data = {\n",
    "                \"response\": response,  # Save only the isolated assistant response\n",
    "                \"metadata\": asdict(question),\n",
    "                \"prompt\": prompt_str,\n",
    "            }\n",
    "            with open(fname, \"w\") as f:\n",
    "                json.dump(data, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {question.question_number}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt_str(question, messages):\n",
    "    \"\"\"Formats the full prompt string.\"\"\"\n",
    "    _messages = messages + [prompt.get_natural_language_message(question)]\n",
    "    prompt_str = convert_messages_to_llama3(_messages)\n",
    "    return prompt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "667894bc-20b4-4e58-97e9-75b6ecb637b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = Args(name=\"DeepSeek_R1_Qwen_32B_baseline\")\n",
    "\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load_model_and_tokenizer(args.model)\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = MiniF2FMATH()\n",
    "    algebra = dataset.get_subject(\"algebra\")\n",
    "    algebra_ids = FEWSHOTIDS.algebra\n",
    "    numtheory = dataset.get_subject(\"numbertheory\")\n",
    "    numtheory_ids = FEWSHOTIDS.numbertheory\n",
    "\n",
    "    algebra_data = (\"algebra\", algebra, algebra_ids)\n",
    "    numtheory_data = (\"numbertheory\", numtheory, numtheory_ids)\n",
    "\n",
    "    for data in (algebra_data, numtheory_data):\n",
    "        dataset_name, dataset, ids = data\n",
    "        log_dir = Path(\"artifacts\") / args.name / dataset_name\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        params = asdict(args)\n",
    "\n",
    "        # Save parameters\n",
    "        with open(log_dir / \"params.json\", \"w\") as f:\n",
    "            json.dump(params, f)\n",
    "\n",
    "        # Remove \"model\" from params to avoid conflict\n",
    "        params.pop(\"name\", None)  # Optional: Remove name too if not needed\n",
    "        params.pop(\"model\", None)\n",
    "\n",
    "        # Pass the remaining params\n",
    "        run_experiment(dataset, ids, log_dir, model, tokenizer, **params)\n",
    "\n",
    "    print(\"Experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486d75c-82d2-4d1c-9f3a-156cb817d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc01867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
