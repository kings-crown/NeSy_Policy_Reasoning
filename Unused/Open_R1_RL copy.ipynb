{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ArqgnsfGhgHI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-02-16 16:59:10,530] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "INFO 02-16 16:59:11 __init__.py:183] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "import time\n",
        "import openai\n",
        "import json\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "import random\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, AdamW\n",
        ")\n",
        "from accelerate import Accelerator\n",
        "import random\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, AdamW\n",
        ")\n",
        "\n",
        "from peft import LoraConfig\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import textwrap\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from typing import Any, Callable, Optional, Union\n",
        "from unittest.mock import patch\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import transformers\n",
        "from accelerate.utils import broadcast_object_list, gather_object\n",
        "from transformers import (\n",
        "    PreTrainedModel,\n",
        "    Trainer,\n",
        ")\n",
        "from trl.trainer import GRPOTrainer\n",
        "from trl.data_utils import (\n",
        "    apply_chat_template,\n",
        "    is_conversational,\n",
        "    maybe_apply_chat_template,\n",
        ")\n",
        "from trl.models import unwrap_model_for_generation\n",
        "from trl.trainer.grpo_config import GRPOConfig\n",
        "from trl.trainer.utils import pad\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import set_seed\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import subprocess\n",
        "from typing import TYPE_CHECKING, Dict, Union\n",
        "from concurrent.futures import Future\n",
        "\n",
        "from transformers import AutoConfig\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import re\n",
        "from latex2sympy2_extended import NormalizationConfig\n",
        "from math_verify import LatexExtractionConfig, parse, verify\n",
        "\n",
        "import trl\n",
        "\n",
        "import subprocess\n",
        "from typing import List\n",
        "\n",
        "from transformers import TrainerCallback\n",
        "from transformers.trainer_callback import TrainerControl, TrainerState\n",
        "from transformers.training_args import TrainingArguments\n",
        "\n",
        "from trl import ModelConfig, ScriptArguments, TrlParser, get_peft_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bz7whDjhk4jJ"
      },
      "outputs": [],
      "source": [
        "#from utils.callbacks import get_callbacks\n",
        "def get_callbacks(train_config, model_config) -> List[TrainerCallback]:\n",
        "    callbacks = []\n",
        "    for callback_name in train_config.callbacks:\n",
        "        if callback_name not in CALLBACKS:\n",
        "            raise ValueError(f\"Callback {callback_name} not found in CALLBACKS.\")\n",
        "        callbacks.append(CALLBACKS[callback_name](model_config))\n",
        "\n",
        "    return callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Checker(object):\n",
        "    \"\"\"A modified version of the Draft, Sketch, Prove proof-checking client.\n",
        "    (https://github.com/albertqjiang/draft_sketch_prove/blob/main/autoformalization/checker.py)\n",
        "\n",
        "    This checker supports Isabelle2022 via the new version of PISA\n",
        "    (https://albertqjiang.github.io/Portal-to-ISAbelle/).\n",
        "\n",
        "    It supports checking a miniF2F-style proof via `check`.\n",
        "\n",
        "    Finally, it replaces `sledgehammer` with a call to `normalhammer`.\n",
        "    \"\"\"\n",
        "    def __init__(self, working_dir, isa_path, theory_file, port=9000):\n",
        "        sys.path.append(os.environ['PISA_PATH'])\n",
        "        try:\n",
        "            from pisa_client import initialise_env\n",
        "            self.initialise_env = initialise_env\n",
        "        except:\n",
        "            print(\"Set $PISA_PATH to /yourpath/to/Portal-to-ISAbelle/src/main/python\")\n",
        "\n",
        "        self.working_dir = working_dir\n",
        "        self.isa_path = isa_path\n",
        "        self.theory_file = theory_file\n",
        "        self.port = port\n",
        "\n",
        "    def _initialize(self):\n",
        "        env = self.initialise_env(\n",
        "            self.port,\n",
        "            isa_path=self.isa_path,\n",
        "            theory_file_path=self.theory_file,\n",
        "            working_directory=self.working_dir\n",
        "        )\n",
        "        return env\n",
        "\n",
        "    def _exit(self, env):\n",
        "        try:\n",
        "            env.post('exit')\n",
        "        except:\n",
        "            print(\"env.post('exit') timed out\")\n",
        "            pass\n",
        "        os.system(\"ps aux | grep Isabelle | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
        "        os.system(\"ps aux | grep poly | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
        "\n",
        "    def _parse_output(self, obs):\n",
        "        \"\"\"Parse the sledgehammer output, otherwise return an empty string\"\"\"\n",
        "        if '<hammer>' in obs:\n",
        "            output = obs.split('<hammer>')[0]\n",
        "        else:\n",
        "            output = ''\n",
        "        return output\n",
        "\n",
        "    def _run_step(self, step, i, tls_name, env):\n",
        "        obs, reward, done, metadata = env.step_to_top_level_state(\n",
        "            action=step,\n",
        "            tls_name=tls_name,\n",
        "            new_name='default_%d' % i\n",
        "        )\n",
        "        error = None\n",
        "        if 'error:' in obs or 'Step error' in obs or 'Unknown error' in obs:\n",
        "            error = obs\n",
        "        return obs, reward, done, metadata, error\n",
        "\n",
        "    def _run_sledgehammer(self, step, i, tls_name, env):\n",
        "        # First try heuristics\n",
        "        for heuristic in ['by auto', 'by simp', 'by blast', 'by fastforce', 'by force', 'by eval', 'by presburger', 'by sos', 'by arith', 'by linarith', 'by (auto simp: field_simps)']:\n",
        "            step_ = step.replace('normalhammer', heuristic)\n",
        "            obs, reward, done, metadata, error = self._run_step(step_, i, tls_name, env)\n",
        "            if error is None:\n",
        "                obs = '%s <hammer> %s' % (heuristic, obs)\n",
        "                return obs, reward, done, metadata, error\n",
        "        # Try sledgehammer\n",
        "        out = self._run_step(step, i, tls_name, env)\n",
        "        return out\n",
        "\n",
        "    def check(self, statement_and_proof):\n",
        "        # Initialize environment\n",
        "        env = self._initialize()\n",
        "        env.initialise()\n",
        "\n",
        "        # Wrap and parse theorem\n",
        "        theory = Checker.wrap_theorem(statement_and_proof)\n",
        "        steps = Checker.get_parsed(env, theory)\n",
        "\n",
        "        result = self._check(env, steps)\n",
        "        return result\n",
        "\n",
        "    def _check(self, env, steps):\n",
        "        done = False\n",
        "        reason = ''\n",
        "        success = False\n",
        "        step_results = []\n",
        "        tls_name = 'default'\n",
        "        for i, step in enumerate(steps):\n",
        "            try:\n",
        "                time0 = time.time()\n",
        "                if 'normalhammer' in step:\n",
        "                    obs, reward, done, metadata, error = self._run_sledgehammer(step, i, tls_name, env)\n",
        "                else:\n",
        "                    obs, reward, done, metadata, error = self._run_step(step, i, tls_name, env)\n",
        "                step_time = time.time() - time0\n",
        "                step_results.append(dict(index=i, step=step, output=self._parse_output(obs), step_time=step_time))\n",
        "                if error is not None:\n",
        "                    reason = error\n",
        "                    success = False\n",
        "                    done = False\n",
        "                    break\n",
        "            except:\n",
        "                # Timeout - end the proof attempt\n",
        "                success = False\n",
        "                done = False\n",
        "                reason = 'timeout (%d)' % len(step_results)\n",
        "                step_results.append(dict(index=i, step=step, output=''))\n",
        "                break\n",
        "\n",
        "            # Change when successful\n",
        "            tls_name = 'default_%d' % i\n",
        "\n",
        "        if done and reward == 1.0:\n",
        "            success = True\n",
        "\n",
        "        result = {\n",
        "            'success': success,\n",
        "            'reason': reason,\n",
        "            'num_steps': len(steps),\n",
        "            'last_step': len(step_results),\n",
        "            'step_results': step_results,\n",
        "            'theorem_and_proof': self.reconstruct(step_results) if success else ''\n",
        "        }\n",
        "        # Exit environment\n",
        "        self._exit(env)\n",
        "        return result\n",
        "    \n",
        "    @staticmethod\n",
        "    def reconstruct(step_results):\n",
        "        steps = []\n",
        "        for step_result in step_results[1:]:\n",
        "            if step_result['output'] != '':\n",
        "                steps.append(step_result['output'].strip())\n",
        "            else:\n",
        "                steps.append(step_result['step'].strip())\n",
        "        theorem_and_proof = '\\n'.join(steps)\n",
        "        return theorem_and_proof\n",
        "\n",
        "    @staticmethod\n",
        "    def wrap_theorem(theorem):\n",
        "        return 'theory Interactive imports HOL.HOL Complex_Main \"HOL-Library.Code_Target_Numeral\" \"HOL-Library.Sum_of_Squares\" \"Symmetric_Polynomials.Vieta\" \"HOL-Computational_Algebra.Computational_Algebra\" \"HOL-Number_Theory.Number_Theory\" \\n begin\\n%s' % theorem\n",
        "\n",
        "    @staticmethod\n",
        "    def get_parsed(env, theory, tls_name='default'):\n",
        "        # HACK: the parsing doesn't work well with `normalhammer`, so we replace\n",
        "        # all hammer calls with sorry, then replace sorry to normalhammer after parsing.\n",
        "        theory = theory.replace('sledgehammer', 'sorry')\n",
        "        theory = theory.replace('normalhammer', 'sorry')\n",
        "\n",
        "        steps = env.post(f\"<parse text> ${theory}\")\n",
        "        steps = steps.split('<SEP>')\n",
        "        steps = [s for s in steps if s.strip() != '']\n",
        "        # remove weird '$' step and whitespace steps\n",
        "        steps = [s for s in steps if s != '$' and s.strip() != '']\n",
        "        steps = [s.replace('sorry', 'normalhammer') for s in steps]\n",
        "        return steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('../')\n",
        "os.environ['PISA_PATH'] = '/home/siai/Portal-to-ISAbelle/src/main/python'\n",
        "\n",
        "checker = Checker(\n",
        "    working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
        "    isa_path='/home/siai/Isabelle2022',\n",
        "    theory_file='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
        "    port=9000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1UNDdSjJl1vw"
      },
      "outputs": [],
      "source": [
        "#from grpo_trainer import GRPOTrainer\n",
        "\n",
        "\n",
        "RewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]\n",
        "\n",
        "\n",
        "class GRPOTrainer(GRPOTrainer):\n",
        "    # base trl GRPO_trainer\n",
        "    def compute_loss(\n",
        "        self, model, inputs, return_outputs=False, num_items_in_batch=None\n",
        "    ):\n",
        "        if return_outputs:\n",
        "            raise ValueError(\"The GRPOTrainer does not support returning outputs\")\n",
        "\n",
        "        device = self.accelerator.device\n",
        "        prompts = [x[\"prompt\"] for x in inputs]\n",
        "        prompts_text = [\n",
        "            maybe_apply_chat_template(example, self.processing_class)[\"prompt\"]\n",
        "            for example in inputs\n",
        "        ]\n",
        "        prompt_inputs = self.processing_class(\n",
        "            prompts_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            padding_side=\"left\",\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        prompt_inputs = super()._prepare_inputs(prompt_inputs)\n",
        "\n",
        "        if self.max_prompt_length is not None:\n",
        "            prompt_inputs[\"input_ids\"] = prompt_inputs[\"input_ids\"][\n",
        "                :, -self.max_prompt_length :\n",
        "            ]\n",
        "            prompt_inputs[\"attention_mask\"] = prompt_inputs[\"attention_mask\"][\n",
        "                :, -self.max_prompt_length :\n",
        "            ]\n",
        "\n",
        "        # Generate completions using either vLLM or regular generation\n",
        "        if self.args.use_vllm:\n",
        "            # First, have main process load weights if needed\n",
        "            if self.state.global_step != self._last_loaded_step:\n",
        "                with unwrap_model_for_generation(\n",
        "                    model, self.accelerator\n",
        "                ) as unwrapped_model:\n",
        "                    state_dict = unwrapped_model.state_dict()\n",
        "                if self.accelerator.is_main_process:\n",
        "                    llm_model = (\n",
        "                        self.llm.llm_engine.model_executor.driver_worker.model_runner.model\n",
        "                    )\n",
        "                    llm_model.load_weights(state_dict.items())\n",
        "                self._last_loaded_step = self.state.global_step\n",
        "\n",
        "            # Generate completions using vLLM: gather all prompts and use them in a single call in the main process\n",
        "            all_prompts_text = gather_object(prompts_text)\n",
        "            if self.accelerator.is_main_process:\n",
        "                outputs = self.llm.generate(\n",
        "                    all_prompts_text,\n",
        "                    sampling_params=self.sampling_params,\n",
        "                    use_tqdm=False,\n",
        "                )\n",
        "                completion_ids = [\n",
        "                    out.token_ids\n",
        "                    for completions in outputs\n",
        "                    for out in completions.outputs\n",
        "                ]\n",
        "                for output in outputs:\n",
        "                    print(\"-\" * 100)\n",
        "                    print(\"\\n\\n\\n\")\n",
        "                    prompt = output.prompt\n",
        "                    for output_t in output.outputs:\n",
        "                        # print(completion_ids)\n",
        "                        print(\"=\" * 100)\n",
        "                        generated_text = output_t.text\n",
        "                        print(\"【USER】: \", prompt)\n",
        "                        print(\"\\n【ASSISTANT】:\", generated_text)\n",
        "            else:\n",
        "                completion_ids = [None] * len(all_prompts_text) * self.num_generations\n",
        "\n",
        "            # Broadcast the completions from the main process to all processes, ensuring each process receives its\n",
        "            # corresponding slice.\n",
        "            completion_ids = broadcast_object_list(completion_ids, from_process=0)\n",
        "            process_slice = slice(\n",
        "                self.accelerator.process_index * len(prompts) * self.num_generations,\n",
        "                (self.accelerator.process_index + 1)\n",
        "                * len(prompts)\n",
        "                * self.num_generations,\n",
        "            )\n",
        "            completion_ids = completion_ids[process_slice]\n",
        "\n",
        "            # Pad the completions, and concatenate them with the prompts\n",
        "            completion_ids = [\n",
        "                torch.tensor(ids, device=device) for ids in completion_ids\n",
        "            ]\n",
        "            completion_ids = pad(\n",
        "                completion_ids, padding_value=self.processing_class.pad_token_id\n",
        "            )\n",
        "            prompt_inputs_repeated = torch.repeat_interleave(\n",
        "                prompt_inputs[\"input_ids\"], self.num_generations, dim=0\n",
        "            ).to(device)\n",
        "            prompt_completion_ids = torch.cat(\n",
        "                [prompt_inputs_repeated, completion_ids], dim=1\n",
        "            )\n",
        "        else:\n",
        "            # Regular generation path\n",
        "            with unwrap_model_for_generation(\n",
        "                model, self.accelerator\n",
        "            ) as unwrapped_model:\n",
        "                prompt_inputs[\"input_ids\"] = prompt_inputs[\"input_ids\"].to(device)\n",
        "                prompt_inputs[\"attention_mask\"] = prompt_inputs[\"attention_mask\"].to(\n",
        "                    device\n",
        "                )\n",
        "\n",
        "                prompt_completion_ids = unwrapped_model.generate(\n",
        "                    **prompt_inputs, generation_config=self.generation_config\n",
        "                )\n",
        "\n",
        "        prompt_length = prompt_inputs[\"input_ids\"].size(1)\n",
        "        completion_ids = prompt_completion_ids[:, prompt_length:]\n",
        "\n",
        "        # Get the per-token log probabilities for the completions for the model and the reference model\n",
        "        def get_per_token_logps(model, input_ids, num_logits_to_keep):\n",
        "            # We add 1 to `num_logits_to_keep` because the last logits of the sequence is later excluded\n",
        "            logits = model(\n",
        "                input_ids, num_logits_to_keep=num_logits_to_keep + 1\n",
        "            ).logits  # (B, L, V)\n",
        "            logits = logits[\n",
        "                :, :-1, :\n",
        "            ]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred\n",
        "\n",
        "            # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\n",
        "            per_token_logps = []\n",
        "            for logits_row, input_ids_row in zip(\n",
        "                logits, input_ids[:, -num_logits_to_keep:]\n",
        "            ):\n",
        "                log_probs = logits_row.log_softmax(dim=-1)\n",
        "                token_log_prob = torch.gather(\n",
        "                    log_probs, dim=1, index=input_ids_row.unsqueeze(1)\n",
        "                ).squeeze(1)\n",
        "                per_token_logps.append(token_log_prob)\n",
        "            return torch.stack(per_token_logps)\n",
        "\n",
        "        num_logits_to_keep = completion_ids.size(\n",
        "            1\n",
        "        )  # we only need to compute the logits for the completion tokens\n",
        "        per_token_logps = get_per_token_logps(\n",
        "            model, prompt_completion_ids, num_logits_to_keep\n",
        "        )\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            if self.ref_model is not None:\n",
        "                ref_per_token_logps = get_per_token_logps(\n",
        "                    self.ref_model, prompt_completion_ids, num_logits_to_keep\n",
        "                )\n",
        "            else:\n",
        "                with self.accelerator.unwrap_model(model).disable_adapter():\n",
        "                    ref_per_token_logps = get_per_token_logps(\n",
        "                        model, prompt_completion_ids, num_logits_to_keep\n",
        "                    )\n",
        "\n",
        "        # Compute the KL divergence between the model and the reference model\n",
        "        per_token_kl = (\n",
        "            torch.exp(ref_per_token_logps - per_token_logps)\n",
        "            - (ref_per_token_logps - per_token_logps)\n",
        "            - 1\n",
        "        )\n",
        "\n",
        "        # Mask everything after the first EOS token\n",
        "        is_eos = completion_ids == self.processing_class.eos_token_id\n",
        "        eos_idx = torch.full(\n",
        "            (is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device\n",
        "        )\n",
        "        eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\n",
        "        sequence_indices = torch.arange(is_eos.size(1), device=device).expand(\n",
        "            is_eos.size(0), -1\n",
        "        )\n",
        "        completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
        "\n",
        "        # Decode the generated completions\n",
        "        completions = self.processing_class.batch_decode(\n",
        "            completion_ids, skip_special_tokens=True\n",
        "        )\n",
        "        if is_conversational(inputs[0]):\n",
        "            completions = [\n",
        "                [{\"role\": \"assistant\", \"content\": completion}]\n",
        "                for completion in completions\n",
        "            ]\n",
        "\n",
        "        # Compute the rewards\n",
        "        prompts = [prompt for prompt in prompts for _ in range(self.num_generations)]\n",
        "\n",
        "        rewards_per_func = torch.zeros(\n",
        "            len(prompts), len(self.reward_funcs), device=device\n",
        "        )\n",
        "        for i, (reward_func, reward_processing_class) in enumerate(\n",
        "            zip(self.reward_funcs, self.reward_processing_classes)\n",
        "        ):\n",
        "            if isinstance(reward_func, PreTrainedModel):\n",
        "                if is_conversational(inputs[0]):\n",
        "                    messages = [\n",
        "                        {\"messages\": p + c} for p, c in zip(prompts, completions)\n",
        "                    ]\n",
        "                    texts = [\n",
        "                        apply_chat_template(x, reward_processing_class)[\"text\"]\n",
        "                        for x in messages\n",
        "                    ]\n",
        "                else:\n",
        "                    texts = [p + c for p, c in zip(prompts, completions)]\n",
        "                reward_inputs = reward_processing_class(\n",
        "                    texts,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    padding_side=\"right\",\n",
        "                    add_special_tokens=False,\n",
        "                )\n",
        "                reward_inputs = super()._prepare_inputs(reward_inputs)\n",
        "                with torch.inference_mode():\n",
        "                    rewards_per_func[:, i] = reward_func(**reward_inputs).logits[\n",
        "                        :, 0\n",
        "                    ]  # Shape (B*G,)\n",
        "            else:\n",
        "                # Repeat all input columns (but \"prompt\" and \"completion\") to match the number of generations\n",
        "                reward_kwargs = {\n",
        "                    key: []\n",
        "                    for key in inputs[0].keys()\n",
        "                    if key not in [\"prompt\", \"completion\"]\n",
        "                }\n",
        "                for key in reward_kwargs:\n",
        "                    for example in inputs:\n",
        "                        # Repeat each value in the column for `num_generations` times\n",
        "                        reward_kwargs[key].extend([example[key]] * self.num_generations)\n",
        "                output_reward_func = reward_func(\n",
        "                    prompts=prompts, completions=completions, **reward_kwargs\n",
        "                )\n",
        "                rewards_per_func[:, i] = torch.tensor(\n",
        "                    output_reward_func, dtype=torch.float32, device=device\n",
        "                )\n",
        "\n",
        "        # Sum the rewards from all reward functions\n",
        "        rewards = rewards_per_func.sum(dim=1)\n",
        "\n",
        "        # Compute grouped-wise rewards\n",
        "        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n",
        "        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n",
        "\n",
        "        # Normalize the rewards to compute the advantages\n",
        "        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(\n",
        "            self.num_generations, dim=0\n",
        "        )\n",
        "        std_grouped_rewards = std_grouped_rewards.repeat_interleave(\n",
        "            self.num_generations, dim=0\n",
        "        )\n",
        "        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n",
        "\n",
        "        # x - x.detach() allows for preserving gradients from x\n",
        "        per_token_loss = torch.exp(\n",
        "            per_token_logps - per_token_logps.detach()\n",
        "        ) * advantages.unsqueeze(1)\n",
        "        per_token_loss = -(per_token_loss - self.beta * per_token_kl)\n",
        "        loss = (\n",
        "            (per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n",
        "        ).mean()\n",
        "\n",
        "        # Log the metrics\n",
        "        completion_length = (\n",
        "            self.accelerator.gather_for_metrics(completion_mask.sum(1))\n",
        "            .float()\n",
        "            .mean()\n",
        "            .item()\n",
        "        )\n",
        "        self._metrics[\"completion_length\"].append(completion_length)\n",
        "\n",
        "        reward_per_func = self.accelerator.gather_for_metrics(rewards_per_func).mean(0)\n",
        "        for i, reward_func in enumerate(self.reward_funcs):\n",
        "            if isinstance(reward_func, PreTrainedModel):\n",
        "                reward_func_name = reward_func.config._name_or_path.split(\"/\")[-1]\n",
        "            else:\n",
        "                reward_func_name = reward_func.__name__\n",
        "            self._metrics[f\"rewards/{reward_func_name}\"].append(\n",
        "                reward_per_func[i].item()\n",
        "            )\n",
        "\n",
        "        self._metrics[\"reward\"].append(\n",
        "            self.accelerator.gather_for_metrics(rewards).mean().item()\n",
        "        )\n",
        "\n",
        "        self._metrics[\"reward_std\"].append(\n",
        "            self.accelerator.gather_for_metrics(std_grouped_rewards).mean().item()\n",
        "        )\n",
        "\n",
        "        mean_kl = (\n",
        "            (per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n",
        "        ).mean()\n",
        "        self._metrics[\"kl\"].append(\n",
        "            self.accelerator.gather_for_metrics(mean_kl).mean().item()\n",
        "        )\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SIsFSQYik_vJ"
      },
      "outputs": [],
      "source": [
        "#from .evaluation import run_benchmark_jobs\n",
        "def run_benchmark_jobs(training_args: Union[\"SFTConfig\", \"GRPOConfig\"], model_args: \"ModelConfig\") -> None:\n",
        "    benchmarks = training_args.benchmarks\n",
        "    if len(benchmarks) == 1 and benchmarks[0] == \"all\":\n",
        "        benchmarks = get_lighteval_tasks()\n",
        "        # Evaluate on all supported benchmarks. Later we may want to include a `chat` option\n",
        "        # that just evaluates on `ifeval` and `mt_bench` etc.\n",
        "\n",
        "    for benchmark in benchmarks:\n",
        "        print(f\"Launching benchmark `{benchmark}`\")\n",
        "        if benchmark in get_lighteval_tasks():\n",
        "            run_lighteval_job(benchmark, training_args, model_args)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown benchmark {benchmark}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zGcuLNQyiTOz"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GRPOConfig(trl.GRPOConfig):\n",
        "    \"\"\"\n",
        "    args for callbacks, benchmarks etc\n",
        "    \"\"\"\n",
        "\n",
        "    benchmarks: list[str] = field(\n",
        "        default_factory=lambda: [],\n",
        "        metadata={\"help\": \"The benchmarks to run after training.\"},\n",
        "    )\n",
        "    callbacks: list[str] = field(\n",
        "        default_factory=lambda: [],\n",
        "        metadata={\"help\": \"The callbacks to run during training.\"},\n",
        "    )\n",
        "    system_prompt: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The optional system prompt to use for benchmarking.\"},\n",
        "    )\n",
        "    hub_model_revision: Optional[str] = field(\n",
        "        default=\"main\", metadata={\"help\": \"The Hub model branch to push the model to.\"}\n",
        "    )\n",
        "    overwrite_hub_revision: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to overwrite the Hub revision.\"}\n",
        "    )\n",
        "    push_to_hub_revision: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to push to a Hub revision/branch.\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SFTConfig(trl.SFTConfig):\n",
        "    \"\"\"\n",
        "    args for callbacks, benchmarks etc\n",
        "    \"\"\"\n",
        "\n",
        "    benchmarks: list[str] = field(\n",
        "        default_factory=lambda: [],\n",
        "        metadata={\"help\": \"The benchmarks to run after training.\"},\n",
        "    )\n",
        "    callbacks: list[str] = field(\n",
        "        default_factory=lambda: [],\n",
        "        metadata={\"help\": \"The callbacks to run during training.\"},\n",
        "    )\n",
        "    system_prompt: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The optional system prompt to use for benchmarking.\"},\n",
        "    )\n",
        "    hub_model_revision: Optional[str] = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The Hub model branch to push the model to.\"},\n",
        "    )\n",
        "    overwrite_hub_revision: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to overwrite the Hub revision.\"}\n",
        "    )\n",
        "    push_to_hub_revision: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to push to a Hub revision/branch.\"}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4USTS_ADlIA1"
      },
      "outputs": [],
      "source": [
        "#from .hub import push_to_hub_revision\n",
        "def push_to_hub_revision(training_args: SFTConfig | GRPOConfig, extra_ignore_patterns=[]) -> Future:\n",
        "    \"\"\"Pushes the model to branch on a Hub repo.\"\"\"\n",
        "\n",
        "    # Create a repo if it doesn't exist yet\n",
        "    repo_url = create_repo(repo_id=training_args.hub_model_id, private=True, exist_ok=True)\n",
        "    # Get initial commit to branch from\n",
        "    initial_commit = list_repo_commits(training_args.hub_model_id)[-1]\n",
        "    # Now create the branch we'll be pushing to\n",
        "    create_branch(\n",
        "        repo_id=training_args.hub_model_id,\n",
        "        branch=training_args.hub_model_revision,\n",
        "        revision=initial_commit.commit_id,\n",
        "        exist_ok=True,\n",
        "    )\n",
        "    logger.info(f\"Created target repo at {repo_url}\")\n",
        "    logger.info(f\"Pushing to the Hub revision {training_args.hub_model_revision}...\")\n",
        "    ignore_patterns = [\"checkpoint-*\", \"*.pth\"]\n",
        "    ignore_patterns.extend(extra_ignore_patterns)\n",
        "    future = upload_folder(\n",
        "        repo_id=training_args.hub_model_id,\n",
        "        folder_path=training_args.output_dir,\n",
        "        revision=training_args.hub_model_revision,\n",
        "        commit_message=f\"Add {training_args.hub_model_revision} checkpoint\",\n",
        "        ignore_patterns=ignore_patterns,\n",
        "        run_as_future=True,\n",
        "    )\n",
        "    logger.info(f\"Pushed to {repo_url} revision {training_args.hub_model_revision} successfully!\")\n",
        "\n",
        "    return future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RRJND9mSkUbo"
      },
      "outputs": [],
      "source": [
        "#from rewards import REWARD_FUNCS_REGISTRY\n",
        "\n",
        "def accuracy_reward(completions, solution, **kwargs):\n",
        "    \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    rewards = []\n",
        "    for content, sol in zip(contents, solution):\n",
        "        gold_parsed = parse(\n",
        "            sol,\n",
        "            extraction_mode=\"first_match\",\n",
        "            extraction_config=[LatexExtractionConfig()],\n",
        "        )\n",
        "        if len(gold_parsed) != 0:\n",
        "            # print('latex gold parsed')\n",
        "            # We require the answer to be provided in correct latex (no malformed operators)\n",
        "            answer_parsed = parse(\n",
        "                content,\n",
        "                extraction_config=[\n",
        "                    LatexExtractionConfig(\n",
        "                        normalization_config=NormalizationConfig(\n",
        "                            nits=False,\n",
        "                            malformed_operators=False,\n",
        "                            basic_latex=True,\n",
        "                            equations=True,\n",
        "                            boxed=\"all\",\n",
        "                            units=True,\n",
        "                        ),\n",
        "                        # Ensures that boxed is tried first\n",
        "                        boxed_match_priority=0,\n",
        "                        try_extract_without_anchor=False,\n",
        "                    )\n",
        "                ],\n",
        "                extraction_mode=\"first_match\",\n",
        "            )\n",
        "            # Reward 1 if the content is the same as the ground truth, 0 otherwise\n",
        "            reward = float(verify(answer_parsed, gold_parsed))\n",
        "            # print('\\nprompt:', prompt)\n",
        "            print(\"-\" * 100)\n",
        "            print(\n",
        "                \"\\nanswer_parsed:\",\n",
        "                answer_parsed,\n",
        "                \"\\ngold_parsed:\",\n",
        "                gold_parsed,\n",
        "                \"\\nreward:\",\n",
        "                reward,\n",
        "            )\n",
        "        else:\n",
        "            reward = 7.0\n",
        "            print(\"Failed to parse gold solution: \", sol)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    print(\"\\naccuracy rewards:\", rewards)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def Isabelle_reward(completions, solution, **kwargs):\n",
        "    \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    rewards = []\n",
        "    for content, sol in zip(contents, solution):\n",
        "        gold_parsed = parse(\n",
        "            sol,\n",
        "            extraction_mode=\"first_match\",\n",
        "            extraction_config=[LatexExtractionConfig()],\n",
        "        )\n",
        "        if len(gold_parsed) != 0:\n",
        "            # print('latex gold parsed')\n",
        "            # We require the answer to be provided in correct latex (no malformed operators)\n",
        "            answer_parsed = parse(\n",
        "                content,\n",
        "                extraction_config=[\n",
        "                    LatexExtractionConfig(\n",
        "                        normalization_config=NormalizationConfig(\n",
        "                            nits=False,\n",
        "                            malformed_operators=False,\n",
        "                            basic_latex=True,\n",
        "                            equations=True,\n",
        "                            boxed=\"all\",\n",
        "                            units=True,\n",
        "                        ),\n",
        "                        # Ensures that boxed is tried first\n",
        "                        boxed_match_priority=0,\n",
        "                        try_extract_without_anchor=False,\n",
        "                    )\n",
        "                ],\n",
        "                extraction_mode=\"first_match\",\n",
        "            )\n",
        "            # Reward 1 if the content is the same as the ground truth, 0 otherwise\n",
        "            reward = float(verify(answer_parsed, gold_parsed))\n",
        "            print('\\nprompt:', prompt)\n",
        "            print('\\completions:', completions)\n",
        "            print(\"-\" * 100)\n",
        "            print(\n",
        "                \"\\nanswer_parsed:\",\n",
        "                answer_parsed,\n",
        "                \"\\ngold_parsed:\",\n",
        "                gold_parsed,\n",
        "                \"\\nreward:\",\n",
        "                reward,\n",
        "            )\n",
        "        else:\n",
        "            reward = 7.0\n",
        "            print(\"Failed to parse gold solution: \", sol)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    print(\"\\naccuracy rewards:\", rewards)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def format_reward(completions, **kwargs):\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
        "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, content) for content in completion_contents]\n",
        "\n",
        "    rewards = [5.0 if match else 0.0 for match in matches]\n",
        "    print(\"-\" * 100)\n",
        "    print(\"\\nformat rewards:\", rewards)\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def reasoning_steps_reward(completions, **kwargs):\n",
        "    \"\"\"Reward function that checks for clear step-by-step reasoning.\n",
        "    Regex pattern:\n",
        "        Step \\d+: - matches \"Step 1:\", \"Step 2:\", etc.\n",
        "        ^\\d+\\. - matches numbered lists like \"1.\", \"2.\", etc. at start of line\n",
        "        \\n- - matches bullet points with hyphens\n",
        "        \\n\\* - matches bullet points with asterisks\n",
        "        First,|Second,|Next,|Finally, - matches transition words\n",
        "    \"\"\"\n",
        "    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n",
        "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [len(re.findall(pattern, content)) for content in completion_contents]\n",
        "\n",
        "    # Magic nubmer 3 to encourage 3 steps and more, otherwise partial reward\n",
        "    return [min(6.0, count / 3) for count in matches]\n",
        "\n",
        "\n",
        "REWARD_FUNCS_REGISTRY = {\n",
        "    \"accuracy\": accuracy_reward,\n",
        "    \"format\": format_reward,\n",
        "    \"reasoning_steps\": reasoning_steps_reward,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q-os0NYmhmP1"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l1b8IlQxmYS1"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GRPOScriptArguments(ScriptArguments):\n",
        "    reward_funcs: list[str] = field(\n",
        "        default_factory=lambda: [\"accuracy\", \"format\", \"reasoning_steps\"],\n",
        "        metadata={\n",
        "            \"help\": f\"List of reward functions. Possible values: {', '.join(REWARD_FUNCS_REGISTRY.keys())}\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
        "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
        "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
        "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AIgFJH5Omegn"
      },
      "outputs": [],
      "source": [
        "def main(script_args, training_args, model_args):\n",
        "    # Set seed for reproducibility\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    ###############\n",
        "    # Setup logging\n",
        "    ###############\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    )\n",
        "    log_level = training_args.get_process_log_level()\n",
        "    logger.setLevel(log_level)\n",
        "    datasets.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.enable_default_handler()\n",
        "    transformers.utils.logging.enable_explicit_format()\n",
        "\n",
        "    # Log on each process a small summary\n",
        "    logger.warning(\n",
        "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        "    )\n",
        "    logger.info(f\"Model parameters {model_args}\")\n",
        "    logger.info(f\"Script parameters {script_args}\")\n",
        "    logger.info(f\"Data parameters {training_args}\")\n",
        "\n",
        "    # Check for last checkpoint\n",
        "    last_checkpoint = None\n",
        "    if os.path.isdir(training_args.output_dir):\n",
        "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
        "        logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint=}.\")\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\n",
        "\n",
        "    # Get reward functions\n",
        "    reward_funcs = [REWARD_FUNCS_REGISTRY[func] for func in script_args.reward_funcs]\n",
        "\n",
        "\n",
        "    # Format into conversation\n",
        "    def make_conversation(example):\n",
        "        return {\n",
        "            \"prompt\": [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
        "            ],\n",
        "        }\n",
        "\n",
        "    dataset = dataset.map(make_conversation)\n",
        "    for split in dataset:\n",
        "        if \"messages\" in dataset[split].column_names:\n",
        "            dataset[split] = dataset[split].remove_columns(\"messages\")\n",
        "\n",
        "    logger.info(\"*** Initializing model kwargs ***\")\n",
        "    torch_dtype = (\n",
        "        model_args.torch_dtype\n",
        "        if model_args.torch_dtype in [\"auto\", None]\n",
        "        else getattr(torch, model_args.torch_dtype)\n",
        "    )\n",
        "\n",
        "    training_args.gradient_checkpointing = True\n",
        "    model_kwargs = dict(\n",
        "        revision=model_args.model_revision,\n",
        "        trust_remote_code=model_args.trust_remote_code,\n",
        "        attn_implementation=model_args.attn_implementation,\n",
        "        torch_dtype=torch_dtype,\n",
        "        use_cache=False if training_args.gradient_checkpointing else True,\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_args.model_name_or_path, load_in_4bit=False, **model_kwargs\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        model_args.model_name_or_path,\n",
        "    )\n",
        "    #############################\n",
        "    # Initialize the GRPO trainer\n",
        "    #############################\n",
        "    trainer = GRPOTrainer(\n",
        "        # model=model_args.model_name_or_path,\n",
        "        model=model,\n",
        "        reward_funcs=reward_funcs,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[script_args.dataset_train_split],\n",
        "        eval_dataset=(\n",
        "            dataset[script_args.dataset_test_split]\n",
        "            if training_args.eval_strategy != \"no\"\n",
        "            else None\n",
        "        ),\n",
        "        callbacks=get_callbacks(training_args, model_args),\n",
        "    )\n",
        "\n",
        "    ###############\n",
        "    # Training loop\n",
        "    ###############\n",
        "    logger.info(\"*** Train ***\")\n",
        "    checkpoint = None\n",
        "    if training_args.resume_from_checkpoint is not None:\n",
        "        checkpoint = training_args.resume_from_checkpoint\n",
        "    elif last_checkpoint is not None:\n",
        "        checkpoint = last_checkpoint\n",
        "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "    metrics = train_result.metrics\n",
        "    metrics[\"train_samples\"] = len(dataset[script_args.dataset_train_split])\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "    trainer.save_state()\n",
        "\n",
        "    ##################################\n",
        "    # Save model and create model card\n",
        "    ##################################\n",
        "    logger.info(\"*** Save model ***\")\n",
        "    trainer.save_model(training_args.output_dir)\n",
        "    logger.info(f\"Model saved to {training_args.output_dir}\")\n",
        "\n",
        "    # Save everything else on main process\n",
        "    kwargs = {\n",
        "        \"dataset_name\": script_args.dataset_name,\n",
        "        \"tags\": [\"OvO-R1\"],\n",
        "    }\n",
        "    if trainer.accelerator.is_main_process:\n",
        "        trainer.create_model_card(**kwargs)\n",
        "        # Restore k,v cache for fast inference\n",
        "        trainer.model.config.use_cache = True\n",
        "        trainer.model.config.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GKC3z82fmlem"
      },
      "outputs": [],
      "source": [
        "sys.argv = [\n",
        "    \"notebook\",  # sys.argv[0] is the script name in a real execution\n",
        "    \"--model_name_or_path\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
        "    \"--model_revision\", \"main\",\n",
        "    \"--torch_dtype\", \"bfloat16\",\n",
        "    \"--attn_implementation\", \"flash_attention_2\",\n",
        "\n",
        "    \"--dataset_name\", \"xiaodongguaAIGC/X-R1-750\",\n",
        "    #\"--dataset_configs\", \"train\",\n",
        "    #\"--num_processes\", \"3\",\n",
        "\n",
        "    \"--bf16\", \"true\",\n",
        "    \"--use_vllm\", \"false\",\n",
        "    #\"--vllm_device\", \"auto\",\n",
        "    #\"--vllm_gpu_memory_utilization\", \"0.7\",\n",
        "    \"--do_eval\", \"false\",\n",
        "    \"--eval_strategy\", \"no\",\n",
        "    \"--eval_steps\", \"5\",\n",
        "    \"--gradient_accumulation_steps\", \"4\",\n",
        "    \"--gradient_checkpointing\", \"true\",\n",
        "    \"--gradient_checkpointing_kwargs\", '{\"use_reentrant\": false}',\n",
        "    \"--hub_strategy\", \"every_save\",\n",
        "    \"--learning_rate\", \"3.0e-06\",\n",
        "    \"--log_level\", \"info\",\n",
        "    \"--logging_steps\", \"5\",\n",
        "    \"--logging_strategy\", \"steps\",\n",
        "    \"--lr_scheduler_type\", \"cosine\",\n",
        "    \"--max_prompt_length\", \"256\",\n",
        "    \"--num_generations\", \"4\",\n",
        "    \"--max_completion_length\", \"1024\",\n",
        "    \"--max_steps\", \"-1\",\n",
        "    \"--num_train_epochs\", \"3\",\n",
        "    \"--output_dir\", \"output/OvO-R1_instruct\",\n",
        "    \"--overwrite_output_dir\", \"true\",\n",
        "    \"--per_device_eval_batch_size\", \"1\",\n",
        "    \"--per_device_train_batch_size\", \"2\",\n",
        "    \"--push_to_hub\", \"false\",\n",
        "    \"--report_to\", \"wandb\",\n",
        "    \"--save_strategy\", \"epoch\",\n",
        "    \"--seed\", \"42\",\n",
        "    \"--warmup_ratio\", \"0.1\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AoymiaP6mkX6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-16 16:59:12 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2 distributed training: True, 16-bits training: False\n",
            "2025-02-16 16:59:12 - INFO - __main__ - Model parameters ModelConfig(model_name_or_path='Qwen/Qwen2.5-0.5B-Instruct', model_revision='main', torch_dtype='bfloat16', trust_remote_code=False, attn_implementation='flash_attention_2', use_peft=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', use_rslora=False, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)\n",
            "2025-02-16 16:59:12 - INFO - __main__ - Script parameters GRPOScriptArguments(dataset_name='xiaodongguaAIGC/X-R1-750', dataset_config=None, dataset_train_split='train', dataset_test_split='test', gradient_checkpointing_use_reentrant=False, ignore_bias_buffers=False, reward_funcs=['accuracy', 'format', 'reasoning_steps'])\n",
            "2025-02-16 16:59:12 - INFO - __main__ - Data parameters GRPOConfig(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "benchmarks=[],\n",
            "beta=0.04,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "callbacks=[],\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=10.0,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_model_revision=main,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=info,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/OvO-R1_instruct/runs/Feb16_16-59-12_siai-4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=cosine,\n",
            "max_completion_length=1024,\n",
            "max_grad_norm=1.0,\n",
            "max_prompt_length=256,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "model_init_kwargs=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_generations=2,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=output/OvO-R1_instruct,\n",
            "overwrite_hub_revision=False,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_revision=False,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=['wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output/OvO-R1_instruct,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "system_prompt=None,\n",
            "temperature=0.9,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "use_vllm=False,\n",
            "vllm_device=auto,\n",
            "vllm_gpu_memory_utilization=0.9,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overwrite dataset info from restored data version if exists.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-16 16:59:13 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading Dataset info from /home/siai/.cache/huggingface/datasets/xiaodongguaAIGC___x-r1-750/default/0.0.0/1a2e75b1147e199697374f5decea05e3b13d42ec\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-16 16:59:13 - INFO - datasets.info - Loading Dataset info from /home/siai/.cache/huggingface/datasets/xiaodongguaAIGC___x-r1-750/default/0.0.0/1a2e75b1147e199697374f5decea05e3b13d42ec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset x-r1-750 (/home/siai/.cache/huggingface/datasets/xiaodongguaAIGC___x-r1-750/default/0.0.0/1a2e75b1147e199697374f5decea05e3b13d42ec)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-16 16:59:13 - INFO - datasets.builder - Found cached dataset x-r1-750 (/home/siai/.cache/huggingface/datasets/xiaodongguaAIGC___x-r1-750/default/0.0.0/1a2e75b1147e199697374f5decea05e3b13d42ec)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading Dataset info from /home/siai/.cache/huggingface/datasets/xiaodongguaAIGC___x-r1-750/default/0.0.0/1a2e75b1147e199697374f5decea05e3b13d42ec\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-16 16:59:13 - INFO - datasets.info - Loading Dataset info from /home/siai/.cache/huggingface/datasets/xiaodongguaAIGC___x-r1-750/default/0.0.0/1a2e75b1147e199697374f5decea05e3b13d42ec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /home/siai/.cache/huggingface/datasets/xiaodongguaAIGC___x-r1-750/default/0.0.0/1a2e75b1147e199697374f5decea05e3b13d42ec/cache-ccbf56eb47acd183.arrow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-16 16:59:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/siai/.cache/huggingface/datasets/xiaodongguaAIGC___x-r1-750/default/0.0.0/1a2e75b1147e199697374f5decea05e3b13d42ec/cache-ccbf56eb47acd183.arrow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /home/siai/.cache/huggingface/datasets/xiaodongguaAIGC___x-r1-750/default/0.0.0/1a2e75b1147e199697374f5decea05e3b13d42ec/cache-159553dc8712ce84.arrow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-16 16:59:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/siai/.cache/huggingface/datasets/xiaodongguaAIGC___x-r1-750/default/0.0.0/1a2e75b1147e199697374f5decea05e3b13d42ec/cache-159553dc8712ce84.arrow\n",
            "2025-02-16 16:59:13 - INFO - __main__ - *** Initializing model kwargs ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO|configuration_utils.py:696] 2025-02-16 16:59:13,102 >> loading configuration file config.json from cache at /home/siai/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-02-16 16:59:13,104 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3904] 2025-02-16 16:59:13,130 >> loading weights file model.safetensors from cache at /home/siai/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/model.safetensors\n",
            "[INFO|modeling_utils.py:1582] 2025-02-16 16:59:13,137 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[WARNING|logging.py:328] 2025-02-16 16:59:13,140 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "[INFO|configuration_utils.py:1140] 2025-02-16 16:59:13,141 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4888] 2025-02-16 16:59:13,181 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4896] 2025-02-16 16:59:13,181 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-02-16 16:59:13,215 >> loading configuration file generation_config.json from cache at /home/siai/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-02-16 16:59:13,215 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qwen/Qwen2.5-0.5B-Instruct\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO|tokenization_utils_base.py:2034] 2025-02-16 16:59:13,317 >> loading file vocab.json from cache at /home/siai/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-02-16 16:59:13,317 >> loading file merges.txt from cache at /home/siai/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-02-16 16:59:13,318 >> loading file tokenizer.json from cache at /home/siai/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-02-16 16:59:13,318 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-02-16 16:59:13,318 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-02-16 16:59:13,319 >> loading file tokenizer_config.json from cache at /home/siai/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-02-16 16:59:13,319 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2304] 2025-02-16 16:59:13,525 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|trainer.py:741] 2025-02-16 16:59:13,720 >> Using auto half precision backend\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-16 16:59:13 - INFO - __main__ - *** Train ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO|trainer.py:2369] 2025-02-16 16:59:14,090 >> ***** Running training *****\n",
            "[INFO|trainer.py:2370] 2025-02-16 16:59:14,091 >>   Num examples = 750\n",
            "[INFO|trainer.py:2371] 2025-02-16 16:59:14,091 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2372] 2025-02-16 16:59:14,091 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2374] 2025-02-16 16:59:14,092 >>   Training with DataParallel so batch size has been adjusted to: 4\n",
            "[INFO|trainer.py:2375] 2025-02-16 16:59:14,092 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2376] 2025-02-16 16:59:14,092 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2377] 2025-02-16 16:59:14,092 >>   Total optimization steps = 141\n",
            "[INFO|trainer.py:2378] 2025-02-16 16:59:14,093 >>   Number of trainable parameters = 494,032,768\n",
            "[INFO|integration_utils.py:817] 2025-02-16 16:59:14,094 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbalaji-vir1997\u001b[0m (\u001b[33mbalaji-vir1997-stevens-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/siai/NeSy_T/wandb/run-20250216_165914-3g7r1vqd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/3g7r1vqd' target=\"_blank\">output/OvO-R1_instruct</a></strong> to <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/3g7r1vqd' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/3g7r1vqd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[WARNING|logging.py:328] 2025-02-16 16:59:15,206 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "answer_parsed: [5, '5'] \n",
            "gold_parsed: [5, '5'] \n",
            "reward: 1.0\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "answer_parsed: [4, '4'] \n",
            "gold_parsed: [5, '5'] \n",
            "reward: 0.0\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "answer_parsed: [] \n",
            "gold_parsed: [{6, 8*,;*10}, '6, 8\\\\text{, ; }10'] \n",
            "reward: 0.0\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "answer_parsed: [{-10, 10}, '10, -10'] \n",
            "gold_parsed: [{6, 8*,;*10}, '6, 8\\\\text{, ; }10'] \n",
            "reward: 0.0\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "answer_parsed: [3/2, '\\\\frac{3}{2}'] \n",
            "gold_parsed: [2/3, '\\\\frac{2}{3}'] \n",
            "reward: 0.0\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "answer_parsed: [4, '4'] \n",
            "gold_parsed: [2/3, '\\\\frac{2}{3}'] \n",
            "reward: 0.0\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "answer_parsed: [19, '19'] \n",
            "gold_parsed: [5, '5'] \n",
            "reward: 0.0\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "answer_parsed: [-69, '-69'] \n",
            "gold_parsed: [5, '5'] \n",
            "reward: 0.0\n",
            "\n",
            "accuracy rewards: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "format rewards: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_28854/3597659164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrlParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRPOScriptArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRPOConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mscript_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args_and_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_28854/4071804043.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(script_args, training_args, model_args)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlast_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_samples\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscript_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_train_split\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2529\u001b[0m                     )\n\u001b[1;32m   2530\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m                     if (\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3675\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3677\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_28854/1267390160.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 )\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 prompt_completion_ids = unwrapped_model.generate(\n\u001b[0m\u001b[1;32m    114\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mprompt_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 )\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3257\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                 layer_outputs = self._gradient_checkpointing_func(\n\u001b[0m\u001b[1;32m    566\u001b[0m                     \u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;31m# Runs pre-forward logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;31m# Runs post-forward logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mcos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsqueeze_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsqueeze_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mq_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mk_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;34m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = TrlParser((GRPOScriptArguments, GRPOConfig, ModelConfig))\n",
        "    script_args, training_args, model_args = parser.parse_args_and_config()\n",
        "    main(script_args, training_args, model_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHeuieMa76FA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
