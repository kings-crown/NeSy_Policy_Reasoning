{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted characters and clean text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Normalize new lines\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove excessive spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "def filter_valid_entries(df):\n",
    "    \"\"\"Filter dataset entries to remove invalid data.\"\"\"\n",
    "    # Remove rows where any important column is missing\n",
    "    df = df.dropna(subset=[\"natural_language_statement\", \"isabelle_translation\"])\n",
    "\n",
    "    # Remove problems containing \"<image>\", \"<span\", or weird HTML-like patterns\n",
    "    df = df[\n",
    "        ~df[\"natural_language_statement\"].str.contains(\"<image>|<span \", regex=True, na=False)\n",
    "    ]\n",
    "\n",
    "    # Filter based on proof length (keeping it reasonable for training)\n",
    "    df[\"proof_length\"] = df[\"formal_proof\"].apply(\n",
    "        lambda x: len(tokenizer.tokenize(x)) if isinstance(x, str) else 0\n",
    "    )\n",
    "    df = df[df[\"proof_length\"] < 2048]\n",
    "    df = df[df[\"proof_length\"] > 64]\n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_valid_entries_fevl(df):\n",
    "    \"\"\"Filter dataset entries to remove invalid data.\"\"\"\n",
    "    # Remove rows where any important column is missing\n",
    "    df = df.dropna(subset=[\"natural_language_statement\", \"isabelle_translation\"])\n",
    "\n",
    "    # Remove problems containing \"<image>\", \"<span\", or weird HTML-like patterns\n",
    "    df = df[\n",
    "        ~df[\"natural_language_statement\"].str.contains(\"<image>|<span \", regex=True, na=False)\n",
    "    ]\n",
    "\n",
    "    # Filter based on proof length (keeping it reasonable for training)\n",
    "    df[\"proof_length\"] = df[\"formal_proof\"].apply(\n",
    "        lambda x: len(tokenizer.tokenize(x)) if isinstance(x, str) else 0\n",
    "    )\n",
    "    df = df[df[\"proof_length\"] < 2048]\n",
    "    df = df[df[\"proof_length\"] > 64]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"/data/isabelle\", exist_ok=True)\n",
    "    \n",
    "    # Load dataset\n",
    "    ds = load_dataset(\"kings-crown/Isabelle_SFT\", trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "    for split in ds.keys():\n",
    "        df = ds[split].to_pandas()\n",
    "        df[\"task_id\"] = np.arange(len(df))\n",
    "\n",
    "        # Clean text fields\n",
    "        for col in [\"natural_language_statement\", \"isabelle_translation\", \"formal_proof\", \"isabelle_body\"]:\n",
    "            df[col] = df[col].apply(clean_text)\n",
    "\n",
    "        # Filter dataset to remove problematic entries\n",
    "        df = filter_valid_entries(df)\n",
    "\n",
    "        # Select only relevant fields\n",
    "        df = df[[\"task_id\", \"natural_language_statement\", \"isabelle_translation\", \"formal_proof\", \"isabelle_body\"]]\n",
    "\n",
    "        # Deduplicate by `natural_language_statement`\n",
    "        df = df.drop_duplicates(subset=[\"natural_language_statement\"])\n",
    "\n",
    "        # Save to JSONL format\n",
    "        df.to_json(\n",
    "            f\"scripts/data/isabelle/{split}.jsonl\",\n",
    "            lines=True,\n",
    "            orient=\"records\",\n",
    "            force_ascii=False,\n",
    "        )\n",
    "        del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Load dataset\n",
    "    ds = load_dataset(\"kings-crown/Isabelle_SFT\", trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "\n",
    "    for split in ds.keys():\n",
    "        df = ds[split].to_pandas()\n",
    "        df[\"task_id\"] = np.arange(len(df))\n",
    "\n",
    "        # Clean text fields\n",
    "        for col in [\"natural_language_statement\", \"isabelle_translation\", \"formal_proof\", \"isabelle_body\"]:\n",
    "            df[col] = df[col].apply(clean_text)\n",
    "\n",
    "        # Filter dataset to remove problematic entries\n",
    "        df = filter_valid_entries(df)\n",
    "\n",
    "        # Select only relevant fields\n",
    "        df = df[[\"task_id\", \"natural_language_statement\", \"isabelle_translation\", \"formal_proof\", \"isabelle_body\"]]\n",
    "\n",
    "        # Deduplicate by `natural_language_statement`\n",
    "        df = df.drop_duplicates(subset=[\"natural_language_statement\"])\n",
    "\n",
    "        # Save to JSONL format\n",
    "        df.to_json(\n",
    "            f\"{split}.jsonl\",\n",
    "            lines=True,\n",
    "            orient=\"records\",\n",
    "            force_ascii=False,\n",
    "        )\n",
    "        del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
