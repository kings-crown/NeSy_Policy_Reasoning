{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted characters and clean text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Normalize new lines\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove excessive spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "def filter_valid_entries(df):\n",
    "    \"\"\"Filter dataset entries to remove invalid data.\"\"\"\n",
    "    # Remove rows where any important column is missing\n",
    "    df = df.dropna(subset=[\"natural_language_statement\"])\n",
    "\n",
    "    # Remove problems containing \"<image>\", \"<span\", or weird HTML-like patterns\n",
    "    df = df[\n",
    "        ~df[\"natural_language_statement\"].str.contains(\"<image>|<span \", regex=True, na=False)\n",
    "    ]\n",
    "\n",
    "    # Filter based on proof length (keeping it reasonable for training)\n",
    "    df[\"proof_length\"] = df[\"formal_proof\"].apply(\n",
    "        lambda x: len(tokenizer.tokenize(x)) if isinstance(x, str) else 0\n",
    "    )\n",
    "    df = df[df[\"proof_length\"] < 2048]\n",
    "    df = df[df[\"proof_length\"] > 64]\n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_valid_entries_fevl(df):\n",
    "    \"\"\"Filter dataset entries to remove invalid data.\"\"\"\n",
    "    # Remove rows where any important column is missing\n",
    "    df = df.dropna(subset=[\"natural_language_statement\"])\n",
    "\n",
    "    # Remove problems containing \"<image>\", \"<span\", or weird HTML-like patterns\n",
    "    df = df[\n",
    "        ~df[\"natural_language_statement\"].str.contains(\"<image>|<span \", regex=True, na=False)\n",
    "    ]\n",
    "\n",
    "    # Filter based on proof length (keeping it reasonable for training)\n",
    "    df[\"proof_length\"] = df[\"formal_proof\"].apply(\n",
    "        lambda x: len(tokenizer.tokenize(x)) if isinstance(x, str) else 0\n",
    "    )\n",
    "    df = df[df[\"proof_length\"] < 2048]\n",
    "    df = df[df[\"proof_length\"] > 64]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/data/isabelle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3919063/3856572889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/isabelle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kings-crown/Isabelle_SFT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/data/isabelle'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"/data/isabelle\", exist_ok=True)\n",
    "    \n",
    "    # Load dataset\n",
    "    ds = load_dataset(\"kings-crown/Isabelle_SFT\", trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "    for split in ds.keys():\n",
    "        df = ds[split].to_pandas()\n",
    "        df[\"task_id\"] = np.arange(len(df))\n",
    "\n",
    "        # Clean text fields\n",
    "        for col in [\"natural_language_statement\", \"isabelle_translation\", \"formal_proof\", \"isabelle_body\"]:\n",
    "            df[col] = df[col].apply(clean_text)\n",
    "\n",
    "        # Filter dataset to remove problematic entries\n",
    "        df = filter_valid_entries(df)\n",
    "\n",
    "        # Select only relevant fields\n",
    "        df = df[[\"task_id\", \"natural_language_statement\", \"isabelle_translation\", \"formal_proof\", \"isabelle_body\"]]\n",
    "\n",
    "        # Deduplicate by `natural_language_statement`\n",
    "        df = df.drop_duplicates(subset=[\"natural_language_statement\"])\n",
    "\n",
    "        # Save to JSONL format\n",
    "        df.to_json(\n",
    "            f\"scripts/data/isabelle/{split}.jsonl\",\n",
    "            lines=True,\n",
    "            orient=\"records\",\n",
    "            force_ascii=False,\n",
    "        )\n",
    "        del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Load dataset\n",
    "    ds = load_dataset(\"kings-crown/SFT_Isabelle_mvp\", trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "    for split in ds.keys():\n",
    "        df = ds[split].to_pandas()\n",
    "        df[\"task_id\"] = np.arange(len(df))\n",
    "\n",
    "        # Clean text fields\n",
    "        #for col in [\"natural_language_statement\", \"isabelle_translation\", \"formal_proof\", \"isabelle_body\"]:\n",
    "        for col in [\"natural_language_statement\", \"formal_proof\"]:\n",
    "            df[col] = df[col].apply(clean_text)\n",
    "\n",
    "        # Filter dataset to remove problematic entries\n",
    "        df = filter_valid_entries(df)\n",
    "\n",
    "        # Select only relevant fields\n",
    "        #df = df[[\"task_id\", \"natural_language_statement\", \"isabelle_translation\", \"formal_proof\", \"isabelle_body\"]]\n",
    "        df = df[[\"task_id\", \"natural_language_statement\", \"formal_proof\"]]\n",
    "\n",
    "        # Deduplicate by `natural_language_statement`\n",
    "        df = df.drop_duplicates(subset=[\"natural_language_statement\"])\n",
    "\n",
    "        # Save to JSONL format\n",
    "        df.to_json(\n",
    "            f\"{split}.jsonl\",\n",
    "            lines=True,\n",
    "            orient=\"records\",\n",
    "            force_ascii=False,\n",
    "        )\n",
    "        del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
