{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import gym\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import openai\n",
    "import peft\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    TrainerCallback,\n",
    "    AdamW,\n",
    ")\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import LlamaTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from peft import LoraConfig\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb  6 19:04:31 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  | 00000000:2D:00.0 Off |                  Off |\n",
      "| 30%   39C    P8               9W / 300W |     13MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               On  | 00000000:41:00.0  On |                  Off |\n",
      "| 32%   59C    P5              52W / 300W |   1089MiB / 49140MiB |     40%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     89750      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     89750      G   /usr/lib/xorg/Xorg                          461MiB |\n",
      "|    1   N/A  N/A     90047      G   /usr/bin/gnome-shell                        122MiB |\n",
      "|    1   N/A  N/A    180148      G   ...erProcess --variations-seed-version      343MiB |\n",
      "|    1   N/A  N/A    183237      G   ...irefox/5647/usr/lib/firefox/firefox       37MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMFunction(object):\n",
    "    def __init__(self, engine='gpt-4', max_tokens=512):\n",
    "        self.engine = engine\n",
    "        self.max_tokens = max_tokens\n",
    "        self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    def _call_api(self, prompt, engine, max_tokens, max_retries=10, retry_wait=2):\n",
    "        for i in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=engine,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=1.0\n",
    "                )\n",
    "                return response\n",
    "            except openai.APIError as e:  # New error handling\n",
    "                time.sleep(retry_wait)\n",
    "        return {'choices': [{'message': {'content': ''}}]}\n",
    "\n",
    "    def _parse_message(self, msg):\n",
    "        try:\n",
    "            content = msg.choices[0].message.content\n",
    "        except (IndexError, AttributeError):\n",
    "            content = ''\n",
    "        return content\n",
    "\n",
    "    def f(self, prompt, x):\n",
    "        msg = self._call_api(\n",
    "            prompt=prompt+x,\n",
    "            engine=self.engine,\n",
    "            max_tokens=self.max_tokens\n",
    "        )\n",
    "        evaluation = self._parse_message(msg)\n",
    "        return evaluation\n",
    "\n",
    "\n",
    "class Checker(object):\n",
    "    \"\"\"A modified version of the Draft, Sketch, Prove proof-checking client.\n",
    "    (https://github.com/albertqjiang/draft_sketch_prove/blob/main/autoformalization/checker.py)\n",
    "\n",
    "    This checker supports Isabelle2022 via the new version of PISA\n",
    "    (https://albertqjiang.github.io/Portal-to-ISAbelle/).\n",
    "\n",
    "    It supports checking a miniF2F-style proof via `check`.\n",
    "\n",
    "    Finally, it replaces `sledgehammer` with a call to `normalhammer`.\n",
    "    \"\"\"\n",
    "    def __init__(self, working_dir, isa_path, theory_file, port=9000):\n",
    "        sys.path.append(os.environ['PISA_PATH'])\n",
    "        try:\n",
    "            from pisa_client import initialise_env # type: ignore\n",
    "            self.initialise_env = initialise_env\n",
    "        except:\n",
    "            print(\"Set $PISA_PATH to /yourpath/to/Portal-to-ISAbelle/src/main/python\")\n",
    "\n",
    "        self.working_dir = working_dir\n",
    "        self.isa_path = isa_path\n",
    "        self.theory_file = theory_file\n",
    "        self.port = port\n",
    "\n",
    "    def _initialize(self):\n",
    "        env = self.initialise_env(\n",
    "            self.port,\n",
    "            isa_path=self.isa_path,\n",
    "            theory_file_path=self.theory_file,\n",
    "            working_directory=self.working_dir\n",
    "        )\n",
    "        return env\n",
    "\n",
    "    def _exit(self, env):\n",
    "        try:\n",
    "            env.post('exit')\n",
    "        except:\n",
    "            print(\"env.post('exit') timed out\")\n",
    "            pass\n",
    "        os.system(\"ps aux | grep Isabelle | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "        os.system(\"ps aux | grep poly | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "\n",
    "    def _parse_output(self, obs):\n",
    "        \"\"\"Parse the sledgehammer output, otherwise return an empty string\"\"\"\n",
    "        if '<hammer>' in obs:\n",
    "            output = obs.split('<hammer>')[0]\n",
    "        else:\n",
    "            output = ''\n",
    "        return output\n",
    "\n",
    "    def _run_step(self, step, i, tls_name, env):\n",
    "        obs, reward, done, metadata = env.step_to_top_level_state(\n",
    "            action=step,\n",
    "            tls_name=tls_name,\n",
    "            new_name='default_%d' % i\n",
    "        )\n",
    "        error = None\n",
    "        if 'error:' in obs or 'Step error' in obs or 'Unknown error' in obs:\n",
    "            error = obs\n",
    "        return obs, reward, done, metadata, error\n",
    "\n",
    "    def _run_sledgehammer(self, step, i, tls_name, env):\n",
    "        # First try heuristics\n",
    "        for heuristic in ['by auto', 'by simp', 'by blast', 'by fastforce', 'by force', 'by eval', 'by presburger', 'by sos', 'by arith', 'by linarith', 'by (auto simp: field_simps)']:\n",
    "            step_ = step.replace('normalhammer', heuristic)\n",
    "            obs, reward, done, metadata, error = self._run_step(step_, i, tls_name, env)\n",
    "            if error is None:\n",
    "                obs = '%s <hammer> %s' % (heuristic, obs)\n",
    "                return obs, reward, done, metadata, error\n",
    "        # Try sledgehammer\n",
    "        out = self._run_step(step, i, tls_name, env)\n",
    "        return out\n",
    "\n",
    "    def check(self, statement_and_proof):\n",
    "        # Initialize environment\n",
    "        env = self._initialize()\n",
    "        env.initialise()\n",
    "\n",
    "        # Wrap and parse theorem\n",
    "        theory = Checker.wrap_theorem(statement_and_proof)\n",
    "        steps = Checker.get_parsed(env, theory)\n",
    "\n",
    "        result = self._check(env, steps)\n",
    "        return result\n",
    "\n",
    "    def _check(self, env, steps):\n",
    "        done = False\n",
    "        reason = ''\n",
    "        success = False\n",
    "        step_results = []\n",
    "        tls_name = 'default'\n",
    "        for i, step in enumerate(steps):\n",
    "            try:\n",
    "                time0 = time.time()\n",
    "                if 'normalhammer' in step:\n",
    "                    obs, reward, done, metadata, error = self._run_sledgehammer(step, i, tls_name, env)\n",
    "                else:\n",
    "                    obs, reward, done, metadata, error = self._run_step(step, i, tls_name, env)\n",
    "                step_time = time.time() - time0\n",
    "                step_results.append(dict(index=i, step=step, output=self._parse_output(obs), step_time=step_time))\n",
    "                if error is not None:\n",
    "                    reason = error\n",
    "                    success = False\n",
    "                    done = False\n",
    "                    break\n",
    "            except:\n",
    "                # Timeout - end the proof attempt\n",
    "                success = False\n",
    "                done = False\n",
    "                reason = 'timeout (%d)' % len(step_results)\n",
    "                step_results.append(dict(index=i, step=step, output=''))\n",
    "                break\n",
    "\n",
    "            # Change when successful\n",
    "            tls_name = 'default_%d' % i\n",
    "\n",
    "        if done and reward == 1.0:\n",
    "            success = True\n",
    "\n",
    "        result = {\n",
    "            'success': success,\n",
    "            'reason': reason,\n",
    "            'num_steps': len(steps),\n",
    "            'last_step': len(step_results),\n",
    "            'step_results': step_results,\n",
    "            'theorem_and_proof': self.reconstruct(step_results) if success else ''\n",
    "        }\n",
    "        # Exit environment\n",
    "        self._exit(env)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def reconstruct(step_results):\n",
    "        steps = []\n",
    "        for step_result in step_results[1:]:\n",
    "            if step_result['output'] != '':\n",
    "                steps.append(step_result['output'].strip())\n",
    "            else:\n",
    "                steps.append(step_result['step'].strip())\n",
    "        theorem_and_proof = '\\n'.join(steps)\n",
    "        return theorem_and_proof\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_theorem(theorem):\n",
    "        return 'theory Interactive imports HOL.HOL Complex_Main \"HOL-Library.Code_Target_Numeral\" \"HOL-Library.Sum_of_Squares\" \"Symmetric_Polynomials.Vieta\" \"HOL-Computational_Algebra.Computational_Algebra\" \"HOL-Number_Theory.Number_Theory\" \\n begin\\n%s' % theorem\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parsed(env, theory, tls_name='default'):\n",
    "        # HACK: the parsing doesn't work well with `normalhammer`, so we replace\n",
    "        # all hammer calls with sorry, then replace sorry to normalhammer after parsing.\n",
    "        theory = theory.replace('sledgehammer', 'sorry')\n",
    "        theory = theory.replace('normalhammer', 'sorry')\n",
    "\n",
    "        steps = env.post(f\"<parse text> ${theory}\")\n",
    "        steps = steps.split('<SEP>')\n",
    "        steps = [s for s in steps if s.strip() != '']\n",
    "        # remove weird '$' step and whitespace steps\n",
    "        steps = [s for s in steps if s != '$' and s.strip() != '']\n",
    "        steps = [s.replace('sorry', 'normalhammer') for s in steps]\n",
    "        return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ['PISA_PATH'] = '/home/siai/Portal-to-ISAbelle/src/main/python'\n",
    "\n",
    "checker = Checker(\n",
    "    working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
    "    isa_path='/home/siai/Isabelle2022',\n",
    "    theory_file='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "    port=9000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_theorem_data(json_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Loads a JSON file containing an array of objects with:\n",
    "      { \"statement\": \"...\", \"state\": \"...\", \"step\": \"...\" }\n",
    "    Returns a list of dicts with these fields.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_train_val_data(json_path: str, test_size=0.1, random_seed=42):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    train_data, val_data = train_test_split(data, test_size=test_size, random_state=random_seed)\n",
    "    return train_data, val_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_loss(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            batch_size = batch[\"input_ids\"].size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_count += batch_size\n",
    "    model.train()\n",
    "    return total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trl_format(dataset: List[Dict[str,str]]) -> List[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Convert each record { \"statement\", \"state\", \"step\" } into TRL's conversation format:\n",
    "      \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "        {\"role\": \"user\", \"content\": statement + \"\\n\\n\" + state}\n",
    "      ],\n",
    "      \"answer\": step\n",
    "\n",
    "    This single-turn approach: the user \"says\" the lemma & subgoal, the model must produce the final \"step\".\n",
    "    \"\"\"\n",
    "    trl_data = []\n",
    "    for rec in dataset:\n",
    "        stm = rec.get(\"statement\",\"\")\n",
    "        stt = rec.get(\"state\",\"\")\n",
    "        sp  = rec.get(\"step\",\"\")\n",
    "        prompt_content = stm + \"\\n\\n\" + stt\n",
    "        sample = {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "                {\"role\": \"user\",   \"content\": prompt_content}\n",
    "            ],\n",
    "            \"answer\": sp  \n",
    "        }\n",
    "        trl_data.append(sample)\n",
    "    \n",
    "    return trl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineImitationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For supervised training: input = statement + \"\\n\\n\" + state\n",
    "                            target = step\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_data: List[Dict[str,str]]):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        for rec in raw_data:\n",
    "            inp = rec[\"statement\"] + \"\\n\\n\" + rec[\"state\"]\n",
    "            tgt = rec[\"step\"]\n",
    "            self.samples.append((inp, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_collate_fn(batch, tokenizer, max_length=512):\n",
    "    inp_texts, tgt_texts = zip(*batch)\n",
    "    enc = tokenizer(\n",
    "        list(inp_texts),\n",
    "        text_target=list(tgt_texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"labels\": enc[\"labels\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_offline_imitation_learning(\n",
    "    json_path: str,\n",
    "    model_name: str,\n",
    "    output_dir: str,\n",
    "    epochs: int = 1,\n",
    "    lr: float = 1e-5,\n",
    "    max_length: int = 512,\n",
    "    batch_size: int = 2\n",
    "):\n",
    "\n",
    "    # Load data\n",
    "    raw_data = load_theorem_data(json_path)\n",
    "    dataset_obj = OfflineImitationDataset(raw_data)\n",
    "\n",
    "    # Load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.pad_token = \"<PAD>\"\n",
    "    \n",
    "    model.config.use_cache = False   \n",
    "    model.train()\n",
    "    model.cuda()\n",
    "\n",
    "    # Build DataLoader\n",
    "    loader = DataLoader(\n",
    "        dataset_obj,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, max_length)\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train loop\n",
    "    step_idx = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step_idx += 1\n",
    "            print(f\"Epoch {epoch} Step {step_idx}, loss={loss.item():.4f}\")\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Offline training done, saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef generate_proof_step(model, tokenizer, statement, state, max_length=32):\\n    input_text = statement + \"\\n\\n\" + state\\n    enc_in = tokenizer.encode(input_text, return_tensors=\"pt\")\\n\\n    \\n    with torch.no_grad():\\n        out_ids = model.generate(\\n            enc_in,\\n            max_new_tokens=max_length,\\n            do_sample=False,\\n            top_p=1.0,\\n            temperature=1.0,\\n            top_k=0, num_beams=5)\\n\\n    out_text = tokenizer.batch_decode(out_ids[0], skip_special_tokens=True)\\n    return out_text\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_proof_step(model, tokenizer, statement, state, max_length=32):\n",
    "    # Build the input text for the model\n",
    "    input_text = statement + \"\\n\\n\" + state\n",
    "    # Encode\n",
    "    enc_in = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\n",
    "    # Generate\n",
    "    out_ids = model.generate(\n",
    "        enc_in,\n",
    "        max_new_tokens=max_length,\n",
    "        do_sample=False,   # or False for  greedy\n",
    "        top_p=0.9,        \n",
    "        temperature=0.8,  # or 1.0\n",
    "    )\n",
    "    # Decode\n",
    "    out_text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    return out_text\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def generate_proof_step(model, tokenizer, statement, state, max_length=32):\n",
    "    input_text = statement + \"\\n\\n\" + state\n",
    "    enc_in = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            enc_in,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=False,\n",
    "            top_p=1.0,\n",
    "            temperature=1.0,\n",
    "            top_k=0, num_beams=5)\n",
    "\n",
    "    out_text = tokenizer.batch_decode(out_ids[0], skip_special_tokens=True)\n",
    "    return out_text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker_reward_func(prompts, completions, answer, checker, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Single-step approach: \n",
    "      prompts[i][-1]['content'] = statement+state\n",
    "      completions[i][0]['content'] = model's final step\n",
    "    We unify them, pass to checker, parse partial or full success => returns float.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for i in range(len(prompts)):\n",
    "        theorem_text = prompts[i][-1]['content']\n",
    "        step_text = completions[i][0]['content']\n",
    "        combined = f\"{theorem_text}\\n\\n{step_text}\"\n",
    "        result = checker.check(combined)\n",
    "        reason = result[\"reason\"]\n",
    "        if result[\"success\"]:\n",
    "            # Full success\n",
    "            rewards.append(2.0)\n",
    "        elif reason == \"partial\":\n",
    "            rewards.append(0.5)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completions, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    For demonstration, add a small reward if the output contains \"by \" or \"sledgehammer \" or \"normalhammer\" .\n",
    "    \"\"\"\n",
    "    outs = []\n",
    "    for c in completions:\n",
    "        txt = c[0][\"content\"].lower()\n",
    "        if \"by \" or \"sledgehammer \" or \"normalhammer\" in txt:\n",
    "            outs.append(0.1)\n",
    "        else:\n",
    "            outs.append(0.0)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker_reward(prompts, completions, answer, checker=None, **kwargs):\n",
    "    \"\"\"\n",
    "    We clamp the final reward if it is NaN/Inf. \n",
    "    \"\"\"\n",
    "    base_rewards = checker_reward_func(prompts, completions, answer, checker=checker)\n",
    "    new_rewards = []\n",
    "    for r in base_rewards:\n",
    "        if math.isnan(r) or math.isinf(r):\n",
    "            new_rewards.append(0.0)\n",
    "        else:\n",
    "            new_rewards.append(r)\n",
    "    return new_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rl(trainer, val_data_subset, num_samples=200):\n",
    "    \"\"\"\n",
    "    Evaluate the average reward on a subset of the validation data.\n",
    "    We do *greedy* generation (no sampling) to reduce risk of infinite/NaN probabilities.\n",
    "    \"\"\"\n",
    "    subset = random.sample(val_data_subset, min(num_samples, len(val_data_subset)))\n",
    "    prompts = [s[\"prompt\"] for s in subset]\n",
    "    gold_ans = [s[\"answer\"] for s in subset]\n",
    "\n",
    "    # Force do_sample=False, top_p=1.0\n",
    "    completions = trainer.generate(\n",
    "        prompts,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=False,      \n",
    "        top_p=1.0,            \n",
    "        temperature=1.0       \n",
    "    )\n",
    "\n",
    "    c_reward = checker_reward(prompts, completions, gold_ans)\n",
    "    f_reward = format_reward_func(completions)\n",
    "    total_reward = [r1 + r2 for (r1, r2) in zip(c_reward, f_reward)]\n",
    "    avg_reward = sum(total_reward) / len(total_reward) if len(total_reward) > 0 else 0.0\n",
    "\n",
    "    # clamp final\n",
    "    if math.isnan(avg_reward) or math.isinf(avg_reward):\n",
    "        avg_reward = 0.0\n",
    "\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    max_global_steps=100000,\n",
    "    eval_every=100,\n",
    "    patience=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model, validating every 'eval_every' steps.\n",
    "    Stop early if validation fails to improve for 'patience' intervals.\n",
    "\n",
    "    :param model: your HF model\n",
    "    :param train_loader: DataLoader for training\n",
    "    :param val_loader: DataLoader for validation\n",
    "    :param optimizer: optimizer\n",
    "    :param max_global_steps: max steps we allow in total\n",
    "    :param eval_every: how often (in steps) to run validation\n",
    "    :param patience: how many times in a row we allow no improvement\n",
    "    \"\"\"\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    no_improvement_count = 0\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(999999):  # effectively \"infinite\" until we break\n",
    "        for step_idx, batch in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % 50 == 0:\n",
    "                print(f\"[Global Step {global_step}] training loss={loss.item():.4f}\")\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                val_loss = evaluate_validation_loss(model, val_loader)  # your function\n",
    "                print(f\"[Global Step {global_step}] val_loss={val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    print(\"Validation improved!\")\n",
    "                    best_val_loss = val_loss\n",
    "                    no_improvement_count = 0\n",
    "                    # optionally save checkpoint\n",
    "                    model.save_pretrained(\"checkpoint_best\")\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "                    print(f\"No improvement count={no_improvement_count}\")\n",
    "\n",
    "                # ---- Early stopping if patience exceeded ----\n",
    "                if no_improvement_count >= patience:\n",
    "                    print(\"Early stopping: no improvement in val_loss for too long.\")\n",
    "                    return  # or break out of loops\n",
    "\n",
    "            # ---- End if we exceed max steps ----\n",
    "            if global_step >= max_global_steps:\n",
    "                print(\"Reached maximum global steps.\")\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    JSON_DATASET_PATH = \"deduplicated_dataset.json\"\n",
    "    MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "    OFFLINE_SAVE_DIR = \"offline_ckpt\"\n",
    "    OFFLINE_EPOCHS = 2\n",
    "    BATCH_SIZE = 2\n",
    "    LR = 1e-5\n",
    "    MAX_LENGTH = 512\n",
    "    EVAL_EVERY = 1000\n",
    "    PATIENCE = 3\n",
    "\n",
    "\n",
    "    # Initialize Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Load dataset\n",
    "    train_data, val_data = load_train_val_data(JSON_DATASET_PATH, test_size=0.01)\n",
    "    train_dataset = OfflineImitationDataset(train_data)\n",
    "    val_dataset = OfflineImitationDataset(val_data)\n",
    "\n",
    "    # Load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Move model to Accelerator (multi-GPU, FP16 support)\n",
    "    model.config.use_cache = False\n",
    "    model.train()\n",
    "\n",
    "    # Dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, MAX_LENGTH)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, MAX_LENGTH)\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # Prepare everything for Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    # Training Variables\n",
    "    global_step = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    no_improvement_count = 0\n",
    "    stop_early = False\n",
    "\n",
    "    for epoch in range(OFFLINE_EPOCHS):\n",
    "        if stop_early:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        for step_idx, batch in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            # Move batch to the correct device\n",
    "            with accelerator.accumulate(model):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "\n",
    "            # Print training loss every 10 steps\n",
    "            if step_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, step {step_idx}, train loss={loss.item():.4f}\")\n",
    "\n",
    "            # Generate a sample output every 500 steps\n",
    "            if step_idx % 500 == 0 and len(val_data) > 0:\n",
    "                sample = random.choice(val_data)\n",
    "                gen_step = generate_proof_step(model, tokenizer, sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"-\"*50)\n",
    "                print(f\"Sample statement+state:\\n {sample['statement']} {sample['state']}\")\n",
    "                print(\"\\n\\nGenerated step:\\n\", gen_step)\n",
    "                print(\"\\n\\nReference step:\\n\", sample[\"step\"])\n",
    "                print(\"-\"*50)\n",
    "\n",
    "            # Validation after every EVAL_EVERY steps\n",
    "            if global_step % EVAL_EVERY == 0:\n",
    "                model.eval()\n",
    "                val_loss = evaluate_validation_loss(model, val_loader)\n",
    "                print(f\"[Global Step {global_step}] Interim val_loss={val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    no_improvement_count = 0\n",
    "                    print(\"  (New best val_loss!)\")\n",
    "                    accelerator.unwrap_model(model).save_pretrained(\"checkpoint_best\")\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "                    print(f\"  (No improvement. Count={no_improvement_count})\")\n",
    "                    if no_improvement_count >= PATIENCE:\n",
    "                        print(\"Early stopping triggered (no val improvement).\")\n",
    "                        stop_early = True\n",
    "                        break\n",
    "                model.train()\n",
    "\n",
    "        # End of epoch => Final validation\n",
    "        if not stop_early:\n",
    "            model.eval()\n",
    "            val_loss = evaluate_validation_loss(model, val_loader)\n",
    "            print(f\"Epoch {epoch}, validation loss={val_loss:.4f}\")\n",
    "\n",
    "            if len(val_data) > 0:\n",
    "                sample = random.choice(val_data)\n",
    "                gen_step = generate_proof_step(model, tokenizer, sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"Sample statement+state:\\n\", sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"Generated step:\\n\", gen_step)\n",
    "                print(\"Reference step:\\n\", sample[\"step\"])\n",
    "            model.train()\n",
    "\n",
    "    # Save final model\n",
    "    accelerator.unwrap_model(model).save_pretrained(OFFLINE_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(OFFLINE_SAVE_DIR)\n",
    "    print(f\"Offline training done. Model saved to: {OFFLINE_SAVE_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback that prints training info after each logging event (on_log)\n",
    "    and does sample generation every 'sample_every' steps (optional).\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, sample_every=500, val_data=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_every = sample_every\n",
    "        self.val_data = val_data if val_data else []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Called by the trainer after logs are created, typically each 'logging_steps'.\n",
    "        \"\"\"\n",
    "        # Print the logs in a style similar to Script 2\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"Step {state.global_step}, training loss={logs['loss']:.4f}\")\n",
    "        else:\n",
    "            print(f\"Step {state.global_step}, logs={logs}\")\n",
    "\n",
    "        # Generate a sample output if we've hit sample_every steps\n",
    "        if self.sample_every > 0 and state.global_step > 0 and state.global_step % self.sample_every == 0:\n",
    "            if len(self.val_data) > 0:\n",
    "                sample = random.choice(self.val_data)\n",
    "                prompt = [\n",
    "                    {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "                    {\"role\": \"user\",   \"content\": sample[\"statement\"] + \"\\n\\n\" + sample[\"state\"]}\n",
    "                ]\n",
    "                print(f\"(Callback) Generating proof at step {state.global_step}, sample statement[:50]: {sample['statement'][:50]}...\")\n",
    "                with torch.no_grad():\n",
    "                    completion = kwargs[\"trainer\"].generate(\n",
    "                        [prompt],\n",
    "                        max_new_tokens=50,\n",
    "                        do_sample=False,\n",
    "                        top_p=1.0,\n",
    "                        temperature=1.0\n",
    "                    )\n",
    "                out_text = completion[0][0][\"content\"] if completion else \"\"\n",
    "                print(f\"(Callback) RL-generated step => {out_text[:80]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_grpo():\n",
    "    \"\"\"\n",
    "    Multi-GPU RL function using TRL’s GRPO with:\n",
    "      - Chunked incremental training\n",
    "      - Sample output printing every 100 steps\n",
    "      - Standard early stopping based on reward\n",
    "      - RL validation on a held-out set\n",
    "    \"\"\"\n",
    "\n",
    "    # Set environment variables to fix tokenizer issues\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Force CUDA debugging\n",
    "\n",
    "    JSON_DATASET_PATH = \"deduplicated_dataset.json\"\n",
    "    OFFLINE_CKPT = \"offline_ckpt\"  \n",
    "    RL_SAVE_DIR = \"outputs/Qwen-Theorem-GRPO\"\n",
    "\n",
    "    # Training settings\n",
    "    EVAL_EVERY = 1000   \n",
    "    PATIENCE = 3       \n",
    "    MAX_GLOBAL_STEPS = 10000\n",
    "    CHUNK_SIZE = 100    \n",
    "\n",
    "    # Initialize accelerator for multi-GPU support\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Check CUDA Capability\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    print(f\"CUDA Device Capability: {capability}\")\n",
    "    if capability[0] < 8:\n",
    "        print(\"Warning: This GPU may not support bfloat16. Switching to float16 instead.\")\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        torch_dtype = torch.bfloat16\n",
    "\n",
    "    # Setup Isabelle Checker\n",
    "    checker = Checker(\n",
    "        working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
    "        isa_path='/home/siai/Isabelle2022',\n",
    "        theory_file='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "        port=9000\n",
    "    )\n",
    "\n",
    "    # Load dataset & split\n",
    "    train_data, val_data = load_train_val_data(JSON_DATASET_PATH, test_size=0.1)\n",
    "    \n",
    "    print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "    train_trl_data = build_trl_format(train_data)\n",
    "    val_trl_data   = build_trl_format(val_data)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OFFLINE_CKPT)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Ensure Model Loads on Correct Device\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        OFFLINE_CKPT,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        #device_map=\"auto\"\n",
    "    ).to(device)\n",
    "    generation_config = GenerationConfig( do_sample=False, top_k=0, temperature = 1.0)\n",
    "    print(f\"Model loaded successfully on {model.device}\")\n",
    "    #print(model.hf_device_map)  # See how layers are placed\n",
    "\n",
    "    # LoRA configuration (optional)\n",
    "    peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        lora_dropout=0.05,\n",
    "    )\n",
    "\n",
    "    # Reward function combining format + checker\n",
    "    def checker_reward(prompts, completions, answer, **kwargs):\n",
    "        return checker_reward_func(prompts, completions, answer, checker=checker)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=RL_SAVE_DIR,\n",
    "        run_name=\"Qwen-GRPO-theorems\",\n",
    "        learning_rate=5e-7, #1e-6 or 5e-6 is bigger.\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.99,\n",
    "        weight_decay=0.1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=10,\n",
    "        bf16=(torch_dtype == torch.bfloat16),\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_generations=1,\n",
    "        max_prompt_length=512,\n",
    "        max_completion_length=512,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=9999999,\n",
    "        max_grad_norm=0.1,\n",
    "        report_to=\"wandb\",\n",
    "        log_on_each_node=False,        \n",
    "    )\n",
    "\n",
    "    # Initialize Trainer with Accelerate\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[format_reward_func, checker_reward],\n",
    "        args=training_args,\n",
    "        train_dataset=train_trl_data,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(f\"Model Device: {model.device}\")\n",
    "    print(f\"Accelerator Device: {device}\")\n",
    "\n",
    "    best_val_reward = -9999\n",
    "    no_improvement_count = 0\n",
    "    global_step = 0\n",
    "    stop_early = False\n",
    "\n",
    "    print(\"Tokenizer vocab size =\", len(tokenizer))\n",
    "    print(\"Model config vocab size =\", model.config.vocab_size)\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    while not stop_early and global_step < MAX_GLOBAL_STEPS:\n",
    "        steps_to_run = min(CHUNK_SIZE, MAX_GLOBAL_STEPS - global_step)\n",
    "        trainer.args.max_steps = steps_to_run\n",
    "        trainer.args.num_train_epochs = 1\n",
    "        trainer.generation_config.do_sample = False\n",
    "        trainer.generation_config.top_k = None  \n",
    "        trainer.generation_config.num_return_sequences = 1\n",
    "        trainer.generation_config.do_sample=False\n",
    "        print(\"Trainer Generation Config:\", trainer.generation_config) ## debug\n",
    "\n",
    "        print(f\"Training {steps_to_run} steps...\")\n",
    "        trainer.train()\n",
    "        global_step += steps_to_run\n",
    "        print(f\"Finished {steps_to_run} RL steps, Global Step={global_step}\")\n",
    "\n",
    "        # Generate a sample output every chunk\n",
    "        if len(val_data) > 0:\n",
    "            sample = random.choice(val_data)\n",
    "            prompt = [\n",
    "                {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "                {\"role\": \"user\", \"content\": sample[\"statement\"] + \"\\n\\n\" + sample[\"state\"]}\n",
    "            ]\n",
    "\n",
    "            print(f\"Generating proof step for sample: {sample['statement'][:50]}...\")\n",
    "            with torch.no_grad():  # Prevents CUDA memory issues\n",
    "                completions = trainer.generate(\n",
    "                    [prompt], max_new_tokens=50, do_sample=False, top_p=1.0, temperature=1.0\n",
    "                )\n",
    "            gen_text = completions[0][0]['content'] if completions else \"\"\n",
    "            print(f\"RL-generated step: {gen_text[:50]}\")\n",
    "\n",
    "        # RL Validation\n",
    "        if global_step % EVAL_EVERY == 0:\n",
    "            print(f\"Running RL Validation at step {global_step}\")\n",
    "            val_reward = evaluate_rl(trainer, val_trl_data, num_samples=200)\n",
    "            print(f\"[RL Validation] Global Step={global_step}, Val Reward={val_reward:.4f}\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_reward > best_val_reward:\n",
    "                best_val_reward = val_reward\n",
    "                no_improvement_count = 0\n",
    "                print(\"New best RL reward! Saving checkpoint...\")\n",
    "                accelerator.unwrap_model(trainer.model).save_pretrained(\"checkpoint_best_rl\")\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                print(f\"No improvement. Count={no_improvement_count}\")\n",
    "                if no_improvement_count >= PATIENCE:\n",
    "                    print(\"Early stopping triggered (RL val reward not improving).\")\n",
    "                    stop_early = True\n",
    "\n",
    "    print(f\"RL Training Complete! Best Val Reward={best_val_reward:.4f}\")\n",
    "    accelerator.unwrap_model(trainer.model).save_pretrained(RL_SAVE_DIR)\n",
    "    print(f\"RL Final Model Saved in: {RL_SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format to generate isabelle proofs to verify the input prompts:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "where the <reasoning> describes your step-by-step Isabelle-like reasoning, \n",
    "and <answer> is the final succinct proof step or conclusion for the checker.\n",
    "OUTPUT ISABELLE PROOFS ONLY WITHIN </answer>.....</answer> tags.\n",
    "\"\"\"\n",
    "\n",
    "# Example reward functions from your inspiration snippet:\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the text inside <answer>...</answer>.\n",
    "    Returns the extracted content or an empty string if not found.\n",
    "    \"\"\"\n",
    "    if \"<answer>\" not in text:\n",
    "        return \"\"\n",
    "    answer = text.split(\"<answer>\", 1)[-1]\n",
    "    answer = answer.split(\"</answer>\", 1)[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_xml_reasoning(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the text inside <reasoning>...</reasoning>.\n",
    "    Returns the extracted content or an empty string if not found.\n",
    "    \"\"\"\n",
    "    if \"<reasoning>\" not in text:\n",
    "        return \"\"\n",
    "    reasoning = text.split(\"<reasoning>\", 1)[-1]\n",
    "    reasoning = reasoning.split(\"</reasoning>\", 1)[0]\n",
    "    return reasoning.strip()\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward partial presence of <reasoning> and <answer> tags.\"\"\"\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    for c in contents:\n",
    "        reward = 0.0\n",
    "        # Check for presence of <reasoning> or <answer> tags\n",
    "        if \"<reasoning>\" in c and \"</reasoning>\" in c:\n",
    "            reward += 0.1\n",
    "        if \"<answer>\" in c and \"</answer>\" in c:\n",
    "            reward += 0.1\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Reward function that checks if the completion has the exact format:\n",
    "      <reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n\n",
    "    Using a simple regex approach.\n",
    "    \"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n?$\"\n",
    "    # You can make the trailing newline optional.\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    return [1.0 if re.match(pattern, r, flags=re.DOTALL) else 0.0 for r in responses]\n",
    "\n",
    "def isabelle_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Example: If we want to see 'proof' or 'have' or 'thus' keywords that \n",
    "    Isabelle proofs often contain. Just a simple substring check.\n",
    "    \"\"\"\n",
    "    responses = [completion[0][\"content\"].lower() for completion in completions]\n",
    "    reward_list = []\n",
    "    for resp in responses:\n",
    "        reward = 0.0\n",
    "        if \"proof\" in resp:\n",
    "            reward += 0.1\n",
    "        if \"have\" in resp:\n",
    "            reward += 0.1\n",
    "        if \"thus\" in resp:\n",
    "            reward += 0.1\n",
    "        if \"qed\" in resp:\n",
    "            reward += 0.1\n",
    "        reward_list.append(reward)\n",
    "    return reward_list\n",
    "\n",
    "\n",
    "# Additional callback to show logs\n",
    "class PrintCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Prints training info after each logging event,\n",
    "    does a sample generation every N steps, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, sample_every=500, val_data=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_every = sample_every\n",
    "        self.val_data = val_data if val_data else []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"[Step {state.global_step}] training loss={logs['loss']:.4f}\")\n",
    "        else:\n",
    "            print(f\"[Step {state.global_step}] logs={logs}\")\n",
    "        # Sample generation\n",
    "        if self.sample_every > 0 and state.global_step > 0 and state.global_step % self.sample_every == 0:\n",
    "            if len(self.val_data) > 0:\n",
    "                sample = random.choice(self.val_data)\n",
    "                prompt = [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\",   \"content\": sample[\"statement\"] + \"\\n\\n\" + sample[\"state\"]}\n",
    "                ]\n",
    "                print(f\"[Callback] Generating proof at step {state.global_step}, sample statement[:50]: {sample['statement'][:50]} ...\")\n",
    "                with torch.no_grad():\n",
    "                    completion = kwargs[\"trainer\"].generate(\n",
    "                        [prompt],\n",
    "                        max_new_tokens=50,\n",
    "                        do_sample=False,\n",
    "                        top_p=1.0,\n",
    "                        temperature=1.0,\n",
    "                    )\n",
    "                out_text = completion[0][0][\"content\"] if completion else \"\"\n",
    "                print(f\"[Callback] RL-generated step => {out_text[:120]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def main_grpo():\n",
    "    \"\"\"\n",
    "    Multi-GPU RL function using TRL’s GRPO with:\n",
    "      - Chunked incremental training\n",
    "      - Frequent printing & sample generation (via callback)\n",
    "      - Standard early stopping based on reward\n",
    "      - RL validation on a held-out set\n",
    "      - Heavily incentivizes \"thinking\" & \"reasoning\" in Isabelle-like proofs\n",
    "    \"\"\"\n",
    "\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"]   = \"1\"\n",
    "\n",
    "    # 2) Basic config\n",
    "    JSON_DATASET_PATH = \"deduplicated_dataset.json\"\n",
    "    OFFLINE_CKPT = \"offline_ckpt\"  \n",
    "    RL_SAVE_DIR = \"outputs/Qwen-Theorem-GRPO\"\n",
    "\n",
    "    CHUNK_SIZE = int(os.environ.get(\"CHUNK_SIZE\", \"100\"))\n",
    "    EVAL_EVERY = 1000\n",
    "    PATIENCE   = 10\n",
    "    MAX_GLOBAL_STEPS = 10000\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Check GPU\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    print(f\"CUDA Device Capability: {capability}\")\n",
    "    if capability[0] < 8:\n",
    "        print(\"Warning: GPU < SM80 => using float16\")\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        torch_dtype = torch.bfloat16\n",
    "\n",
    "    checker = Checker(\n",
    "        working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
    "        isa_path='/home/siai/Isabelle2022',\n",
    "        theory_file='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "        port=9000\n",
    "    )\n",
    "\n",
    "    train_data, val_data = load_train_val_data(JSON_DATASET_PATH, test_size=0.1)\n",
    "    print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "    train_trl_data = build_trl_format(train_data)\n",
    "    val_trl_data   = build_trl_format(val_data)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OFFLINE_CKPT)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        OFFLINE_CKPT,\n",
    "        torch_dtype=torch_dtype,\n",
    "    ).to(device)\n",
    "    print(f\"Model loaded successfully on {model.device}\")\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        do_sample=False,\n",
    "        top_k=0,\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        lora_dropout=0.05,\n",
    "    )\n",
    "\n",
    "    def checker_reward(prompts, completions, answer, **kwargs):\n",
    "        reward = checker_reward_func(prompts, completions, answer, checker=checker)\n",
    "        #print(\"\\n--- Training Step ---\")\n",
    "        #print(f\"Input: {prompts}\")\n",
    "        #print(f\"Model Output: {completions}\")\n",
    "        #print(f\"Expected Output: {answer}\")\n",
    "        #print(f\"Reward Given: {reward}\")\n",
    "        #print(\"----------------------\\n\")\n",
    "        return reward\n",
    "\n",
    "    reward_func_list = [\n",
    "        xmlcount_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        isabelle_format_reward_func,\n",
    "        checker_reward,\n",
    "    ]\n",
    "\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=RL_SAVE_DIR,\n",
    "        run_name=\"Qwen-GRPO-theorems\",\n",
    "        learning_rate=5e-7,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.99,\n",
    "        weight_decay=0.1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=1,\n",
    "        bf16=(torch_dtype == torch.bfloat16),\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_generations=1,\n",
    "        max_prompt_length=512,\n",
    "        max_completion_length=512,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=9999999,\n",
    "        max_grad_norm=0.1,\n",
    "        report_to=\"wandb\",\n",
    "        log_on_each_node=False,\n",
    "    )\n",
    "\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=reward_func_list,\n",
    "        args=training_args,\n",
    "        train_dataset=train_trl_data,\n",
    "    )\n",
    "\n",
    "    from functools import partial\n",
    "    print_callback = PrintCallback(model, tokenizer, sample_every=200, val_data=val_data)\n",
    "    trainer.add_callback(print_callback)\n",
    "\n",
    "    print(f\"Model Device: {model.device}\")\n",
    "    print(f\"Accelerator Device: {device}\")\n",
    "    print(\"Tokenizer vocab size =\", len(tokenizer))\n",
    "    print(\"Model config vocab size =\", model.config.vocab_size)\n",
    "\n",
    "    best_val_reward = -9999\n",
    "    no_improvement_count = 0\n",
    "    global_step = 0\n",
    "    stop_early = False\n",
    "\n",
    "    while not stop_early and global_step < MAX_GLOBAL_STEPS:\n",
    "        steps_to_run = min(CHUNK_SIZE, MAX_GLOBAL_STEPS - global_step)\n",
    "        trainer.args.max_steps = steps_to_run\n",
    "        trainer.args.num_train_epochs = 1\n",
    "\n",
    "        trainer.generation_config.do_sample = False\n",
    "        trainer.generation_config.top_k = None\n",
    "        trainer.generation_config.num_return_sequences = 1\n",
    "        print(\"Trainer Generation Config:\", trainer.generation_config)\n",
    "\n",
    "        print(f\"Training {steps_to_run} steps...\")\n",
    "        trainer.train()\n",
    "        global_step += steps_to_run\n",
    "        trainer.progress_bar.update(steps_to_run)\n",
    "        trainer.progress_bar.set_postfix(Step=global_step, Training_Loss=trainer.training_loss)\n",
    "        print(f\"Finished {steps_to_run} RL steps, Global Step={global_step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA Device Capability: (8, 6)\n",
      "Train size: 333031, Val size: 37004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c89e0ba28c4d1baf1a1944d02bea87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda:0\n",
      "Model Device: cuda:0\n",
      "Accelerator Device: cuda\n",
      "Tokenizer vocab size = 151665\n",
      "Model config vocab size = 152064\n",
      "Trainer Generation Config: GenerationConfig {\n",
      "  \"max_new_tokens\": 512,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.9,\n",
      "  \"top_k\": null\n",
      "}\n",
      "\n",
      "Training 100 steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbalaji-vir1997\u001b[0m (\u001b[33mbalaji-vir1997-stevens-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/siai/NeSy_T/wandb/run-20250206_190439-2zin9923</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/2zin9923' target=\"_blank\">Qwen-GRPO-theorems</a></strong> to <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/2zin9923' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/2zin9923</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_grpo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
