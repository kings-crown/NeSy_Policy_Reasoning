{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from transformers import BitsAndBytesConfig\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    ")\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from transformers import TrainerCallback\n",
    "from peft import LoraConfig\n",
    "from accelerate import Accelerator\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    ")\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "from transformers import LlamaTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb  6 18:28:23 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40-48Q      On   | 00000000:02:01.0 Off |                    0 |\n",
      "| N/A   N/A    P8    N/A /  N/A |     47MiB / 49152MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1757      G   /usr/lib/xorg/Xorg                 23MiB |\n",
      "|    0   N/A  N/A   2396036      G   /usr/lib/xorg/Xorg                 23MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMFunction(object):\n",
    "    def __init__(self, engine='gpt-4', max_tokens=512):\n",
    "        self.engine = engine\n",
    "        self.max_tokens = max_tokens\n",
    "        self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    def _call_api(self, prompt, engine, max_tokens, max_retries=10, retry_wait=2):\n",
    "        for i in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=engine,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=1.0\n",
    "                )\n",
    "                return response\n",
    "            except openai.APIError as e:  # New error handling\n",
    "                time.sleep(retry_wait)\n",
    "        return {'choices': [{'message': {'content': ''}}]}\n",
    "\n",
    "    def _parse_message(self, msg):\n",
    "        try:\n",
    "            content = msg.choices[0].message.content\n",
    "        except (IndexError, AttributeError):\n",
    "            content = ''\n",
    "        return content\n",
    "\n",
    "    def f(self, prompt, x):\n",
    "        msg = self._call_api(\n",
    "            prompt=prompt+x,\n",
    "            engine=self.engine,\n",
    "            max_tokens=self.max_tokens\n",
    "        )\n",
    "        evaluation = self._parse_message(msg)\n",
    "        return evaluation\n",
    "\n",
    "\n",
    "class Checker(object):\n",
    "    \"\"\"A modified version of the Draft, Sketch, Prove proof-checking client.\n",
    "    (https://github.com/albertqjiang/draft_sketch_prove/blob/main/autoformalization/checker.py)\n",
    "\n",
    "    This checker supports Isabelle2022 via the new version of PISA\n",
    "    (https://albertqjiang.github.io/Portal-to-ISAbelle/).\n",
    "\n",
    "    It supports checking a miniF2F-style proof via `check`.\n",
    "\n",
    "    Finally, it replaces `sledgehammer` with a call to `normalhammer`.\n",
    "    \"\"\"\n",
    "    def __init__(self, working_dir, isa_path, theory_file, port=9000):\n",
    "        sys.path.append(os.environ['PISA_PATH'])\n",
    "        try:\n",
    "            from pisa_client import initialise_env\n",
    "            self.initialise_env = initialise_env\n",
    "        except:\n",
    "            print(\"Set $PISA_PATH to /yourpath/to/Portal-to-ISAbelle/src/main/python\")\n",
    "\n",
    "        self.working_dir = working_dir\n",
    "        self.isa_path = isa_path\n",
    "        self.theory_file = theory_file\n",
    "        self.port = port\n",
    "\n",
    "    def _initialize(self):\n",
    "        env = self.initialise_env(\n",
    "            self.port,\n",
    "            isa_path=self.isa_path,\n",
    "            theory_file_path=self.theory_file,\n",
    "            working_directory=self.working_dir\n",
    "        )\n",
    "        return env\n",
    "\n",
    "    def _exit(self, env):\n",
    "        try:\n",
    "            env.post('exit')\n",
    "        except:\n",
    "            print(\"env.post('exit') timed out\")\n",
    "            pass\n",
    "        os.system(\"ps aux | grep Isabelle | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "        os.system(\"ps aux | grep poly | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "\n",
    "    def _parse_output(self, obs):\n",
    "        \"\"\"Parse the sledgehammer output, otherwise return an empty string\"\"\"\n",
    "        if '<hammer>' in obs:\n",
    "            output = obs.split('<hammer>')[0]\n",
    "        else:\n",
    "            output = ''\n",
    "        return output\n",
    "\n",
    "    def _run_step(self, step, i, tls_name, env):\n",
    "        obs, reward, done, metadata = env.step_to_top_level_state(\n",
    "            action=step,\n",
    "            tls_name=tls_name,\n",
    "            new_name='default_%d' % i\n",
    "        )\n",
    "        error = None\n",
    "        if 'error:' in obs or 'Step error' in obs or 'Unknown error' in obs:\n",
    "            error = obs\n",
    "        return obs, reward, done, metadata, error\n",
    "\n",
    "    def _run_sledgehammer(self, step, i, tls_name, env):\n",
    "        # First try heuristics\n",
    "        for heuristic in ['by auto', 'by simp', 'by blast', 'by fastforce', 'by force', 'by eval', 'by presburger', 'by sos', 'by arith', 'by linarith', 'by (auto simp: field_simps)']:\n",
    "            step_ = step.replace('normalhammer', heuristic)\n",
    "            obs, reward, done, metadata, error = self._run_step(step_, i, tls_name, env)\n",
    "            if error is None:\n",
    "                obs = '%s <hammer> %s' % (heuristic, obs)\n",
    "                return obs, reward, done, metadata, error\n",
    "        # Try sledgehammer\n",
    "        out = self._run_step(step, i, tls_name, env)\n",
    "        return out\n",
    "\n",
    "    def check(self, statement_and_proof):\n",
    "        # Initialize environment\n",
    "        env = self._initialize()\n",
    "        env.initialise()\n",
    "\n",
    "        # Wrap and parse theorem\n",
    "        theory = Checker.wrap_theorem(statement_and_proof)\n",
    "        steps = Checker.get_parsed(env, theory)\n",
    "\n",
    "        result = self._check(env, steps)\n",
    "        return result\n",
    "\n",
    "    def _check(self, env, steps):\n",
    "        done = False\n",
    "        reason = ''\n",
    "        success = False\n",
    "        step_results = []\n",
    "        tls_name = 'default'\n",
    "        for i, step in enumerate(steps):\n",
    "            try:\n",
    "                time0 = time.time()\n",
    "                if 'normalhammer' in step:\n",
    "                    obs, reward, done, metadata, error = self._run_sledgehammer(step, i, tls_name, env)\n",
    "                else:\n",
    "                    obs, reward, done, metadata, error = self._run_step(step, i, tls_name, env)\n",
    "                step_time = time.time() - time0\n",
    "                step_results.append(dict(index=i, step=step, output=self._parse_output(obs), step_time=step_time))\n",
    "                if error is not None:\n",
    "                    reason = error\n",
    "                    success = False\n",
    "                    done = False\n",
    "                    break\n",
    "            except:\n",
    "                # Timeout - end the proof attempt\n",
    "                success = False\n",
    "                done = False\n",
    "                reason = 'timeout (%d)' % len(step_results)\n",
    "                step_results.append(dict(index=i, step=step, output=''))\n",
    "                break\n",
    "\n",
    "            # Change when successful\n",
    "            tls_name = 'default_%d' % i\n",
    "\n",
    "        if done and reward == 1.0:\n",
    "            success = True\n",
    "\n",
    "        result = {\n",
    "            'success': success,\n",
    "            'reason': reason,\n",
    "            'num_steps': len(steps),\n",
    "            'last_step': len(step_results),\n",
    "            'step_results': step_results,\n",
    "            'theorem_and_proof': self.reconstruct(step_results) if success else ''\n",
    "        }\n",
    "        # Exit environment\n",
    "        self._exit(env)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def reconstruct(step_results):\n",
    "        steps = []\n",
    "        for step_result in step_results[1:]:\n",
    "            if step_result['output'] != '':\n",
    "                steps.append(step_result['output'].strip())\n",
    "            else:\n",
    "                steps.append(step_result['step'].strip())\n",
    "        theorem_and_proof = '\\n'.join(steps)\n",
    "        return theorem_and_proof\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_theorem(theorem):\n",
    "        return 'theory Interactive imports HOL.HOL Complex_Main \"HOL-Library.Code_Target_Numeral\" \"HOL-Library.Sum_of_Squares\" \"Symmetric_Polynomials.Vieta\" \"HOL-Computational_Algebra.Computational_Algebra\" \"HOL-Number_Theory.Number_Theory\" \\n begin\\n%s' % theorem\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parsed(env, theory, tls_name='default'):\n",
    "        # HACK: the parsing doesn't work well with `normalhammer`, so we replace\n",
    "        # all hammer calls with sorry, then replace sorry to normalhammer after parsing.\n",
    "        theory = theory.replace('sledgehammer', 'sorry')\n",
    "        theory = theory.replace('normalhammer', 'sorry')\n",
    "\n",
    "        steps = env.post(f\"<parse text> ${theory}\")\n",
    "        steps = steps.split('<SEP>')\n",
    "        steps = [s for s in steps if s.strip() != '']\n",
    "        # remove weird '$' step and whitespace steps\n",
    "        steps = [s for s in steps if s != '$' and s.strip() != '']\n",
    "        steps = [s.replace('sorry', 'normalhammer') for s in steps]\n",
    "        return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "os.environ['PISA_PATH'] = '/home/balaji/Desktop/Theorem_Proving/Portal-to-ISAbelle/src/main/python'\n",
    "\n",
    "checker = Checker(\n",
    "    working_dir='/home/balaji/Desktop/Theorem_Proving/Isabelle2022/src/HOL/Examples',\n",
    "    isa_path='/home/balaji/Desktop/Theorem_Proving/Isabelle2022',\n",
    "    theory_file='/home/balaji/Desktop/Theorem_Proving/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "    port=9000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_theorem_data(json_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Loads a JSON file containing an array of objects with:\n",
    "      { \"statement\": \"...\", \"state\": \"...\", \"step\": \"...\" }\n",
    "    Returns a list of dicts with these fields.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_train_val_data(json_path: str, test_size=0.1, random_seed=42):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    train_data, val_data = train_test_split(data, test_size=test_size, random_state=random_seed)\n",
    "    return train_data, val_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_loss(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            batch_size = batch[\"input_ids\"].size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_count += batch_size\n",
    "    model.train()\n",
    "    return total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trl_format(dataset: List[Dict[str,str]]) -> List[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Convert each record { \"statement\", \"state\", \"step\" } into TRL's conversation format:\n",
    "      \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "        {\"role\": \"user\", \"content\": statement + \"\\n\\n\" + state}\n",
    "      ],\n",
    "      \"answer\": step\n",
    "\n",
    "    This single-turn approach: the user \"says\" the lemma & subgoal, the model must produce the final \"step\".\n",
    "    \"\"\"\n",
    "    trl_data = []\n",
    "    for rec in dataset:\n",
    "        stm = rec.get(\"statement\",\"\")\n",
    "        stt = rec.get(\"state\",\"\")\n",
    "        sp  = rec.get(\"step\",\"\")\n",
    "        prompt_content = stm + \"\\n\\n\" + stt\n",
    "        sample = {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "                {\"role\": \"user\",   \"content\": prompt_content}\n",
    "            ],\n",
    "            \"answer\": sp  \n",
    "        }\n",
    "        trl_data.append(sample)\n",
    "    \n",
    "    return trl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineImitationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For supervised training: input = statement + \"\\n\\n\" + state\n",
    "                            target = step\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_data: List[Dict[str,str]]):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        for rec in raw_data:\n",
    "            inp = rec[\"statement\"] + \"\\n\\n\" + rec[\"state\"]\n",
    "            tgt = rec[\"step\"]\n",
    "            self.samples.append((inp, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_collate_fn(batch, tokenizer, max_length=512):\n",
    "    inp_texts, tgt_texts = zip(*batch)\n",
    "    enc = tokenizer(\n",
    "        list(inp_texts),\n",
    "        text_target=list(tgt_texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"labels\": enc[\"labels\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_offline_imitation_learning(\n",
    "    json_path: str,\n",
    "    model_name: str,\n",
    "    output_dir: str,\n",
    "    epochs: int = 1,\n",
    "    lr: float = 1e-5,\n",
    "    max_length: int = 512,\n",
    "    batch_size: int = 2\n",
    "):\n",
    "\n",
    "    # Load data\n",
    "    raw_data = load_theorem_data(json_path)\n",
    "    dataset_obj = OfflineImitationDataset(raw_data)\n",
    "\n",
    "    # Load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.pad_token = \"<PAD>\"\n",
    "    \n",
    "    model.config.use_cache = False   \n",
    "    model.train()\n",
    "    model.cuda()\n",
    "\n",
    "    # Build DataLoader\n",
    "    loader = DataLoader(\n",
    "        dataset_obj,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, max_length)\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train loop\n",
    "    step_idx = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step_idx += 1\n",
    "            print(f\"Epoch {epoch} Step {step_idx}, loss={loss.item():.4f}\")\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Offline training done, saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef generate_proof_step(model, tokenizer, statement, state, max_length=32):\\n    input_text = statement + \"\\n\\n\" + state\\n    enc_in = tokenizer.encode(input_text, return_tensors=\"pt\")\\n\\n    \\n    with torch.no_grad():\\n        out_ids = model.generate(\\n            enc_in,\\n            max_new_tokens=max_length,\\n            do_sample=False,\\n            top_p=1.0,\\n            temperature=1.0,\\n            top_k=0, num_beams=5)\\n\\n    out_text = tokenizer.batch_decode(out_ids[0], skip_special_tokens=True)\\n    return out_text\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_proof_step(model, tokenizer, statement, state, max_length=32):\n",
    "    # Build the input text for the model\n",
    "    input_text = statement + \"\\n\\n\" + state\n",
    "    # Encode\n",
    "    enc_in = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\n",
    "    # Generate\n",
    "    out_ids = model.generate(\n",
    "        enc_in,\n",
    "        max_new_tokens=max_length,\n",
    "        do_sample=False,   # or False for  greedy\n",
    "        top_p=0.9,        \n",
    "        temperature=0.8,  # or 1.0\n",
    "    )\n",
    "    # Decode\n",
    "    out_text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    return out_text\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def generate_proof_step(model, tokenizer, statement, state, max_length=32):\n",
    "    input_text = statement + \"\\n\\n\" + state\n",
    "    enc_in = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            enc_in,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=False,\n",
    "            top_p=1.0,\n",
    "            temperature=1.0,\n",
    "            top_k=0, num_beams=5)\n",
    "\n",
    "    out_text = tokenizer.batch_decode(out_ids[0], skip_special_tokens=True)\n",
    "    return out_text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker_reward_func(prompts, completions, answer, checker, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Single-step approach: \n",
    "      prompts[i][-1]['content'] = statement+state\n",
    "      completions[i][0]['content'] = model's final step\n",
    "    We unify them, pass to checker, parse partial or full success => returns float.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for i in range(len(prompts)):\n",
    "        theorem_text = prompts[i][-1]['content']\n",
    "        step_text = completions[i][0]['content']\n",
    "        combined = f\"{theorem_text}\\n\\n{step_text}\"\n",
    "        result = checker.check(combined)\n",
    "        reason = result[\"reason\"]\n",
    "        if result[\"success\"]:\n",
    "            # Full success\n",
    "            rewards.append(2.0)\n",
    "        elif reason == \"partial\":\n",
    "            rewards.append(0.5)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completions, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    For demonstration, add a small reward if the output contains \"by \" or \"sledgehammer \" or \"normalhammer\" .\n",
    "    \"\"\"\n",
    "    outs = []\n",
    "    for c in completions:\n",
    "        txt = c[0][\"content\"].lower()\n",
    "        if \"by \" or \"sledgehammer \" or \"normalhammer\" in txt:\n",
    "            outs.append(0.1)\n",
    "        else:\n",
    "            outs.append(0.0)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker_reward(prompts, completions, answer, checker=None, **kwargs):\n",
    "    \"\"\"\n",
    "    We clamp the final reward if it is NaN/Inf. \n",
    "    \"\"\"\n",
    "    base_rewards = checker_reward_func(prompts, completions, answer, checker=checker)\n",
    "    new_rewards = []\n",
    "    for r in base_rewards:\n",
    "        if math.isnan(r) or math.isinf(r):\n",
    "            new_rewards.append(0.0)\n",
    "        else:\n",
    "            new_rewards.append(r)\n",
    "    return new_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rl(trainer, val_data_subset, num_samples=200):\n",
    "    \"\"\"\n",
    "    Evaluate the average reward on a subset of the validation data.\n",
    "    We do *greedy* generation (no sampling) to reduce risk of infinite/NaN probabilities.\n",
    "    \"\"\"\n",
    "    subset = random.sample(val_data_subset, min(num_samples, len(val_data_subset)))\n",
    "    prompts = [s[\"prompt\"] for s in subset]\n",
    "    gold_ans = [s[\"answer\"] for s in subset]\n",
    "\n",
    "    # Force do_sample=False, top_p=1.0\n",
    "    completions = trainer.generate(\n",
    "        prompts,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=False,      \n",
    "        top_p=1.0,            \n",
    "        temperature=1.0       \n",
    "    )\n",
    "\n",
    "    c_reward = checker_reward(prompts, completions, gold_ans)\n",
    "    f_reward = format_reward_func(completions)\n",
    "    total_reward = [r1 + r2 for (r1, r2) in zip(c_reward, f_reward)]\n",
    "    avg_reward = sum(total_reward) / len(total_reward) if len(total_reward) > 0 else 0.0\n",
    "\n",
    "    # clamp final\n",
    "    if math.isnan(avg_reward) or math.isinf(avg_reward):\n",
    "        avg_reward = 0.0\n",
    "\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    max_global_steps=100000,\n",
    "    eval_every=100,\n",
    "    patience=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model, validating every 'eval_every' steps.\n",
    "    Stop early if validation fails to improve for 'patience' intervals.\n",
    "\n",
    "    :param model: your HF model\n",
    "    :param train_loader: DataLoader for training\n",
    "    :param val_loader: DataLoader for validation\n",
    "    :param optimizer: optimizer\n",
    "    :param max_global_steps: max steps we allow in total\n",
    "    :param eval_every: how often (in steps) to run validation\n",
    "    :param patience: how many times in a row we allow no improvement\n",
    "    \"\"\"\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    no_improvement_count = 0\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(999999):  # effectively \"infinite\" until we break\n",
    "        for step_idx, batch in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % 50 == 0:\n",
    "                print(f\"[Global Step {global_step}] training loss={loss.item():.4f}\")\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                val_loss = evaluate_validation_loss(model, val_loader)  # your function\n",
    "                print(f\"[Global Step {global_step}] val_loss={val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    print(\"Validation improved!\")\n",
    "                    best_val_loss = val_loss\n",
    "                    no_improvement_count = 0\n",
    "                    # optionally save checkpoint\n",
    "                    model.save_pretrained(\"checkpoint_best\")\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "                    print(f\"No improvement count={no_improvement_count}\")\n",
    "\n",
    "                # ---- Early stopping if patience exceeded ----\n",
    "                if no_improvement_count >= patience:\n",
    "                    print(\"Early stopping: no improvement in val_loss for too long.\")\n",
    "                    return  # or break out of loops\n",
    "\n",
    "            # ---- End if we exceed max steps ----\n",
    "            if global_step >= max_global_steps:\n",
    "                print(\"Reached maximum global steps.\")\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    JSON_DATASET_PATH = \"MagnusData/deduplicated_dataset.json\"\n",
    "    MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    OFFLINE_SAVE_DIR = \"offline_ckpt\"\n",
    "    OFFLINE_EPOCHS = 2\n",
    "    BATCH_SIZE = 2\n",
    "    LR = 1e-5\n",
    "    MAX_LENGTH = 512\n",
    "    EVAL_EVERY = 1000\n",
    "    PATIENCE = 3\n",
    "\n",
    "    # Initialize Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Load dataset\n",
    "    train_data, val_data = load_train_val_data(JSON_DATASET_PATH, test_size=0.01)\n",
    "    train_dataset = OfflineImitationDataset(train_data)\n",
    "    val_dataset = OfflineImitationDataset(val_data)\n",
    "\n",
    "    # Load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Move model to Accelerator (multi-GPU, FP16 support)\n",
    "    model.config.use_cache = False\n",
    "    model.train()\n",
    "\n",
    "    # Dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, MAX_LENGTH)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, MAX_LENGTH)\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # Prepare everything for Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    # Training Variables\n",
    "    global_step = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    no_improvement_count = 0\n",
    "    stop_early = False\n",
    "\n",
    "    for epoch in range(OFFLINE_EPOCHS):\n",
    "        if stop_early:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        for step_idx, batch in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            # Move batch to the correct device\n",
    "            with accelerator.accumulate(model):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "\n",
    "            # Print training loss every 10 steps\n",
    "            if step_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, step {step_idx}, train loss={loss.item():.4f}\")\n",
    "\n",
    "            # Generate a sample output every 500 steps\n",
    "            if step_idx % 500 == 0 and len(val_data) > 0:\n",
    "                sample = random.choice(val_data)\n",
    "                gen_step = generate_proof_step(model, tokenizer, sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"-\"*50)\n",
    "                print(f\"Sample statement+state:\\n {sample['statement']} {sample['state']}\")\n",
    "                print(\"\\n\\nGenerated step:\\n\", gen_step)\n",
    "                print(\"\\n\\nReference step:\\n\", sample[\"step\"])\n",
    "                print(\"-\"*50)\n",
    "\n",
    "            # Validation after every EVAL_EVERY steps\n",
    "            if global_step % EVAL_EVERY == 0:\n",
    "                model.eval()\n",
    "                val_loss = evaluate_validation_loss(model, val_loader)\n",
    "                print(f\"[Global Step {global_step}] Interim val_loss={val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    no_improvement_count = 0\n",
    "                    print(\"  (New best val_loss!)\")\n",
    "                    accelerator.unwrap_model(model).save_pretrained(\"checkpoint_best\")\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "                    print(f\"  (No improvement. Count={no_improvement_count})\")\n",
    "                    if no_improvement_count >= PATIENCE:\n",
    "                        print(\"Early stopping triggered (no val improvement).\")\n",
    "                        stop_early = True\n",
    "                        break\n",
    "                model.train()\n",
    "\n",
    "        # End of epoch => Final validation\n",
    "        if not stop_early:\n",
    "            model.eval()\n",
    "            val_loss = evaluate_validation_loss(model, val_loader)\n",
    "            print(f\"Epoch {epoch}, validation loss={val_loss:.4f}\")\n",
    "\n",
    "            if len(val_data) > 0:\n",
    "                sample = random.choice(val_data)\n",
    "                gen_step = generate_proof_step(model, tokenizer, sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"Sample statement+state:\\n\", sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"Generated step:\\n\", gen_step)\n",
    "                print(\"Reference step:\\n\", sample[\"step\"])\n",
    "            model.train()\n",
    "\n",
    "    # Save final model\n",
    "    accelerator.unwrap_model(model).save_pretrained(OFFLINE_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(OFFLINE_SAVE_DIR)\n",
    "    print(f\"Offline training done. Model saved to: {OFFLINE_SAVE_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback that prints training info after each logging event (on_log)\n",
    "    and does sample generation every 'sample_every' steps (optional).\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, sample_every=500, val_data=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_every = sample_every\n",
    "        self.val_data = val_data if val_data else []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Called by the trainer after logs are created, typically each 'logging_steps'.\n",
    "        \"\"\"\n",
    "        # Print the logs in a style similar to Script 2\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"Step {state.global_step}, training loss={logs['loss']:.4f}\")\n",
    "        else:\n",
    "            print(f\"Step {state.global_step}, logs={logs}\")\n",
    "\n",
    "        # Generate a sample output if we've hit sample_every steps\n",
    "        if self.sample_every > 0 and state.global_step > 0 and state.global_step % self.sample_every == 0:\n",
    "            if len(self.val_data) > 0:\n",
    "                sample = random.choice(self.val_data)\n",
    "                prompt = [\n",
    "                    {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "                    {\"role\": \"user\",   \"content\": sample[\"statement\"] + \"\\n\\n\" + sample[\"state\"]}\n",
    "                ]\n",
    "                print(f\"(Callback) Generating proof at step {state.global_step}, sample statement[:50]: {sample['statement'][:50]}...\")\n",
    "                with torch.no_grad():\n",
    "                    completion = kwargs[\"trainer\"].generate(\n",
    "                        [prompt],\n",
    "                        max_new_tokens=50,\n",
    "                        do_sample=False,\n",
    "                        top_p=1.0,\n",
    "                        temperature=1.0\n",
    "                    )\n",
    "                out_text = completion[0][0][\"content\"] if completion else \"\"\n",
    "                print(f\"(Callback) RL-generated step => {out_text[:80]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_grpo():\n",
    "    \"\"\"\n",
    "    Multi-GPU RL function using TRL’s GRPO with:\n",
    "      - Chunked incremental training\n",
    "      - Sample output printing every 100 steps\n",
    "      - Standard early stopping based on reward\n",
    "      - RL validation on a held-out set\n",
    "    \"\"\"\n",
    "\n",
    "    # Set environment variables to fix tokenizer issues\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Force CUDA debugging\n",
    "\n",
    "    JSON_DATASET_PATH = \"MagnusData/deduplicated_dataset.json\"\n",
    "    OFFLINE_CKPT = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  \n",
    "    RL_SAVE_DIR = \"outputs/Qwen-Theorem-GRPO\"\n",
    "\n",
    "    # Training settings\n",
    "    EVAL_EVERY = 1000   \n",
    "    PATIENCE = 3       \n",
    "    MAX_GLOBAL_STEPS = 10000\n",
    "    CHUNK_SIZE = 100    \n",
    "\n",
    "    # Initialize accelerator for multi-GPU support\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Check CUDA Capability\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    print(f\"CUDA Device Capability: {capability}\")\n",
    "    if capability[0] < 8:\n",
    "        print(\"Warning: This GPU may not support bfloat16. Switching to float16 instead.\")\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        torch_dtype = torch.bfloat16\n",
    "\n",
    "    # Setup Isabelle Checker\n",
    "    checker = Checker(\n",
    "        working_dir='/home/balaji/Desktop/Theorem_Proving/Isabelle2022/src/HOL/Examples',\n",
    "        isa_path='/home/balaji/Desktop/Theorem_Proving/Isabelle2022',\n",
    "        theory_file='/home/balaji/Desktop/Theorem_Proving/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "        port=9000\n",
    "    )\n",
    "\n",
    "    # Load dataset & split\n",
    "    train_data, val_data = load_train_val_data(JSON_DATASET_PATH, test_size=0.1)\n",
    "    \n",
    "    print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "    train_trl_data = build_trl_format(train_data)\n",
    "    val_trl_data   = build_trl_format(val_data)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Ensure Model Loads on Correct Device\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        OFFLINE_CKPT,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        #device_map=\"auto\"\n",
    "    ).to(device)\n",
    "    generation_config = GenerationConfig( do_sample=False, top_k=0, temperature = 1.0)\n",
    "    print(f\"Model loaded successfully on {model.device}\")\n",
    "    #print(model.hf_device_map)  # See how layers are placed\n",
    "\n",
    "    # LoRA configuration (optional)\n",
    "    peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        lora_dropout=0.05,\n",
    "    )\n",
    "\n",
    "    # Reward function combining format + checker\n",
    "    def checker_reward(prompts, completions, answer, **kwargs):\n",
    "        return checker_reward_func(prompts, completions, answer, checker=checker)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=RL_SAVE_DIR,\n",
    "        run_name=\"Qwen-GRPO-theorems\",\n",
    "        learning_rate=5e-7, #1e-6 or 5e-6 is bigger.\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.99,\n",
    "        weight_decay=0.1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=10,\n",
    "        bf16=(torch_dtype == torch.bfloat16),\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_generations=1,\n",
    "        max_prompt_length=512,\n",
    "        max_completion_length=512,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=9999999,\n",
    "        max_grad_norm=0.1,\n",
    "        report_to=\"wandb\",\n",
    "        log_on_each_node=False,        \n",
    "    )\n",
    "\n",
    "    # Initialize Trainer with Accelerate\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[format_reward_func, checker_reward],\n",
    "        args=training_args,\n",
    "        train_dataset=train_trl_data,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(f\"Model Device: {model.device}\")\n",
    "    print(f\"Accelerator Device: {device}\")\n",
    "\n",
    "    best_val_reward = -9999\n",
    "    no_improvement_count = 0\n",
    "    global_step = 0\n",
    "    stop_early = False\n",
    "\n",
    "    print(\"Tokenizer vocab size =\", len(tokenizer))\n",
    "    print(\"Model config vocab size =\", model.config.vocab_size)\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    while not stop_early and global_step < MAX_GLOBAL_STEPS:\n",
    "        steps_to_run = min(CHUNK_SIZE, MAX_GLOBAL_STEPS - global_step)\n",
    "        trainer.args.max_steps = steps_to_run\n",
    "        trainer.args.num_train_epochs = 1\n",
    "        trainer.generation_config.do_sample = False\n",
    "        trainer.generation_config.top_k = None  \n",
    "        trainer.generation_config.num_return_sequences = 1\n",
    "        trainer.generation_config.do_sample=False\n",
    "        print(\"Trainer Generation Config:\", trainer.generation_config) ## debug\n",
    "\n",
    "        print(f\"Training {steps_to_run} steps...\")\n",
    "        trainer.train()\n",
    "        global_step += steps_to_run\n",
    "        print(f\"Finished {steps_to_run} RL steps, Global Step={global_step}\")\n",
    "\n",
    "        # Generate a sample output every chunk\n",
    "        if len(val_data) > 0:\n",
    "            sample = random.choice(val_data)\n",
    "            prompt = [\n",
    "                {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "                {\"role\": \"user\", \"content\": sample[\"statement\"] + \"\\n\\n\" + sample[\"state\"]}\n",
    "            ]\n",
    "\n",
    "            print(f\"Generating proof step for sample: {sample['statement'][:50]}...\")\n",
    "            with torch.no_grad():  # Prevents CUDA memory issues\n",
    "                completions = trainer.generate(\n",
    "                    [prompt], max_new_tokens=50, do_sample=False, top_p=1.0, temperature=1.0\n",
    "                )\n",
    "            gen_text = completions[0][0]['content'] if completions else \"\"\n",
    "            print(f\"RL-generated step: {gen_text[:50]}\")\n",
    "\n",
    "        # RL Validation\n",
    "        if global_step % EVAL_EVERY == 0:\n",
    "            print(f\"Running RL Validation at step {global_step}\")\n",
    "            val_reward = evaluate_rl(trainer, val_trl_data, num_samples=200)\n",
    "            print(f\"[RL Validation] Global Step={global_step}, Val Reward={val_reward:.4f}\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_reward > best_val_reward:\n",
    "                best_val_reward = val_reward\n",
    "                no_improvement_count = 0\n",
    "                print(\"New best RL reward! Saving checkpoint...\")\n",
    "                accelerator.unwrap_model(trainer.model).save_pretrained(\"checkpoint_best_rl\")\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                print(f\"No improvement. Count={no_improvement_count}\")\n",
    "                if no_improvement_count >= PATIENCE:\n",
    "                    print(\"Early stopping triggered (RL val reward not improving).\")\n",
    "                    stop_early = True\n",
    "\n",
    "    print(f\"RL Training Complete! Best Val Reward={best_val_reward:.4f}\")\n",
    "    accelerator.unwrap_model(trainer.model).save_pretrained(RL_SAVE_DIR)\n",
    "    print(f\"RL Final Model Saved in: {RL_SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "where the <reasoning> describes your step-by-step Isabelle-like reasoning, \n",
    "and <answer> is the final succinct proof step or conclusion for the checker.\n",
    "\"\"\"\n",
    "\n",
    "# Example reward functions from your inspiration snippet:\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the text inside <answer>...</answer>.\n",
    "    Returns the extracted content or an empty string if not found.\n",
    "    \"\"\"\n",
    "    if \"<answer>\" not in text:\n",
    "        return \"\"\n",
    "    answer = text.split(\"<answer>\", 1)[-1]\n",
    "    answer = answer.split(\"</answer>\", 1)[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_xml_reasoning(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the text inside <reasoning>...</reasoning>.\n",
    "    Returns the extracted content or an empty string if not found.\n",
    "    \"\"\"\n",
    "    if \"<reasoning>\" not in text:\n",
    "        return \"\"\n",
    "    reasoning = text.split(\"<reasoning>\", 1)[-1]\n",
    "    reasoning = reasoning.split(\"</reasoning>\", 1)[0]\n",
    "    return reasoning.strip()\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward partial presence of <reasoning> and <answer> tags.\"\"\"\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    for c in contents:\n",
    "        reward = 0.0\n",
    "        # Check for presence of <reasoning> or <answer> tags\n",
    "        if \"<reasoning>\" in c and \"</reasoning>\" in c:\n",
    "            reward += 0.5\n",
    "        if \"<answer>\" in c and \"</answer>\" in c:\n",
    "            reward += 0.5\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Reward function that checks if the completion has the exact format:\n",
    "      <reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n\n",
    "    Using a simple regex approach.\n",
    "    \"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n?$\"\n",
    "    # You can make the trailing newline optional.\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    return [1.0 if re.match(pattern, r, flags=re.DOTALL) else 0.0 for r in responses]\n",
    "\n",
    "def isabelle_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Example: If we want to see 'proof' or 'have' or 'thus' keywords that \n",
    "    Isabelle proofs often contain. Just a simple substring check.\n",
    "    \"\"\"\n",
    "    responses = [completion[0][\"content\"].lower() for completion in completions]\n",
    "    reward_list = []\n",
    "    for resp in responses:\n",
    "        reward = 0.0\n",
    "        if \"proof\" in resp:\n",
    "            reward += 0.3\n",
    "        if \"have\" in resp:\n",
    "            reward += 0.3\n",
    "        if \"thus\" in resp:\n",
    "            reward += 0.3\n",
    "        if \"qed\" in resp:\n",
    "            reward += 0.1\n",
    "        reward_list.append(reward)\n",
    "    return reward_list\n",
    "\n",
    "# (Optional) combine your existing 'format_reward_func' with the new XML-based rewards if you want\n",
    "# or use your checker for final correctness.\n",
    "\n",
    "# Additional callback to show logs\n",
    "class PrintCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Prints training info after each logging event,\n",
    "    does a sample generation every N steps, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, sample_every=500, val_data=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_every = sample_every\n",
    "        self.val_data = val_data if val_data else []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"[Step {state.global_step}] training loss={logs['loss']:.4f}\")\n",
    "        else:\n",
    "            print(f\"[Step {state.global_step}] logs={logs}\")\n",
    "        # Sample generation\n",
    "        if self.sample_every > 0 and state.global_step > 0 and state.global_step % self.sample_every == 0:\n",
    "            if len(self.val_data) > 0:\n",
    "                sample = random.choice(self.val_data)\n",
    "                prompt = [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\",   \"content\": sample[\"statement\"] + \"\\n\\n\" + sample[\"state\"]}\n",
    "                ]\n",
    "                print(f\"[Callback] Generating proof at step {state.global_step}, sample statement[:50]: {sample['statement'][:50]} ...\")\n",
    "                with torch.no_grad():\n",
    "                    completion = kwargs[\"trainer\"].generate(\n",
    "                        [prompt],\n",
    "                        max_new_tokens=50,\n",
    "                        do_sample=False,\n",
    "                        top_p=1.0,\n",
    "                        temperature=1.0,\n",
    "                    )\n",
    "                out_text = completion[0][0][\"content\"] if completion else \"\"\n",
    "                print(f\"[Callback] RL-generated step => {out_text[:120]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def main_grpo():\n",
    "    \"\"\"\n",
    "    Multi-GPU RL function using TRL’s GRPO with:\n",
    "      - Chunked incremental training\n",
    "      - Frequent printing & sample generation (via callback)\n",
    "      - Standard early stopping based on reward\n",
    "      - RL validation on a held-out set\n",
    "      - Heavily incentivizes \"thinking\" & \"reasoning\" in Isabelle-like proofs\n",
    "    \"\"\"\n",
    "\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"]   = \"1\"\n",
    "\n",
    "    # 2) Basic config\n",
    "    JSON_DATASET_PATH = \"MagnusData/deduplicated_dataset.json\"\n",
    "    OFFLINE_CKPT = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  \n",
    "    RL_SAVE_DIR = \"outputs/Qwen-Theorem-GRPO\"\n",
    "\n",
    "    CHUNK_SIZE = int(os.environ.get(\"CHUNK_SIZE\", \"100\"))\n",
    "    EVAL_EVERY = 1000\n",
    "    PATIENCE   = 3\n",
    "    MAX_GLOBAL_STEPS = 10000\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Check GPU\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    print(f\"CUDA Device Capability: {capability}\")\n",
    "    if capability[0] < 8:\n",
    "        print(\"Warning: GPU < SM80 => using float16\")\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        torch_dtype = torch.bfloat16\n",
    "\n",
    "    checker = Checker(\n",
    "        working_dir='/home/balaji/Desktop/Theorem_Proving/Isabelle2022/src/HOL/Examples',\n",
    "        isa_path='/home/balaji/Desktop/Theorem_Proving/Isabelle2022',\n",
    "        theory_file='/home/balaji/Desktop/Theorem_Proving/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "        port=9000\n",
    "    )\n",
    "\n",
    "    train_data, val_data = load_train_val_data(JSON_DATASET_PATH, test_size=0.1)\n",
    "    print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "    train_trl_data = build_trl_format(train_data)\n",
    "    val_trl_data   = build_trl_format(val_data)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OFFLINE_CKPT)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        OFFLINE_CKPT,\n",
    "        torch_dtype=torch_dtype,\n",
    "    ).to(device)\n",
    "    print(f\"Model loaded successfully on {model.device}\")\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        do_sample=False,\n",
    "        top_k=0,\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        lora_dropout=0.05,\n",
    "    )\n",
    "\n",
    "    def checker_reward(prompts, completions, answer, **kwargs):\n",
    "        reward = checker_reward_func(prompts, completions, answer, checker=checker)\n",
    "        #print(\"\\n--- Training Step ---\")\n",
    "        #print(f\"Input: {prompts}\")\n",
    "        #print(f\"Model Output: {completions}\")\n",
    "        #print(f\"Expected Output: {answer}\")\n",
    "        #print(f\"Reward Given: {reward}\")\n",
    "        #print(\"----------------------\\n\")\n",
    "        return reward\n",
    "\n",
    "    reward_func_list = [\n",
    "        xmlcount_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        isabelle_format_reward_func,\n",
    "        checker_reward,\n",
    "    ]\n",
    "\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=RL_SAVE_DIR,\n",
    "        run_name=\"Qwen-GRPO-theorems\",\n",
    "        learning_rate=5e-7,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.99,\n",
    "        weight_decay=0.1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=1,\n",
    "        bf16=(torch_dtype == torch.bfloat16),\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_generations=1,\n",
    "        max_prompt_length=512,\n",
    "        max_completion_length=512,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=9999999,\n",
    "        max_grad_norm=0.1,\n",
    "        report_to=\"wandb\",\n",
    "        log_on_each_node=False,\n",
    "    )\n",
    "\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=reward_func_list,\n",
    "        args=training_args,\n",
    "        train_dataset=train_trl_data,\n",
    "    )\n",
    "\n",
    "    from functools import partial\n",
    "    print_callback = PrintCallback(model, tokenizer, sample_every=10, val_data=val_data)\n",
    "    trainer.add_callback(print_callback)\n",
    "\n",
    "    print(f\"Model Device: {model.device}\")\n",
    "    print(f\"Accelerator Device: {device}\")\n",
    "    print(\"Tokenizer vocab size =\", len(tokenizer))\n",
    "    print(\"Model config vocab size =\", model.config.vocab_size)\n",
    "\n",
    "    best_val_reward = -9999\n",
    "    no_improvement_count = 0\n",
    "    global_step = 0\n",
    "    stop_early = False\n",
    "\n",
    "    while not stop_early and global_step < MAX_GLOBAL_STEPS:\n",
    "        steps_to_run = min(CHUNK_SIZE, MAX_GLOBAL_STEPS - global_step)\n",
    "        trainer.args.max_steps = steps_to_run\n",
    "        trainer.args.num_train_epochs = 1\n",
    "\n",
    "        trainer.generation_config.do_sample = False\n",
    "        trainer.generation_config.top_k = None\n",
    "        trainer.generation_config.num_return_sequences = 1\n",
    "        print(\"Trainer Generation Config:\", trainer.generation_config)\n",
    "\n",
    "        print(f\"Training {steps_to_run} steps...\")\n",
    "        trainer.train()\n",
    "        global_step += steps_to_run\n",
    "        trainer.progress_bar.update(steps_to_run)\n",
    "        trainer.progress_bar.set_postfix(Step=global_step, Training_Loss=trainer.training_loss)\n",
    "        print(f\"Finished {steps_to_run} RL steps, Global Step={global_step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA Device Capability: (8, 6)\n",
      "Train size: 333031, Val size: 37004\n",
      "Model loaded successfully on cuda:0\n",
      "Model Device: cuda:0\n",
      "Accelerator Device: cuda\n",
      "Tokenizer vocab size = 151665\n",
      "Model config vocab size = 151936\n",
      "Trainer Generation Config: GenerationConfig {\n",
      "  \"max_new_tokens\": 512,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.9,\n",
      "  \"top_k\": null\n",
      "}\n",
      "\n",
      "Training 100 steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbalaji-vir1997\u001b[0m (\u001b[33mbalaji-vir1997-stevens-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/balaji/Desktop/Theorem_Proving/wandb/run-20250206_182830-qal8mhoz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/qal8mhoz' target=\"_blank\">Qwen-GRPO-theorems</a></strong> to <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/qal8mhoz' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/qal8mhoz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/100 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] training loss=0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain_grpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 148\u001b[0m, in \u001b[0;36mmain_grpo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer Generation Config:\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainer\u001b[38;5;241m.\u001b[39mgeneration_config)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps_to_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m steps...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 148\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m steps_to_run\n\u001b[1;32m    150\u001b[0m trainer\u001b[38;5;241m.\u001b[39mprogress_bar\u001b[38;5;241m.\u001b[39mupdate(steps_to_run)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2184\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2490\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2483\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2484\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2487\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2488\u001b[0m )\n\u001b[1;32m   2489\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2490\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2493\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2494\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2495\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2496\u001b[0m ):\n\u001b[1;32m   2497\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:3598\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3597\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3598\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3600\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3603\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3604\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/trl/trainer/grpo_trainer.py:422\u001b[0m, in \u001b[0;36mGRPOTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# Regular generation path\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n\u001b[0;32m--> 422\u001b[0m         prompt_completion_ids \u001b[38;5;241m=\u001b[39m \u001b[43munwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprompt_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m prompt_length \u001b[38;5;241m=\u001b[39m prompt_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    427\u001b[0m completion_ids \u001b[38;5;241m=\u001b[39m prompt_completion_ids[:, prompt_length:]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:2224\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2216\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2217\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2218\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2219\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2220\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2221\u001b[0m     )\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2237\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2238\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2244\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:3208\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3206\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3208\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3211\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3212\u001b[0m     outputs,\n\u001b[1;32m   3213\u001b[0m     model_kwargs,\n\u001b[1;32m   3214\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3215\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:824\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 824\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    839\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:579\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    568\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    569\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m         position_embeddings,\n\u001b[1;32m    577\u001b[0m     )\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:260\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:192\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m--> 192\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# main diff with Llama\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    205\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/integrations/sdpa_attention.py:30\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msdpa_attention_forward\u001b[39m(\n\u001b[1;32m     19\u001b[0m     module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m     20\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     28\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_key_value_groups\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         value \u001b[38;5;241m=\u001b[39m repeat_kv(value, module\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m     33\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m attention_mask\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/integrations/sdpa_attention.py:15\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[1;32m     14\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\u001b[38;5;241m.\u001b[39mexpand(batch, num_key_value_heads, n_rep, slen, head_dim)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_grpo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
