{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  5 15:50:36 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40-48Q      On   | 00000000:02:01.0 Off |                    0 |\n",
      "| N/A   N/A    P0    N/A /  N/A |     47MiB / 49152MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1757      G   /usr/lib/xorg/Xorg                 23MiB |\n",
      "|    0   N/A  N/A   2396036      G   /usr/lib/xorg/Xorg                 23MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMFunction(object):\n",
    "    def __init__(self, engine='gpt-4', max_tokens=512):\n",
    "        self.engine = engine\n",
    "        self.max_tokens = max_tokens\n",
    "        self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    def _call_api(self, prompt, engine, max_tokens, max_retries=10, retry_wait=2):\n",
    "        for i in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=engine,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=1.0\n",
    "                )\n",
    "                return response\n",
    "            except openai.APIError as e:  # New error handling\n",
    "                time.sleep(retry_wait)\n",
    "        return {'choices': [{'message': {'content': ''}}]}\n",
    "\n",
    "    def _parse_message(self, msg):\n",
    "        try:\n",
    "            content = msg.choices[0].message.content\n",
    "        except (IndexError, AttributeError):\n",
    "            content = ''\n",
    "        return content\n",
    "\n",
    "    def f(self, prompt, x):\n",
    "        msg = self._call_api(\n",
    "            prompt=prompt+x,\n",
    "            engine=self.engine,\n",
    "            max_tokens=self.max_tokens\n",
    "        )\n",
    "        evaluation = self._parse_message(msg)\n",
    "        return evaluation\n",
    "\n",
    "\n",
    "class Checker(object):\n",
    "    \"\"\"A modified version of the Draft, Sketch, Prove proof-checking client.\n",
    "    (https://github.com/albertqjiang/draft_sketch_prove/blob/main/autoformalization/checker.py)\n",
    "\n",
    "    This checker supports Isabelle2022 via the new version of PISA\n",
    "    (https://albertqjiang.github.io/Portal-to-ISAbelle/).\n",
    "\n",
    "    It supports checking a miniF2F-style proof via `check`.\n",
    "\n",
    "    Finally, it replaces `sledgehammer` with a call to `normalhammer`.\n",
    "    \"\"\"\n",
    "    def __init__(self, working_dir, isa_path, theory_file, port=9000):\n",
    "        sys.path.append(os.environ['PISA_PATH'])\n",
    "        try:\n",
    "            from pisa_client import initialise_env\n",
    "            self.initialise_env = initialise_env\n",
    "        except:\n",
    "            print(\"Set $PISA_PATH to /yourpath/to/Portal-to-ISAbelle/src/main/python\")\n",
    "\n",
    "        self.working_dir = working_dir\n",
    "        self.isa_path = isa_path\n",
    "        self.theory_file = theory_file\n",
    "        self.port = port\n",
    "\n",
    "    def _initialize(self):\n",
    "        env = self.initialise_env(\n",
    "            self.port,\n",
    "            isa_path=self.isa_path,\n",
    "            theory_file_path=self.theory_file,\n",
    "            working_directory=self.working_dir\n",
    "        )\n",
    "        return env\n",
    "\n",
    "    def _exit(self, env):\n",
    "        try:\n",
    "            env.post('exit')\n",
    "        except:\n",
    "            print(\"env.post('exit') timed out\")\n",
    "            pass\n",
    "        os.system(\"ps aux | grep Isabelle | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "        os.system(\"ps aux | grep poly | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "\n",
    "    def _parse_output(self, obs):\n",
    "        \"\"\"Parse the sledgehammer output, otherwise return an empty string\"\"\"\n",
    "        if '<hammer>' in obs:\n",
    "            output = obs.split('<hammer>')[0]\n",
    "        else:\n",
    "            output = ''\n",
    "        return output\n",
    "\n",
    "    def _run_step(self, step, i, tls_name, env):\n",
    "        obs, reward, done, metadata = env.step_to_top_level_state(\n",
    "            action=step,\n",
    "            tls_name=tls_name,\n",
    "            new_name='default_%d' % i\n",
    "        )\n",
    "        error = None\n",
    "        if 'error:' in obs or 'Step error' in obs or 'Unknown error' in obs:\n",
    "            error = obs\n",
    "        return obs, reward, done, metadata, error\n",
    "\n",
    "    def _run_sledgehammer(self, step, i, tls_name, env):\n",
    "        # First try heuristics\n",
    "        for heuristic in ['by auto', 'by simp', 'by blast', 'by fastforce', 'by force', 'by eval', 'by presburger', 'by sos', 'by arith', 'by linarith', 'by (auto simp: field_simps)']:\n",
    "            step_ = step.replace('normalhammer', heuristic)\n",
    "            obs, reward, done, metadata, error = self._run_step(step_, i, tls_name, env)\n",
    "            if error is None:\n",
    "                obs = '%s <hammer> %s' % (heuristic, obs)\n",
    "                return obs, reward, done, metadata, error\n",
    "        # Try sledgehammer\n",
    "        out = self._run_step(step, i, tls_name, env)\n",
    "        return out\n",
    "\n",
    "    def check(self, statement_and_proof):\n",
    "        # Initialize environment\n",
    "        env = self._initialize()\n",
    "        env.initialise()\n",
    "\n",
    "        # Wrap and parse theorem\n",
    "        theory = Checker.wrap_theorem(statement_and_proof)\n",
    "        steps = Checker.get_parsed(env, theory)\n",
    "\n",
    "        result = self._check(env, steps)\n",
    "        return result\n",
    "\n",
    "    def _check(self, env, steps):\n",
    "        done = False\n",
    "        reason = ''\n",
    "        success = False\n",
    "        step_results = []\n",
    "        tls_name = 'default'\n",
    "        for i, step in enumerate(steps):\n",
    "            try:\n",
    "                time0 = time.time()\n",
    "                if 'normalhammer' in step:\n",
    "                    obs, reward, done, metadata, error = self._run_sledgehammer(step, i, tls_name, env)\n",
    "                else:\n",
    "                    obs, reward, done, metadata, error = self._run_step(step, i, tls_name, env)\n",
    "                step_time = time.time() - time0\n",
    "                step_results.append(dict(index=i, step=step, output=self._parse_output(obs), step_time=step_time))\n",
    "                if error is not None:\n",
    "                    reason = error\n",
    "                    success = False\n",
    "                    done = False\n",
    "                    break\n",
    "            except:\n",
    "                # Timeout - end the proof attempt\n",
    "                success = False\n",
    "                done = False\n",
    "                reason = 'timeout (%d)' % len(step_results)\n",
    "                step_results.append(dict(index=i, step=step, output=''))\n",
    "                break\n",
    "\n",
    "            # Change when successful\n",
    "            tls_name = 'default_%d' % i\n",
    "\n",
    "        if done and reward == 1.0:\n",
    "            success = True\n",
    "\n",
    "        result = {\n",
    "            'success': success,\n",
    "            'reason': reason,\n",
    "            'num_steps': len(steps),\n",
    "            'last_step': len(step_results),\n",
    "            'step_results': step_results,\n",
    "            'theorem_and_proof': self.reconstruct(step_results) if success else ''\n",
    "        }\n",
    "        # Exit environment\n",
    "        self._exit(env)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def reconstruct(step_results):\n",
    "        steps = []\n",
    "        for step_result in step_results[1:]:\n",
    "            if step_result['output'] != '':\n",
    "                steps.append(step_result['output'].strip())\n",
    "            else:\n",
    "                steps.append(step_result['step'].strip())\n",
    "        theorem_and_proof = '\\n'.join(steps)\n",
    "        return theorem_and_proof\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_theorem(theorem):\n",
    "        return 'theory Interactive imports HOL.HOL Complex_Main \"HOL-Library.Code_Target_Numeral\" \"HOL-Library.Sum_of_Squares\" \"Symmetric_Polynomials.Vieta\" \"HOL-Computational_Algebra.Computational_Algebra\" \"HOL-Number_Theory.Number_Theory\" \\n begin\\n%s' % theorem\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parsed(env, theory, tls_name='default'):\n",
    "        # HACK: the parsing doesn't work well with `normalhammer`, so we replace\n",
    "        # all hammer calls with sorry, then replace sorry to normalhammer after parsing.\n",
    "        theory = theory.replace('sledgehammer', 'sorry')\n",
    "        theory = theory.replace('normalhammer', 'sorry')\n",
    "\n",
    "        steps = env.post(f\"<parse text> ${theory}\")\n",
    "        steps = steps.split('<SEP>')\n",
    "        steps = [s for s in steps if s.strip() != '']\n",
    "        # remove weird '$' step and whitespace steps\n",
    "        steps = [s for s in steps if s != '$' and s.strip() != '']\n",
    "        steps = [s.replace('sorry', 'normalhammer') for s in steps]\n",
    "        return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "os.environ['PISA_PATH'] = '/home/balaji/Desktop/Theorem_Proving/Portal-to-ISAbelle/src/main/python'\n",
    "\n",
    "checker = Checker(\n",
    "    working_dir='/home/balaji/Desktop/Theorem_Proving/Isabelle2022/src/HOL/Examples',\n",
    "    isa_path='/home/balaji/Desktop/Theorem_Proving/Isabelle2022',\n",
    "    theory_file='/home/balaji/Desktop/Theorem_Proving/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "    port=9000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_theorem_data(json_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Loads a JSON file containing an array of objects with:\n",
    "      { \"statement\": \"...\", \"state\": \"...\", \"step\": \"...\" }\n",
    "    Returns a list of dicts with these fields.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_train_val_data(json_path: str, test_size=0.1, random_seed=42):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    train_data, val_data = train_test_split(data, test_size=test_size, random_state=random_seed)\n",
    "    return train_data, val_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_loss(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            batch_size = batch[\"input_ids\"].size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_count += batch_size\n",
    "    model.train()\n",
    "    return total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trl_format(dataset: List[Dict[str,str]]) -> List[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Convert each record { \"statement\", \"state\", \"step\" } into TRL's conversation format:\n",
    "      \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "        {\"role\": \"user\", \"content\": statement + \"\\n\\n\" + state}\n",
    "      ],\n",
    "      \"answer\": step\n",
    "\n",
    "    This single-turn approach: the user \"says\" the lemma & subgoal, the model must produce the final \"step\".\n",
    "    \"\"\"\n",
    "    trl_data = []\n",
    "    for rec in dataset:\n",
    "        stm = rec.get(\"statement\",\"\")\n",
    "        stt = rec.get(\"state\",\"\")\n",
    "        sp  = rec.get(\"step\",\"\")\n",
    "        prompt_content = stm + \"\\n\\n\" + stt\n",
    "        sample = {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "                {\"role\": \"user\",   \"content\": prompt_content}\n",
    "            ],\n",
    "            \"answer\": sp  \n",
    "        }\n",
    "        trl_data.append(sample)\n",
    "    return trl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineImitationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For supervised training: input = statement + \"\\n\\n\" + state\n",
    "                            target = step\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_data: List[Dict[str,str]]):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        for rec in raw_data:\n",
    "            inp = rec[\"statement\"] + \"\\n\\n\" + rec[\"state\"]\n",
    "            tgt = rec[\"step\"]\n",
    "            self.samples.append((inp, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_collate_fn(batch, tokenizer, max_length=512):\n",
    "    inp_texts, tgt_texts = zip(*batch)\n",
    "    enc = tokenizer(\n",
    "        list(inp_texts),\n",
    "        text_target=list(tgt_texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"labels\": enc[\"labels\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_offline_imitation_learning(\n",
    "    json_path: str,\n",
    "    model_name: str,\n",
    "    output_dir: str,\n",
    "    epochs: int = 1,\n",
    "    lr: float = 1e-5,\n",
    "    max_length: int = 512,\n",
    "    batch_size: int = 2\n",
    "):\n",
    "\n",
    "    # Load data\n",
    "    raw_data = load_theorem_data(json_path)\n",
    "    dataset_obj = OfflineImitationDataset(raw_data)\n",
    "\n",
    "    # Load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.pad_token = \"<PAD>\"\n",
    "    \n",
    "    model.config.use_cache = False   \n",
    "    model.train()\n",
    "    model.cuda()\n",
    "\n",
    "    # Build DataLoader\n",
    "    loader = DataLoader(\n",
    "        dataset_obj,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, max_length)\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train loop\n",
    "    step_idx = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step_idx += 1\n",
    "            print(f\"Epoch {epoch} Step {step_idx}, loss={loss.item():.4f}\")\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Offline training done, saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_proof_step(model, tokenizer, statement, state, max_length=32):\n",
    "    # Build the input text for the model\n",
    "    input_text = statement + \"\\n\\n\" + state\n",
    "    # Encode\n",
    "    enc_in = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\n",
    "    # Generate\n",
    "    out_ids = model.generate(\n",
    "        enc_in,\n",
    "        max_new_tokens=max_length,\n",
    "        do_sample=False,   # or False for  greedy\n",
    "        top_p=0.9,        \n",
    "        temperature=0.8,  # or 1.0\n",
    "    )\n",
    "    # Decode\n",
    "    out_text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker_reward_func(prompts, completions, answer, checker, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Single-step approach: \n",
    "      prompts[i][-1]['content'] = statement+state\n",
    "      completions[i][0]['content'] = model's final step\n",
    "    We unify them, pass to checker, parse partial or full success => returns float.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for i in range(len(prompts)):\n",
    "        theorem_text = prompts[i][-1]['content']\n",
    "        step_text = completions[i][0]['content']\n",
    "        combined = f\"{theorem_text}\\n\\n{step_text}\"\n",
    "        result = checker.check(combined)\n",
    "        reason = result[\"reason\"]\n",
    "        if result[\"success\"]:\n",
    "            # Full success\n",
    "            rewards.append(2.0)\n",
    "        elif reason == \"partial\":\n",
    "            rewards.append(0.5)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completions, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    For demonstration, add a small reward if the output contains \"by \" or \"sledgehammer \" or \"normalhammer\" .\n",
    "    \"\"\"\n",
    "    outs = []\n",
    "    for c in completions:\n",
    "        txt = c[0][\"content\"].lower()\n",
    "        if \"by \" or \"sledgehammer \" or \"normalhammer\" in txt:\n",
    "            outs.append(0.1)\n",
    "        else:\n",
    "            outs.append(0.0)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    max_global_steps=100000,\n",
    "    eval_every=100,\n",
    "    patience=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model, validating every 'eval_every' steps.\n",
    "    Stop early if validation fails to improve for 'patience' intervals.\n",
    "\n",
    "    :param model: your HF model\n",
    "    :param train_loader: DataLoader for training\n",
    "    :param val_loader: DataLoader for validation\n",
    "    :param optimizer: optimizer\n",
    "    :param max_global_steps: max steps we allow in total\n",
    "    :param eval_every: how often (in steps) to run validation\n",
    "    :param patience: how many times in a row we allow no improvement\n",
    "    \"\"\"\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    no_improvement_count = 0\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(999999):  # effectively \"infinite\" until we break\n",
    "        for step_idx, batch in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % 50 == 0:\n",
    "                print(f\"[Global Step {global_step}] training loss={loss.item():.4f}\")\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                val_loss = evaluate_validation_loss(model, val_loader)  # your function\n",
    "                print(f\"[Global Step {global_step}] val_loss={val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    print(\"Validation improved!\")\n",
    "                    best_val_loss = val_loss\n",
    "                    no_improvement_count = 0\n",
    "                    # optionally save checkpoint\n",
    "                    model.save_pretrained(\"checkpoint_best\")\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "                    print(f\"No improvement count={no_improvement_count}\")\n",
    "\n",
    "                # ---- Early stopping if patience exceeded ----\n",
    "                if no_improvement_count >= patience:\n",
    "                    print(\"Early stopping: no improvement in val_loss for too long.\")\n",
    "                    return  # or break out of loops\n",
    "\n",
    "            # ---- End if we exceed max steps ----\n",
    "            if global_step >= max_global_steps:\n",
    "                print(\"Reached maximum global steps.\")\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    JSON_DATASET_PATH = \"MagnusData/deduplicated_dataset.json\"\n",
    "    MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    OFFLINE_SAVE_DIR = \"offline_ckpt\"\n",
    "    OFFLINE_EPOCHS = 2\n",
    "    BATCH_SIZE = 2\n",
    "    LR = 1e-5\n",
    "    MAX_LENGTH = 512\n",
    "    EVAL_EVERY = 1000\n",
    "    PATIENCE = 3\n",
    "\n",
    "    # Initialize Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Load dataset\n",
    "    train_data, val_data = load_train_val_data(JSON_DATASET_PATH, test_size=0.01)\n",
    "    train_dataset = OfflineImitationDataset(train_data)\n",
    "    val_dataset = OfflineImitationDataset(val_data)\n",
    "\n",
    "    # Load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Move model to Accelerator (multi-GPU, FP16 support)\n",
    "    model.config.use_cache = False\n",
    "    model.train()\n",
    "\n",
    "    # Dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, MAX_LENGTH)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, MAX_LENGTH)\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # Prepare everything for Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    # Training Variables\n",
    "    global_step = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    no_improvement_count = 0\n",
    "    stop_early = False\n",
    "\n",
    "    for epoch in range(OFFLINE_EPOCHS):\n",
    "        if stop_early:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        for step_idx, batch in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            # Move batch to the correct device\n",
    "            with accelerator.accumulate(model):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "\n",
    "            # Print training loss every 10 steps\n",
    "            if step_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, step {step_idx}, train loss={loss.item():.4f}\")\n",
    "\n",
    "            # Generate a sample output every 500 steps\n",
    "            if step_idx % 500 == 0 and len(val_data) > 0:\n",
    "                sample = random.choice(val_data)\n",
    "                gen_step = generate_proof_step(model, tokenizer, sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"-\"*50)\n",
    "                print(f\"Sample statement+state:\\n {sample['statement']} {sample['state']}\")\n",
    "                print(\"\\n\\nGenerated step:\\n\", gen_step)\n",
    "                print(\"\\n\\nReference step:\\n\", sample[\"step\"])\n",
    "                print(\"-\"*50)\n",
    "\n",
    "            # Validation after every EVAL_EVERY steps\n",
    "            if global_step % EVAL_EVERY == 0:\n",
    "                model.eval()\n",
    "                val_loss = evaluate_validation_loss(model, val_loader)\n",
    "                print(f\"[Global Step {global_step}] Interim val_loss={val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    no_improvement_count = 0\n",
    "                    print(\"  (New best val_loss!)\")\n",
    "                    accelerator.unwrap_model(model).save_pretrained(\"checkpoint_best\")\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "                    print(f\"  (No improvement. Count={no_improvement_count})\")\n",
    "                    if no_improvement_count >= PATIENCE:\n",
    "                        print(\"Early stopping triggered (no val improvement).\")\n",
    "                        stop_early = True\n",
    "                        break\n",
    "                model.train()\n",
    "\n",
    "        # End of epoch => Final validation\n",
    "        if not stop_early:\n",
    "            model.eval()\n",
    "            val_loss = evaluate_validation_loss(model, val_loader)\n",
    "            print(f\"Epoch {epoch}, validation loss={val_loss:.4f}\")\n",
    "\n",
    "            if len(val_data) > 0:\n",
    "                sample = random.choice(val_data)\n",
    "                gen_step = generate_proof_step(model, tokenizer, sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"Sample statement+state:\\n\", sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"Generated step:\\n\", gen_step)\n",
    "                print(\"Reference step:\\n\", sample[\"step\"])\n",
    "            model.train()\n",
    "\n",
    "    # Save final model\n",
    "    accelerator.unwrap_model(model).save_pretrained(OFFLINE_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(OFFLINE_SAVE_DIR)\n",
    "    print(f\"Offline training done. Model saved to: {OFFLINE_SAVE_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_grpo():\n",
    "    \"\"\"\n",
    "    Multi-GPU RL function using TRL’s GRPO with:\n",
    "      - Chunked incremental training\n",
    "      - Sample output printing every 100 steps\n",
    "      - Standard early stopping based on reward\n",
    "      - RL validation on a held-out set\n",
    "    \"\"\"\n",
    "    JSON_DATASET_PATH = \"MagnusData/deduplicated_dataset.json\"\n",
    "    OFFLINE_CKPT = \"checkpoint_best\"  \n",
    "    RL_SAVE_DIR = \"outputs/Qwen-Theorem-GRPO\"\n",
    "\n",
    "    # Training settings\n",
    "    EVAL_EVERY = 1000   \n",
    "    PATIENCE = 3       \n",
    "    MAX_GLOBAL_STEPS = 10000\n",
    "    CHUNK_SIZE = 100    \n",
    "\n",
    "    # Initialize accelerator for multi-GPU support\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Setup Isabelle Checker\n",
    "    checker = Checker(\n",
    "        working_dir='/home/balaji/Desktop/Theorem_Proving/Isabelle2022/src/HOL/Examples',\n",
    "        isa_path='/home/balaji/Desktop/Theorem_Proving/Isabelle2022',\n",
    "        theory_file='/home/balaji/Desktop/Theorem_Proving/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "        port=9000\n",
    "    )\n",
    "\n",
    "    # Load dataset & split\n",
    "    train_data, val_data = load_train_val_data(JSON_DATASET_PATH, test_size=0.1)\n",
    "    print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "    train_trl_data = build_trl_format(train_data)\n",
    "    val_trl_data   = build_trl_format(val_data)\n",
    "\n",
    "    # Load tokenizer & model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        OFFLINE_CKPT,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"  # Enables multi-GPU training\n",
    "    )\n",
    "\n",
    "    # LoRA configuration (optional)\n",
    "    peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        lora_dropout=0.05,\n",
    "    )\n",
    "\n",
    "    # Reward function combining format + checker\n",
    "    def checker_reward(prompts, completions, answer, **kwargs):\n",
    "        return checker_reward_func(prompts, completions, answer, checker=checker)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=RL_SAVE_DIR,\n",
    "        run_name=\"Qwen-GRPO-theorems\",\n",
    "        learning_rate=1e-6,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.99,\n",
    "        weight_decay=0.1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=10,\n",
    "        bf16=True,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_generations=1,\n",
    "        max_prompt_length=512,\n",
    "        max_completion_length=512,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=9999999,\n",
    "        max_grad_norm=0.1,\n",
    "        report_to=\"wandb\",\n",
    "        log_on_each_node=False,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer with Accelerate\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[format_reward_func, checker_reward],\n",
    "        args=training_args,\n",
    "        train_dataset=train_trl_data,\n",
    "    )\n",
    "\n",
    "    # Prepare trainer for multi-GPU training\n",
    "    trainer.model, trainer.optimizer = accelerator.prepare(\n",
    "        trainer.model, trainer.optimizer\n",
    "    )\n",
    "\n",
    "    best_val_reward = -9999\n",
    "    no_improvement_count = 0\n",
    "    global_step = 0\n",
    "    stop_early = False\n",
    "\n",
    "    # RL Training Loop\n",
    "    while not stop_early and global_step < MAX_GLOBAL_STEPS:\n",
    "        steps_to_run = min(CHUNK_SIZE, MAX_GLOBAL_STEPS - global_step)\n",
    "        trainer.args.max_steps = steps_to_run\n",
    "        trainer.args.num_train_epochs = 1\n",
    "\n",
    "        # ---- Train for 'steps_to_run' steps\n",
    "        trainer.train()\n",
    "        global_step += steps_to_run\n",
    "        print(f\"Finished a block of {steps_to_run} RL steps, global_step={global_step}\")\n",
    "\n",
    "        # ---- Generate a sample output every chunk\n",
    "        if len(val_data) > 0:\n",
    "            sample = random.choice(val_data)\n",
    "            prompt = [\n",
    "                {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "                {\"role\": \"user\", \"content\": sample[\"statement\"] + \"\\n\\n\" + sample[\"state\"]}\n",
    "            ]\n",
    "\n",
    "            completions = trainer.generate(\n",
    "                [prompt], max_new_tokens=50, do_sample=False, top_p=0.9, temperature=0.8\n",
    "            )\n",
    "            gen_text = completions[0][0]['content'] if completions else \"\"\n",
    "\n",
    "            print(\"-\"*50)\n",
    "            print(\"Sample statement+state:\\n\", sample[\"statement\"], sample[\"state\"])\n",
    "            print(\"RL-generated step:\\n\", gen_text)\n",
    "            print(\"Reference step:\\n\", sample[\"step\"])\n",
    "            print(\"-\"*50)\n",
    "\n",
    "        # RL Validation\n",
    "        if global_step % EVAL_EVERY == 0:\n",
    "            val_reward = evaluate_rl(trainer, val_trl_data, num_samples=200)\n",
    "            print(f\"[RL Validation] global_step={global_step}, val_reward={val_reward:.4f}\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_reward > best_val_reward:\n",
    "                best_val_reward = val_reward\n",
    "                no_improvement_count = 0\n",
    "                print(\"  (New best RL reward!)\")\n",
    "                accelerator.unwrap_model(trainer.model).save_pretrained(\"checkpoint_best_rl\")\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                print(f\"  (No improvement. Count={no_improvement_count})\")\n",
    "                if no_improvement_count >= PATIENCE:\n",
    "                    print(\"Early stopping triggered (RL val reward not improving).\")\n",
    "                    stop_early = True\n",
    "\n",
    "    print(f\"RL training complete at global_step={global_step}. Best val_reward={best_val_reward:.4f}\")\n",
    "    accelerator.unwrap_model(trainer.model).save_pretrained(RL_SAVE_DIR)\n",
    "    print(f\"RL final model saved in: {RL_SAVE_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_grpo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 333031, Val size: 37004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1f2eedd9074c8f9b1fff6fb568c447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbalaji-vir1997\u001b[0m (\u001b[33mbalaji-vir1997-stevens-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/balaji/Desktop/Theorem_Proving/wandb/run-20250205_155044-e3uwcdjv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/e3uwcdjv' target=\"_blank\">Qwen-GRPO-theorems</a></strong> to <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/e3uwcdjv' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/e3uwcdjv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/balaji/.local/lib/python3.9/site-packages/torch/amp/autocast_mode.py:44: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `Qwen2ForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return func(*args, **kwargs)\n",
      "/home/balaji/.local/lib/python3.9/site-packages/trl/trainer/grpo_trainer.py:502: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/100 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/TensorCompare.cu:110: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain_grpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 118\u001b[0m, in \u001b[0;36mmain_grpo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m trainer\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_train_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# ---- Train for 'steps_to_run' steps\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m steps_to_run\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished a block of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps_to_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m RL steps, global_step=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2184\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2490\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2483\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2484\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2487\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2488\u001b[0m )\n\u001b[1;32m   2489\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2490\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2493\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2494\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2495\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2496\u001b[0m ):\n\u001b[1;32m   2497\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:3598\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3597\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3598\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3600\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3603\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3604\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/trl/trainer/grpo_trainer.py:422\u001b[0m, in \u001b[0;36mGRPOTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# Regular generation path\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n\u001b[0;32m--> 422\u001b[0m         prompt_completion_ids \u001b[38;5;241m=\u001b[39m \u001b[43munwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprompt_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m prompt_length \u001b[38;5;241m=\u001b[39m prompt_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    427\u001b[0m completion_ids \u001b[38;5;241m=\u001b[39m prompt_completion_ids[:, prompt_length:]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:2224\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2216\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2217\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2218\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2219\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2220\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2221\u001b[0m     )\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2237\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2238\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2244\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:3194\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3191\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[1;32m   3193\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3194\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3197\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3198\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3200\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:2402\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_grpo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
