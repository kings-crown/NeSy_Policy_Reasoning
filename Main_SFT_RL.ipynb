{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  5 17:50:57 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  | 00000000:2D:00.0 Off |                  Off |\n",
      "| 30%   51C    P8              17W / 300W |     13MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               On  | 00000000:41:00.0  On |                  Off |\n",
      "| 36%   59C    P5              61W / 300W |    876MiB / 49140MiB |     44%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     89750      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     89750      G   /usr/lib/xorg/Xorg                          385MiB |\n",
      "|    1   N/A  N/A     90047      G   /usr/bin/gnome-shell                        113MiB |\n",
      "|    1   N/A  N/A     93590      G   ...erProcess --variations-seed-version      322MiB |\n",
      "|    1   N/A  N/A    106812      G   ...irefox/5647/usr/lib/firefox/firefox       36MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMFunction(object):\n",
    "    def __init__(self, engine='gpt-4', max_tokens=512):\n",
    "        self.engine = engine\n",
    "        self.max_tokens = max_tokens\n",
    "        self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    def _call_api(self, prompt, engine, max_tokens, max_retries=10, retry_wait=2):\n",
    "        for i in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=engine,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=1.0\n",
    "                )\n",
    "                return response\n",
    "            except openai.APIError as e:  # New error handling\n",
    "                time.sleep(retry_wait)\n",
    "        return {'choices': [{'message': {'content': ''}}]}\n",
    "\n",
    "    def _parse_message(self, msg):\n",
    "        try:\n",
    "            content = msg.choices[0].message.content\n",
    "        except (IndexError, AttributeError):\n",
    "            content = ''\n",
    "        return content\n",
    "\n",
    "    def f(self, prompt, x):\n",
    "        msg = self._call_api(\n",
    "            prompt=prompt+x,\n",
    "            engine=self.engine,\n",
    "            max_tokens=self.max_tokens\n",
    "        )\n",
    "        evaluation = self._parse_message(msg)\n",
    "        return evaluation\n",
    "\n",
    "\n",
    "class Checker(object):\n",
    "    \"\"\"A modified version of the Draft, Sketch, Prove proof-checking client.\n",
    "    (https://github.com/albertqjiang/draft_sketch_prove/blob/main/autoformalization/checker.py)\n",
    "\n",
    "    This checker supports Isabelle2022 via the new version of PISA\n",
    "    (https://albertqjiang.github.io/Portal-to-ISAbelle/).\n",
    "\n",
    "    It supports checking a miniF2F-style proof via `check`.\n",
    "\n",
    "    Finally, it replaces `sledgehammer` with a call to `normalhammer`.\n",
    "    \"\"\"\n",
    "    def __init__(self, working_dir, isa_path, theory_file, port=9000):\n",
    "        sys.path.append(os.environ['PISA_PATH'])\n",
    "        try:\n",
    "            from pisa_client import initialise_env\n",
    "            self.initialise_env = initialise_env\n",
    "        except:\n",
    "            print(\"Set $PISA_PATH to /yourpath/to/Portal-to-ISAbelle/src/main/python\")\n",
    "\n",
    "        self.working_dir = working_dir\n",
    "        self.isa_path = isa_path\n",
    "        self.theory_file = theory_file\n",
    "        self.port = port\n",
    "\n",
    "    def _initialize(self):\n",
    "        env = self.initialise_env(\n",
    "            self.port,\n",
    "            isa_path=self.isa_path,\n",
    "            theory_file_path=self.theory_file,\n",
    "            working_directory=self.working_dir\n",
    "        )\n",
    "        return env\n",
    "\n",
    "    def _exit(self, env):\n",
    "        try:\n",
    "            env.post('exit')\n",
    "        except:\n",
    "            print(\"env.post('exit') timed out\")\n",
    "            pass\n",
    "        os.system(\"ps aux | grep Isabelle | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "        os.system(\"ps aux | grep poly | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "\n",
    "    def _parse_output(self, obs):\n",
    "        \"\"\"Parse the sledgehammer output, otherwise return an empty string\"\"\"\n",
    "        if '<hammer>' in obs:\n",
    "            output = obs.split('<hammer>')[0]\n",
    "        else:\n",
    "            output = ''\n",
    "        return output\n",
    "\n",
    "    def _run_step(self, step, i, tls_name, env):\n",
    "        obs, reward, done, metadata = env.step_to_top_level_state(\n",
    "            action=step,\n",
    "            tls_name=tls_name,\n",
    "            new_name='default_%d' % i\n",
    "        )\n",
    "        error = None\n",
    "        if 'error:' in obs or 'Step error' in obs or 'Unknown error' in obs:\n",
    "            error = obs\n",
    "        return obs, reward, done, metadata, error\n",
    "\n",
    "    def _run_sledgehammer(self, step, i, tls_name, env):\n",
    "        # First try heuristics\n",
    "        for heuristic in ['by auto', 'by simp', 'by blast', 'by fastforce', 'by force', 'by eval', 'by presburger', 'by sos', 'by arith', 'by linarith', 'by (auto simp: field_simps)']:\n",
    "            step_ = step.replace('normalhammer', heuristic)\n",
    "            obs, reward, done, metadata, error = self._run_step(step_, i, tls_name, env)\n",
    "            if error is None:\n",
    "                obs = '%s <hammer> %s' % (heuristic, obs)\n",
    "                return obs, reward, done, metadata, error\n",
    "        # Try sledgehammer\n",
    "        out = self._run_step(step, i, tls_name, env)\n",
    "        return out\n",
    "\n",
    "    def check(self, statement_and_proof):\n",
    "        # Initialize environment\n",
    "        env = self._initialize()\n",
    "        env.initialise()\n",
    "\n",
    "        # Wrap and parse theorem\n",
    "        theory = Checker.wrap_theorem(statement_and_proof)\n",
    "        steps = Checker.get_parsed(env, theory)\n",
    "\n",
    "        result = self._check(env, steps)\n",
    "        return result\n",
    "\n",
    "    def _check(self, env, steps):\n",
    "        done = False\n",
    "        reason = ''\n",
    "        success = False\n",
    "        step_results = []\n",
    "        tls_name = 'default'\n",
    "        for i, step in enumerate(steps):\n",
    "            try:\n",
    "                time0 = time.time()\n",
    "                if 'normalhammer' in step:\n",
    "                    obs, reward, done, metadata, error = self._run_sledgehammer(step, i, tls_name, env)\n",
    "                else:\n",
    "                    obs, reward, done, metadata, error = self._run_step(step, i, tls_name, env)\n",
    "                step_time = time.time() - time0\n",
    "                step_results.append(dict(index=i, step=step, output=self._parse_output(obs), step_time=step_time))\n",
    "                if error is not None:\n",
    "                    reason = error\n",
    "                    success = False\n",
    "                    done = False\n",
    "                    break\n",
    "            except:\n",
    "                # Timeout - end the proof attempt\n",
    "                success = False\n",
    "                done = False\n",
    "                reason = 'timeout (%d)' % len(step_results)\n",
    "                step_results.append(dict(index=i, step=step, output=''))\n",
    "                break\n",
    "\n",
    "            # Change when successful\n",
    "            tls_name = 'default_%d' % i\n",
    "\n",
    "        if done and reward == 1.0:\n",
    "            success = True\n",
    "\n",
    "        result = {\n",
    "            'success': success,\n",
    "            'reason': reason,\n",
    "            'num_steps': len(steps),\n",
    "            'last_step': len(step_results),\n",
    "            'step_results': step_results,\n",
    "            'theorem_and_proof': self.reconstruct(step_results) if success else ''\n",
    "        }\n",
    "        # Exit environment\n",
    "        self._exit(env)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def reconstruct(step_results):\n",
    "        steps = []\n",
    "        for step_result in step_results[1:]:\n",
    "            if step_result['output'] != '':\n",
    "                steps.append(step_result['output'].strip())\n",
    "            else:\n",
    "                steps.append(step_result['step'].strip())\n",
    "        theorem_and_proof = '\\n'.join(steps)\n",
    "        return theorem_and_proof\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_theorem(theorem):\n",
    "        return 'theory Interactive imports HOL.HOL Complex_Main \"HOL-Library.Code_Target_Numeral\" \"HOL-Library.Sum_of_Squares\" \"Symmetric_Polynomials.Vieta\" \"HOL-Computational_Algebra.Computational_Algebra\" \"HOL-Number_Theory.Number_Theory\" \\n begin\\n%s' % theorem\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parsed(env, theory, tls_name='default'):\n",
    "        # HACK: the parsing doesn't work well with `normalhammer`, so we replace\n",
    "        # all hammer calls with sorry, then replace sorry to normalhammer after parsing.\n",
    "        theory = theory.replace('sledgehammer', 'sorry')\n",
    "        theory = theory.replace('normalhammer', 'sorry')\n",
    "\n",
    "        steps = env.post(f\"<parse text> ${theory}\")\n",
    "        steps = steps.split('<SEP>')\n",
    "        steps = [s for s in steps if s.strip() != '']\n",
    "        # remove weird '$' step and whitespace steps\n",
    "        steps = [s for s in steps if s != '$' and s.strip() != '']\n",
    "        steps = [s.replace('sorry', 'normalhammer') for s in steps]\n",
    "        return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "os.environ['PISA_PATH'] = '/home/siai/Portal-to-ISAbelle/src/main/python'\n",
    "\n",
    "checker = Checker(\n",
    "    working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
    "    isa_path='/home/siai/Isabelle2022',\n",
    "    theory_file='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "    port=9000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_theorem_data(json_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Loads a JSON file containing an array of objects with:\n",
    "      { \"statement\": \"...\", \"state\": \"...\", \"step\": \"...\" }\n",
    "    Returns a list of dicts with these fields.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_train_val_data(json_path: str, test_size=0.1, random_seed=42):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    train_data, val_data = train_test_split(data, test_size=test_size, random_state=random_seed)\n",
    "    return train_data, val_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_loss(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            batch_size = batch[\"input_ids\"].size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_count += batch_size\n",
    "    model.train()\n",
    "    return total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trl_format(dataset: List[Dict[str,str]]) -> List[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Convert each record { \"statement\", \"state\", \"step\" } into TRL's conversation format:\n",
    "      \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "        {\"role\": \"user\", \"content\": statement + \"\\n\\n\" + state}\n",
    "      ],\n",
    "      \"answer\": step\n",
    "\n",
    "    This single-turn approach: the user \"says\" the lemma & subgoal, the model must produce the final \"step\".\n",
    "    \"\"\"\n",
    "    trl_data = []\n",
    "    for rec in dataset:\n",
    "        stm = rec.get(\"statement\",\"\")\n",
    "        stt = rec.get(\"state\",\"\")\n",
    "        sp  = rec.get(\"step\",\"\")\n",
    "        prompt_content = stm + \"\\n\\n\" + stt\n",
    "        sample = {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "                {\"role\": \"user\",   \"content\": prompt_content}\n",
    "            ],\n",
    "            \"answer\": sp  \n",
    "        }\n",
    "        trl_data.append(sample)\n",
    "    return trl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineImitationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For supervised training: input = statement + \"\\n\\n\" + state\n",
    "                            target = step\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_data: List[Dict[str,str]]):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        for rec in raw_data:\n",
    "            inp = rec[\"statement\"] + \"\\n\\n\" + rec[\"state\"]\n",
    "            tgt = rec[\"step\"]\n",
    "            self.samples.append((inp, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_collate_fn(batch, tokenizer, max_length=512):\n",
    "    inp_texts, tgt_texts = zip(*batch)\n",
    "    enc = tokenizer(\n",
    "        list(inp_texts),\n",
    "        text_target=list(tgt_texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"labels\": enc[\"labels\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_offline_imitation_learning(\n",
    "    json_path: str,\n",
    "    model_name: str,\n",
    "    output_dir: str,\n",
    "    epochs: int = 1,\n",
    "    lr: float = 1e-5,\n",
    "    max_length: int = 512,\n",
    "    batch_size: int = 2\n",
    "):\n",
    "\n",
    "    # Load data\n",
    "    raw_data = load_theorem_data(json_path)\n",
    "    dataset_obj = OfflineImitationDataset(raw_data)\n",
    "\n",
    "    # Load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.pad_token = \"<PAD>\"\n",
    "    \n",
    "    model.config.use_cache = False   \n",
    "    model.train()\n",
    "    model.cuda()\n",
    "\n",
    "    # Build DataLoader\n",
    "    loader = DataLoader(\n",
    "        dataset_obj,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, max_length)\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train loop\n",
    "    step_idx = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step_idx += 1\n",
    "            print(f\"Epoch {epoch} Step {step_idx}, loss={loss.item():.4f}\")\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Offline training done, saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_proof_step(model, tokenizer, statement, state, max_length=32):\n",
    "    # Build the input text for the model\n",
    "    input_text = statement + \"\\n\\n\" + state\n",
    "    # Encode\n",
    "    enc_in = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\n",
    "    # Generate\n",
    "    out_ids = model.generate(\n",
    "        enc_in,\n",
    "        max_new_tokens=max_length,\n",
    "        do_sample=False,   # or False for  greedy\n",
    "        top_p=0.9,        \n",
    "        temperature=0.8,  # or 1.0\n",
    "    )\n",
    "    # Decode\n",
    "    out_text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker_reward_func(prompts, completions, answer, checker, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Single-step approach: \n",
    "      prompts[i][-1]['content'] = statement+state\n",
    "      completions[i][0]['content'] = model's final step\n",
    "    We unify them, pass to checker, parse partial or full success => returns float.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for i in range(len(prompts)):\n",
    "        theorem_text = prompts[i][-1]['content']\n",
    "        step_text = completions[i][0]['content']\n",
    "        combined = f\"{theorem_text}\\n\\n{step_text}\"\n",
    "        result = checker.check(combined)\n",
    "        reason = result[\"reason\"]\n",
    "        if result[\"success\"]:\n",
    "            # Full success\n",
    "            rewards.append(2.0)\n",
    "        elif reason == \"partial\":\n",
    "            rewards.append(0.5)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completions, **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    For demonstration, add a small reward if the output contains \"by \" or \"sledgehammer \" or \"normalhammer\" .\n",
    "    \"\"\"\n",
    "    outs = []\n",
    "    for c in completions:\n",
    "        txt = c[0][\"content\"].lower()\n",
    "        if \"by \" or \"sledgehammer \" or \"normalhammer\" in txt:\n",
    "            outs.append(0.1)\n",
    "        else:\n",
    "            outs.append(0.0)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    max_global_steps=100000,\n",
    "    eval_every=100,\n",
    "    patience=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model, validating every 'eval_every' steps.\n",
    "    Stop early if validation fails to improve for 'patience' intervals.\n",
    "\n",
    "    :param model: your HF model\n",
    "    :param train_loader: DataLoader for training\n",
    "    :param val_loader: DataLoader for validation\n",
    "    :param optimizer: optimizer\n",
    "    :param max_global_steps: max steps we allow in total\n",
    "    :param eval_every: how often (in steps) to run validation\n",
    "    :param patience: how many times in a row we allow no improvement\n",
    "    \"\"\"\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    no_improvement_count = 0\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(999999):  # effectively \"infinite\" until we break\n",
    "        for step_idx, batch in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.cuda()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % 50 == 0:\n",
    "                print(f\"[Global Step {global_step}] training loss={loss.item():.4f}\")\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                val_loss = evaluate_validation_loss(model, val_loader)  # your function\n",
    "                print(f\"[Global Step {global_step}] val_loss={val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    print(\"Validation improved!\")\n",
    "                    best_val_loss = val_loss\n",
    "                    no_improvement_count = 0\n",
    "                    # optionally save checkpoint\n",
    "                    model.save_pretrained(\"checkpoint_best\")\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "                    print(f\"No improvement count={no_improvement_count}\")\n",
    "\n",
    "                # ---- Early stopping if patience exceeded ----\n",
    "                if no_improvement_count >= patience:\n",
    "                    print(\"Early stopping: no improvement in val_loss for too long.\")\n",
    "                    return  # or break out of loops\n",
    "\n",
    "            # ---- End if we exceed max steps ----\n",
    "            if global_step >= max_global_steps:\n",
    "                print(\"Reached maximum global steps.\")\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    JSON_DATASET_PATH = \"deduplicated_dataset.json\"\n",
    "    MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "    OFFLINE_SAVE_DIR = \"offline_ckpt\"\n",
    "    OFFLINE_EPOCHS = 2\n",
    "    BATCH_SIZE = 2\n",
    "    LR = 1e-5\n",
    "    MAX_LENGTH = 512\n",
    "    EVAL_EVERY = 1000\n",
    "    PATIENCE = 3\n",
    "\n",
    "    # Initialize Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Load dataset\n",
    "    train_data, val_data = load_train_val_data(JSON_DATASET_PATH, test_size=0.01)\n",
    "    train_dataset = OfflineImitationDataset(train_data)\n",
    "    val_dataset = OfflineImitationDataset(val_data)\n",
    "\n",
    "    # Load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    #model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"  # Enables multi-GPU training\n",
    "    )\n",
    "    # Move model to Accelerator (multi-GPU, FP16 support)\n",
    "    model.config.use_cache = False\n",
    "    model.train()\n",
    "\n",
    "    # Dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, MAX_LENGTH)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: offline_collate_fn(b, tokenizer, MAX_LENGTH)\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # Prepare everything for Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    # Training Variables\n",
    "    global_step = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    no_improvement_count = 0\n",
    "    stop_early = False\n",
    "\n",
    "    for epoch in range(OFFLINE_EPOCHS):\n",
    "        if stop_early:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        for step_idx, batch in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            # Move batch to the correct device\n",
    "            with accelerator.accumulate(model):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "\n",
    "            # Print training loss every 10 steps\n",
    "            if step_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, step {step_idx}, train loss={loss.item():.4f}\")\n",
    "\n",
    "            # Generate a sample output every 500 steps\n",
    "            if step_idx % 500 == 0 and len(val_data) > 0:\n",
    "                sample = random.choice(val_data)\n",
    "                gen_step = generate_proof_step(model, tokenizer, sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"-\"*50)\n",
    "                print(f\"Sample statement+state:\\n {sample['statement']} {sample['state']}\")\n",
    "                print(\"\\n\\nGenerated step:\\n\", gen_step)\n",
    "                print(\"\\n\\nReference step:\\n\", sample[\"step\"])\n",
    "                print(\"-\"*50)\n",
    "\n",
    "            # Validation after every EVAL_EVERY steps\n",
    "            if global_step % EVAL_EVERY == 0:\n",
    "                model.eval()\n",
    "                val_loss = evaluate_validation_loss(model, val_loader)\n",
    "                print(f\"[Global Step {global_step}] Interim val_loss={val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    no_improvement_count = 0\n",
    "                    print(\"  (New best val_loss!)\")\n",
    "                    accelerator.unwrap_model(model).save_pretrained(\"checkpoint_best\")\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "                    print(f\"  (No improvement. Count={no_improvement_count})\")\n",
    "                    if no_improvement_count >= PATIENCE:\n",
    "                        print(\"Early stopping triggered (no val improvement).\")\n",
    "                        stop_early = True\n",
    "                        break\n",
    "                model.train()\n",
    "\n",
    "        # End of epoch => Final validation\n",
    "        if not stop_early:\n",
    "            model.eval()\n",
    "            val_loss = evaluate_validation_loss(model, val_loader)\n",
    "            print(f\"Epoch {epoch}, validation loss={val_loss:.4f}\")\n",
    "\n",
    "            if len(val_data) > 0:\n",
    "                sample = random.choice(val_data)\n",
    "                gen_step = generate_proof_step(model, tokenizer, sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"Sample statement+state:\\n\", sample[\"statement\"], sample[\"state\"])\n",
    "                print(\"Generated step:\\n\", gen_step)\n",
    "                print(\"Reference step:\\n\", sample[\"step\"])\n",
    "            model.train()\n",
    "\n",
    "    # Save final model\n",
    "    accelerator.unwrap_model(model).save_pretrained(OFFLINE_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(OFFLINE_SAVE_DIR)\n",
    "    print(f\"Offline training done. Model saved to: {OFFLINE_SAVE_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b35546400e40108447cb94a0a3c355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 0, train loss=15.9926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma rt_fresh_as_inc_invalidate [simp]:\n",
      "  assumes \"dip\\<in>kD(rt)\"\n",
      "      and \"\\<forall>rip\\<in>dom(dests). rip\\<in>vD(rt) \\<and> the (dests rip) = inc (sqn rt rip)\"\n",
      "    shows \"rt \\<approx>\\<^bsub>dip\\<^esub> invalidate rt dests\" proof (chain)\n",
      "picking this:\n",
      "  dip \\<in> vD rt\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma rt_fresh_as_inc_invalidate [simp]:\n",
      "  assumes \"dip\\<in>kD(rt)\"\n",
      "      and \"\\<forall>rip\\<in>dom(dests). rip\\<in>vD(rt) \\<and> the (dests rip) = inc (sqn rt rip)\"\n",
      "    shows \"rt \\<approx>\\<^bsub>dip\\<^esub> invalidate rt dests\"\n",
      "\n",
      "proof (chain)\n",
      "picking this:\n",
      "  dip \\<in> vD rt\n",
      "  the (dests dip) = inc (sqn rt dip)\n",
      "  dip = dip\n",
      "  the (dests dip) = inc (sq\n",
      "\n",
      "\n",
      "Reference step:\n",
      " using vD_nsqn_sqn by presburger\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 10, train loss=7.0627\n",
      "Epoch 0, step 20, train loss=1.7336\n",
      "Epoch 0, step 30, train loss=0.4751\n",
      "Epoch 0, step 40, train loss=1.5488\n",
      "Epoch 0, step 50, train loss=0.6638\n",
      "Epoch 0, step 60, train loss=0.3875\n",
      "Epoch 0, step 70, train loss=0.9211\n",
      "Epoch 0, step 80, train loss=0.2605\n",
      "Epoch 0, step 90, train loss=0.3437\n",
      "Epoch 0, step 100, train loss=0.2603\n",
      "Epoch 0, step 110, train loss=0.7080\n",
      "Epoch 0, step 120, train loss=0.3322\n",
      "Epoch 0, step 130, train loss=0.3627\n",
      "Epoch 0, step 140, train loss=0.5934\n",
      "Epoch 0, step 150, train loss=0.3521\n",
      "Epoch 0, step 160, train loss=0.3390\n",
      "Epoch 0, step 170, train loss=0.2308\n",
      "Epoch 0, step 180, train loss=0.5674\n",
      "Epoch 0, step 190, train loss=3.4742\n",
      "Epoch 0, step 200, train loss=1.4078\n",
      "Epoch 0, step 210, train loss=0.2096\n",
      "Epoch 0, step 220, train loss=0.1752\n",
      "Epoch 0, step 230, train loss=0.4414\n",
      "Epoch 0, step 240, train loss=0.1410\n",
      "Epoch 0, step 250, train loss=0.2876\n",
      "Epoch 0, step 260, train loss=0.4855\n",
      "Epoch 0, step 270, train loss=0.3974\n",
      "Epoch 0, step 280, train loss=0.3269\n",
      "Epoch 0, step 290, train loss=0.2286\n",
      "Epoch 0, step 300, train loss=0.2248\n",
      "Epoch 0, step 310, train loss=0.2912\n",
      "Epoch 0, step 320, train loss=0.2431\n",
      "Epoch 0, step 330, train loss=0.5869\n",
      "Epoch 0, step 340, train loss=0.2260\n",
      "Epoch 0, step 350, train loss=0.3842\n",
      "Epoch 0, step 360, train loss=0.2193\n",
      "Epoch 0, step 370, train loss=0.3310\n",
      "Epoch 0, step 380, train loss=0.2338\n",
      "Epoch 0, step 390, train loss=0.2361\n",
      "Epoch 0, step 400, train loss=0.2982\n",
      "Epoch 0, step 410, train loss=0.4143\n",
      "Epoch 0, step 420, train loss=0.1916\n",
      "Epoch 0, step 430, train loss=0.2735\n",
      "Epoch 0, step 440, train loss=0.1485\n",
      "Epoch 0, step 450, train loss=0.1573\n",
      "Epoch 0, step 460, train loss=0.1603\n",
      "Epoch 0, step 470, train loss=0.2172\n",
      "Epoch 0, step 480, train loss=0.2528\n",
      "Epoch 0, step 490, train loss=0.2070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 500, train loss=0.3205\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma bi_unique_Union_r:\n",
      "  fixes T :: \"['a, 'b] \\<Rightarrow> bool\" and K\n",
      "  defines K':  \"K' \\<equiv> {(x, y). rel_set T x y} `` K\"\n",
      "  assumes \"bi_unique T\" \n",
      "    and \"\\<Union>K \\<subseteq> Collect (Domainp T)\" \n",
      "    and \"rel_set T (\\<Union>K) xr\" \n",
      "  shows \"\\<Union>K' = xr\" proof (chain)\n",
      "picking this:\n",
      "  x' \\<in> \\<Union> K'\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma bi_unique_Union_r:\n",
      "  fixes T :: \"['a, 'b] \\<Rightarrow> bool\" and K\n",
      "  defines K':  \"K' \\<equiv> {(x, y). rel_set T x y} `` K\"\n",
      "  assumes \"bi_unique T\" \n",
      "    and \"\\<Union>K \\<subseteq> Collect (Domainp T)\" \n",
      "    and \"rel_set T (\\<Union>K) xr\" \n",
      "  shows \"\\<Union>K' = xr\"\n",
      "\n",
      "proof (chain)\n",
      "picking this:\n",
      "  x' \\<in> \\<Union> K'\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by blast\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 510, train loss=0.1203\n",
      "Epoch 0, step 520, train loss=0.4644\n",
      "Epoch 0, step 530, train loss=0.3779\n",
      "Epoch 0, step 540, train loss=0.3666\n",
      "Epoch 0, step 550, train loss=0.1155\n",
      "Epoch 0, step 560, train loss=0.3931\n",
      "Epoch 0, step 570, train loss=0.7130\n",
      "Epoch 0, step 580, train loss=0.2983\n",
      "Epoch 0, step 590, train loss=0.3099\n",
      "Epoch 0, step 600, train loss=0.2134\n",
      "Epoch 0, step 610, train loss=0.6009\n",
      "Epoch 0, step 620, train loss=0.3501\n",
      "Epoch 0, step 630, train loss=0.5893\n",
      "Epoch 0, step 640, train loss=0.1021\n",
      "Epoch 0, step 650, train loss=0.8173\n",
      "Epoch 0, step 660, train loss=0.3833\n",
      "Epoch 0, step 670, train loss=0.1163\n",
      "Epoch 0, step 680, train loss=0.3800\n",
      "Epoch 0, step 690, train loss=0.5960\n",
      "Epoch 0, step 700, train loss=0.2129\n",
      "Epoch 0, step 710, train loss=0.2350\n",
      "Epoch 0, step 720, train loss=0.1539\n",
      "Epoch 0, step 730, train loss=0.2381\n",
      "Epoch 0, step 740, train loss=0.1568\n",
      "Epoch 0, step 750, train loss=0.7083\n",
      "Epoch 0, step 760, train loss=0.2464\n",
      "Epoch 0, step 770, train loss=0.1970\n",
      "Epoch 0, step 780, train loss=0.5269\n",
      "Epoch 0, step 790, train loss=0.3091\n",
      "Epoch 0, step 800, train loss=0.2272\n",
      "Epoch 0, step 810, train loss=0.3719\n",
      "Epoch 0, step 820, train loss=0.2639\n",
      "Epoch 0, step 830, train loss=0.2725\n",
      "Epoch 0, step 840, train loss=0.2775\n",
      "Epoch 0, step 850, train loss=0.1567\n",
      "Epoch 0, step 860, train loss=0.3312\n",
      "Epoch 0, step 870, train loss=0.4692\n",
      "Epoch 0, step 880, train loss=0.1444\n",
      "Epoch 0, step 890, train loss=0.5939\n",
      "Epoch 0, step 900, train loss=0.2132\n",
      "Epoch 0, step 910, train loss=0.1257\n",
      "Epoch 0, step 920, train loss=0.8126\n",
      "Epoch 0, step 930, train loss=0.6607\n",
      "Epoch 0, step 940, train loss=0.2279\n",
      "Epoch 0, step 950, train loss=0.6270\n",
      "Epoch 0, step 960, train loss=0.2037\n",
      "Epoch 0, step 970, train loss=0.3182\n",
      "Epoch 0, step 980, train loss=0.4382\n",
      "Epoch 0, step 990, train loss=0.2632\n",
      "[Global Step 1000] Interim val_loss=0.3184\n",
      "  (New best val_loss!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 1000, train loss=0.7469\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma prime_nat[simp]: \"prime_nat n = prime n\" proof (state)\n",
      "this:\n",
      "  2 \\<le> n\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. \\<not> n < 2 \\<Longrightarrow> prime_nat n = prime n\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma prime_nat[simp]: \"prime_nat n = prime n\"\n",
      "\n",
      "proof (state)\n",
      "this:\n",
      "  2 \\<le> n\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. \\<not> n < 2 \\<Longrightarrow> prime_nat n = prime n\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by fastforce\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 1010, train loss=0.1516\n",
      "Epoch 0, step 1020, train loss=0.1088\n",
      "Epoch 0, step 1030, train loss=0.2033\n",
      "Epoch 0, step 1040, train loss=0.4288\n",
      "Epoch 0, step 1050, train loss=0.3184\n",
      "Epoch 0, step 1060, train loss=0.2315\n",
      "Epoch 0, step 1070, train loss=0.4272\n",
      "Epoch 0, step 1080, train loss=0.1873\n",
      "Epoch 0, step 1090, train loss=0.2178\n",
      "Epoch 0, step 1100, train loss=0.1970\n",
      "Epoch 0, step 1110, train loss=0.3878\n",
      "Epoch 0, step 1120, train loss=0.4860\n",
      "Epoch 0, step 1130, train loss=0.2879\n",
      "Epoch 0, step 1140, train loss=0.3415\n",
      "Epoch 0, step 1150, train loss=0.2423\n",
      "Epoch 0, step 1160, train loss=0.2461\n",
      "Epoch 0, step 1170, train loss=0.1231\n",
      "Epoch 0, step 1180, train loss=0.1874\n",
      "Epoch 0, step 1190, train loss=0.1466\n",
      "Epoch 0, step 1200, train loss=0.3338\n",
      "Epoch 0, step 1210, train loss=0.5121\n",
      "Epoch 0, step 1220, train loss=0.3214\n",
      "Epoch 0, step 1230, train loss=0.3130\n",
      "Epoch 0, step 1240, train loss=0.3476\n",
      "Epoch 0, step 1250, train loss=0.3937\n",
      "Epoch 0, step 1260, train loss=0.3396\n",
      "Epoch 0, step 1270, train loss=0.1446\n",
      "Epoch 0, step 1280, train loss=0.2007\n",
      "Epoch 0, step 1290, train loss=0.3051\n",
      "Epoch 0, step 1300, train loss=0.3115\n",
      "Epoch 0, step 1310, train loss=0.1872\n",
      "Epoch 0, step 1320, train loss=0.3267\n",
      "Epoch 0, step 1330, train loss=0.2201\n",
      "Epoch 0, step 1340, train loss=0.2549\n",
      "Epoch 0, step 1350, train loss=0.4880\n",
      "Epoch 0, step 1360, train loss=0.4470\n",
      "Epoch 0, step 1370, train loss=0.7076\n",
      "Epoch 0, step 1380, train loss=0.3706\n",
      "Epoch 0, step 1390, train loss=0.1256\n",
      "Epoch 0, step 1400, train loss=0.2629\n",
      "Epoch 0, step 1410, train loss=0.5638\n",
      "Epoch 0, step 1420, train loss=0.3382\n",
      "Epoch 0, step 1430, train loss=0.2899\n",
      "Epoch 0, step 1440, train loss=0.2969\n",
      "Epoch 0, step 1450, train loss=0.1101\n",
      "Epoch 0, step 1460, train loss=0.0528\n",
      "Epoch 0, step 1470, train loss=0.0940\n",
      "Epoch 0, step 1480, train loss=0.2690\n",
      "Epoch 0, step 1490, train loss=0.2972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 1500, train loss=0.3249\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma HomFtorContraMapsTo:\n",
      "  assumes \"LSCategory C\" and \"X \\<in> obj\\<^bsub>C\\<^esub>\" and \"f \\<in> mor\\<^bsub>C\\<^esub>\" \n",
      "  shows \"HomC\\<^bsub>C\\<^esub>[f,X] maps\\<^bsub>SET\\<^esub> Hom\\<^bsub>C \\<^esub>(cod\\<^bsub>C\\<^esub> f) X  to Hom\\<^bsub>C \\<^esub>(dom\\<^bsub>C\\<^esub> f) X\" proof (state)\n",
      "this:\n",
      "  Hom\\<^bsub>Op C\\<^esub> X cod\\<^bsub>Op C\\<^esub> f =\n",
      "  Hom\\<^bsub>C\\<^esub> cod\\<^bsub>Op C\\<^esub> f X\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. Hom\\<^bsub>Op C\\<^esub> X cod\\<^bsub>Op C\\<^esub> f =\n",
      "    Hom\\<^bsub>C\\<^esub> dom\\<^bsub>C\\<^esub> f X\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma HomFtorContraMapsTo:\n",
      "  assumes \"LSCategory C\" and \"X \\<in> obj\\<^bsub>C\\<^esub>\" and \"f \\<in> mor\\<^bsub>C\\<^esub>\" \n",
      "  shows \"HomC\\<^bsub>C\\<^esub>[f,X] maps\\<^bsub>SET\\<^esub> Hom\\<^bsub>C \\<^esub>(cod\\<^bsub>C\\<^esub> f) X  to Hom\\<^bsub>C \\<^esub>(dom\\<^bsub>C\\<^esub> f) X\"\n",
      "\n",
      "proof (state)\n",
      "this:\n",
      "  Hom\\<^bsub>Op C\\<^esub> X cod\\<^bsub>Op C\\<^esub> f =\n",
      "  Hom\\<^bsub>C\\<^esub> cod\\<^bsub>Op C\\<^esub> f X\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. Hom\\<^bsub>Op C\\<^esub> X cod\\<^bsub>Op C\\<^esub> f =\n",
      "    Hom\\<^bsub>C\\<^esub> dom\\<^bsub>C\\<^esub> f X\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by (metis MapsToOp MapsTo_def assms(3))\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 1510, train loss=0.3299\n",
      "Epoch 0, step 1520, train loss=0.1791\n",
      "Epoch 0, step 1530, train loss=0.1555\n",
      "Epoch 0, step 1540, train loss=0.3802\n",
      "Epoch 0, step 1550, train loss=0.2947\n",
      "Epoch 0, step 1560, train loss=0.2064\n",
      "Epoch 0, step 1570, train loss=0.2386\n",
      "Epoch 0, step 1580, train loss=0.2076\n",
      "Epoch 0, step 1590, train loss=0.0715\n",
      "Epoch 0, step 1600, train loss=0.0986\n",
      "Epoch 0, step 1610, train loss=0.4290\n",
      "Epoch 0, step 1620, train loss=0.5085\n",
      "Epoch 0, step 1630, train loss=0.4936\n",
      "Epoch 0, step 1640, train loss=0.1590\n",
      "Epoch 0, step 1650, train loss=0.5334\n",
      "Epoch 0, step 1660, train loss=0.4585\n",
      "Epoch 0, step 1670, train loss=0.1168\n",
      "Epoch 0, step 1680, train loss=0.1364\n",
      "Epoch 0, step 1690, train loss=0.2039\n",
      "Epoch 0, step 1700, train loss=0.1875\n",
      "Epoch 0, step 1710, train loss=0.2006\n",
      "Epoch 0, step 1720, train loss=0.1204\n",
      "Epoch 0, step 1730, train loss=0.1728\n",
      "Epoch 0, step 1740, train loss=0.0861\n",
      "Epoch 0, step 1750, train loss=0.6422\n",
      "Epoch 0, step 1760, train loss=0.0976\n",
      "Epoch 0, step 1770, train loss=0.1845\n",
      "Epoch 0, step 1780, train loss=0.1641\n",
      "Epoch 0, step 1790, train loss=0.9391\n",
      "Epoch 0, step 1800, train loss=0.5108\n",
      "Epoch 0, step 1810, train loss=0.3150\n",
      "Epoch 0, step 1820, train loss=0.4023\n",
      "Epoch 0, step 1830, train loss=0.1879\n",
      "Epoch 0, step 1840, train loss=0.9090\n",
      "Epoch 0, step 1850, train loss=0.2217\n",
      "Epoch 0, step 1860, train loss=0.2660\n",
      "Epoch 0, step 1870, train loss=0.1755\n",
      "Epoch 0, step 1880, train loss=0.2320\n",
      "Epoch 0, step 1890, train loss=0.2121\n",
      "Epoch 0, step 1900, train loss=0.3494\n",
      "Epoch 0, step 1910, train loss=0.1897\n",
      "Epoch 0, step 1920, train loss=0.1420\n",
      "Epoch 0, step 1930, train loss=0.3439\n",
      "Epoch 0, step 1940, train loss=0.2377\n",
      "Epoch 0, step 1950, train loss=0.1089\n",
      "Epoch 0, step 1960, train loss=0.1729\n",
      "Epoch 0, step 1970, train loss=0.1903\n",
      "Epoch 0, step 1980, train loss=0.1574\n",
      "Epoch 0, step 1990, train loss=0.7863\n",
      "[Global Step 2000] Interim val_loss=0.2766\n",
      "  (New best val_loss!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 2000, train loss=0.2886\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " theorem p21: \"\\<exists>x. p \\<longrightarrow> F(x) \\<Longrightarrow> \\<exists>x. F(x) \\<longrightarrow> p \\<Longrightarrow> \\<exists>x. p \\<longleftrightarrow> F(x)\" \n",
      "\n",
      "\n",
      "Generated step:\n",
      " theorem p21: \"\\<exists>x. p \\<longrightarrow> F(x) \\<Longrightarrow> \\<exists>x. F(x) \\<longrightarrow> p \\<Longrightarrow> \\<exists>x. p \\<longleftrightarrow> F(x)\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by blast\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 2010, train loss=0.1192\n",
      "Epoch 0, step 2020, train loss=0.3069\n",
      "Epoch 0, step 2030, train loss=0.2970\n",
      "Epoch 0, step 2040, train loss=0.1232\n",
      "Epoch 0, step 2050, train loss=0.3321\n",
      "Epoch 0, step 2060, train loss=0.3725\n",
      "Epoch 0, step 2070, train loss=0.2881\n",
      "Epoch 0, step 2080, train loss=0.1252\n",
      "Epoch 0, step 2090, train loss=0.1086\n",
      "Epoch 0, step 2100, train loss=0.2084\n",
      "Epoch 0, step 2110, train loss=0.1926\n",
      "Epoch 0, step 2120, train loss=0.2046\n",
      "Epoch 0, step 2130, train loss=0.3402\n",
      "Epoch 0, step 2140, train loss=0.1644\n",
      "Epoch 0, step 2150, train loss=0.1421\n",
      "Epoch 0, step 2160, train loss=0.5165\n",
      "Epoch 0, step 2170, train loss=0.1106\n",
      "Epoch 0, step 2180, train loss=0.3447\n",
      "Epoch 0, step 2190, train loss=0.3825\n",
      "Epoch 0, step 2200, train loss=0.1446\n",
      "Epoch 0, step 2210, train loss=0.2943\n",
      "Epoch 0, step 2220, train loss=0.1947\n",
      "Epoch 0, step 2230, train loss=0.0719\n",
      "Epoch 0, step 2240, train loss=0.1204\n",
      "Epoch 0, step 2250, train loss=0.2434\n",
      "Epoch 0, step 2260, train loss=0.1670\n",
      "Epoch 0, step 2270, train loss=0.1501\n",
      "Epoch 0, step 2280, train loss=0.0694\n",
      "Epoch 0, step 2290, train loss=0.1392\n",
      "Epoch 0, step 2300, train loss=0.2061\n",
      "Epoch 0, step 2310, train loss=0.1103\n",
      "Epoch 0, step 2320, train loss=0.3448\n",
      "Epoch 0, step 2330, train loss=0.1620\n",
      "Epoch 0, step 2340, train loss=0.2071\n",
      "Epoch 0, step 2350, train loss=0.0759\n",
      "Epoch 0, step 2360, train loss=0.2128\n",
      "Epoch 0, step 2370, train loss=0.1499\n",
      "Epoch 0, step 2380, train loss=2.5986\n",
      "Epoch 0, step 2390, train loss=0.2994\n",
      "Epoch 0, step 2400, train loss=0.1575\n",
      "Epoch 0, step 2410, train loss=0.2409\n",
      "Epoch 0, step 2420, train loss=0.1924\n",
      "Epoch 0, step 2430, train loss=0.2495\n",
      "Epoch 0, step 2440, train loss=0.3018\n",
      "Epoch 0, step 2450, train loss=2.5151\n",
      "Epoch 0, step 2460, train loss=0.1809\n",
      "Epoch 0, step 2470, train loss=0.5249\n",
      "Epoch 0, step 2480, train loss=0.1721\n",
      "Epoch 0, step 2490, train loss=0.2447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 2500, train loss=0.2625\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma singleton_dotP [simp]: \"dim_vec x = 1 \\<Longrightarrow> [v]\\<^sub>v \\<bullet> x = v * x$0\" \n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma singleton_dotP [simp]: \"dim_vec x = 1 \\<Longrightarrow> [v]\\<^sub>v \\<bullet> x = v * x$0\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by (metis not_gr_zero single_nz_val_dotP single_nz_zero_singleton zero_neq_one)\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 2510, train loss=0.1210\n",
      "Epoch 0, step 2520, train loss=0.3949\n",
      "Epoch 0, step 2530, train loss=0.0916\n",
      "Epoch 0, step 2540, train loss=0.8240\n",
      "Epoch 0, step 2550, train loss=0.3825\n",
      "Epoch 0, step 2560, train loss=0.1705\n",
      "Epoch 0, step 2570, train loss=0.0833\n",
      "Epoch 0, step 2580, train loss=0.0870\n",
      "Epoch 0, step 2590, train loss=0.2330\n",
      "Epoch 0, step 2600, train loss=0.1734\n",
      "Epoch 0, step 2610, train loss=0.4002\n",
      "Epoch 0, step 2620, train loss=0.0898\n",
      "Epoch 0, step 2630, train loss=0.1563\n",
      "Epoch 0, step 2640, train loss=0.1091\n",
      "Epoch 0, step 2650, train loss=0.8491\n",
      "Epoch 0, step 2660, train loss=0.1368\n",
      "Epoch 0, step 2670, train loss=0.1619\n",
      "Epoch 0, step 2680, train loss=0.3383\n",
      "Epoch 0, step 2690, train loss=0.2697\n",
      "Epoch 0, step 2700, train loss=0.1737\n",
      "Epoch 0, step 2710, train loss=0.0543\n",
      "Epoch 0, step 2720, train loss=0.1191\n",
      "Epoch 0, step 2730, train loss=0.6004\n",
      "Epoch 0, step 2740, train loss=0.2919\n",
      "Epoch 0, step 2750, train loss=0.0985\n",
      "Epoch 0, step 2760, train loss=0.7343\n",
      "Epoch 0, step 2770, train loss=0.3246\n",
      "Epoch 0, step 2780, train loss=1.0525\n",
      "Epoch 0, step 2790, train loss=0.1438\n",
      "Epoch 0, step 2800, train loss=0.1844\n",
      "Epoch 0, step 2810, train loss=0.2101\n",
      "Epoch 0, step 2820, train loss=0.2819\n",
      "Epoch 0, step 2830, train loss=0.2517\n",
      "Epoch 0, step 2840, train loss=4.1776\n",
      "Epoch 0, step 2850, train loss=0.1602\n",
      "Epoch 0, step 2860, train loss=0.1221\n",
      "Epoch 0, step 2870, train loss=0.3724\n",
      "Epoch 0, step 2880, train loss=0.2646\n",
      "Epoch 0, step 2890, train loss=0.1850\n",
      "Epoch 0, step 2900, train loss=0.1913\n",
      "Epoch 0, step 2910, train loss=0.2021\n",
      "Epoch 0, step 2920, train loss=0.2457\n",
      "Epoch 0, step 2930, train loss=0.0859\n",
      "Epoch 0, step 2940, train loss=0.1271\n",
      "Epoch 0, step 2950, train loss=0.3271\n",
      "Epoch 0, step 2960, train loss=0.0769\n",
      "Epoch 0, step 2970, train loss=0.7323\n",
      "Epoch 0, step 2980, train loss=0.3849\n",
      "Epoch 0, step 2990, train loss=0.1427\n",
      "[Global Step 3000] Interim val_loss=0.2613\n",
      "  (New best val_loss!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 3000, train loss=0.1945\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma \\<R>_typed_step_no_await:\n",
      "\"\\<lbrakk> \\<turnstile> \\<Gamma>,\\<S>,P { c\\<^sub>1 } \\<Gamma>',\\<S>',P' ;\n",
      "  tyenv_wellformed mds \\<Gamma> \\<S> P; mem\\<^sub>1 =\\<^bsub>\\<Gamma>\\<^esub> mem\\<^sub>2; pred P mem\\<^sub>1;\n",
      "        pred P mem\\<^sub>2; tyenv_sec mds \\<Gamma> mem\\<^sub>1;\n",
      "     \\<langle>c\\<^sub>1, mds, mem\\<^sub>1\\<rangle> \\<leadsto> \\<langle>c\\<^sub>1', mds', mem\\<^sub>1'\\<rangle>; no_await c\\<^sub>1 \\<rbrakk> \\<Longrightarrow>\n",
      "   (\\<exists> c\\<^sub>2' mem\\<^sub>2'. \\<langle>c\\<^sub>1, mds, mem\\<^sub>2\\<rangle> \\<leadsto> \\<langle>c\\<^sub>2', mds', mem\\<^sub>2'\\<rangle> \\<and>\n",
      "                 \\<langle>c\\<^sub>1', mds', mem\\<^sub>1'\\<rangle> \\<R>\\<^sup>u\\<^bsub>\\<Gamma>',\\<S>',P'\\<^esub> \\<langle>c\\<^sub>2', mds', mem\\<^sub>2'\\<rangle>)\" proof (state)\n",
      "this:\n",
      "  \\<forall>x\\<in>\\<C>. mem\\<^sub>1 x = mem\\<^sub>1' x\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. v \\<noteq> x \\<Longrightarrow>\n",
      "    type_max (the ((\\<Gamma>(x \\<mapsto> t)) v)) mem\\<^sub>1'\n",
      "    \\<le> dma mem\\<^sub>1' v\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma \\<R>_typed_step_no_await:\n",
      "\"\\<lbrakk> \\<turnstile> \\<Gamma>,\\<S>,P { c\\<^sub>1 } \\<Gamma>',\\<S>',P' ;\n",
      "  tyenv_wellformed mds \\<Gamma> \\<S> P; mem\\<^sub>1 =\\<^bsub>\\<Gamma>\\<^esub> mem\\<^sub>2; pred P mem\\<^sub>1;\n",
      "        pred P mem\\<^sub>2; tyenv_sec mds \\<Gamma> mem\\<^sub>1;\n",
      "     \\<langle>c\\<^sub>1, mds, mem\\<^sub>1\\<rangle> \\<leadsto> \\<langle>c\\<^sub>1', mds', mem\\<^sub>1'\\<rangle>; no_await c\\<^sub>1 \\<rbrakk> \\<Longrightarrow>\n",
      "   (\\<exists> c\\<^sub>2' mem\\<^sub>2'. \\<langle>c\\<^sub>1, mds, mem\\<^sub>2\\<rangle> \\<leadsto> \\<langle>c\\<^sub>2', mds', mem\\<^sub>2'\\<rangle> \\<and>\n",
      "                 \\<langle>c\\<^sub>1', mds', mem\\<^sub>1'\\<rangle> \\<R>\\<^sup>u\\<^bsub>\\<Gamma>',\\<S>',P'\\<^esub> \\<langle>c\\<^sub>2', mds', mem\\<^sub>2'\\<rangle>)\"\n",
      "\n",
      "proof (state)\n",
      "this:\n",
      "  \\<forall>x\\<in>\\<C>. mem\\<^sub>1 x = mem\\<^sub>1' x\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. v \\<noteq> x \\<Longrightarrow>\n",
      "    type_max (the ((\\<Gamma>(x \\<mapsto> t)) v)) mem\\<^sub>1'\n",
      "    \\<le> dma mem\\<^sub>1' v\n",
      "\n",
      "\n",
      "Reference step:\n",
      " using dma_eq by presburger\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 3010, train loss=0.2994\n",
      "Epoch 0, step 3020, train loss=0.2540\n",
      "Epoch 0, step 3030, train loss=0.2242\n",
      "Epoch 0, step 3040, train loss=0.1958\n",
      "Epoch 0, step 3050, train loss=0.2619\n",
      "Epoch 0, step 3060, train loss=0.9319\n",
      "Epoch 0, step 3070, train loss=0.2235\n",
      "Epoch 0, step 3080, train loss=0.2563\n",
      "Epoch 0, step 3090, train loss=0.2010\n",
      "Epoch 0, step 3100, train loss=0.4342\n",
      "Epoch 0, step 3110, train loss=0.1336\n",
      "Epoch 0, step 3120, train loss=0.1509\n",
      "Epoch 0, step 3130, train loss=0.2120\n",
      "Epoch 0, step 3140, train loss=0.2386\n",
      "Epoch 0, step 3150, train loss=0.5447\n",
      "Epoch 0, step 3160, train loss=0.2633\n",
      "Epoch 0, step 3170, train loss=0.4054\n",
      "Epoch 0, step 3180, train loss=0.0790\n",
      "Epoch 0, step 3190, train loss=0.5441\n",
      "Epoch 0, step 3200, train loss=0.1005\n",
      "Epoch 0, step 3210, train loss=0.2680\n",
      "Epoch 0, step 3220, train loss=0.3334\n",
      "Epoch 0, step 3230, train loss=0.1522\n",
      "Epoch 0, step 3240, train loss=0.3432\n",
      "Epoch 0, step 3250, train loss=0.1439\n",
      "Epoch 0, step 3260, train loss=0.3233\n",
      "Epoch 0, step 3270, train loss=0.1814\n",
      "Epoch 0, step 3280, train loss=0.5789\n",
      "Epoch 0, step 3290, train loss=0.3484\n",
      "Epoch 0, step 3300, train loss=0.1996\n",
      "Epoch 0, step 3310, train loss=0.3472\n",
      "Epoch 0, step 3320, train loss=0.1594\n",
      "Epoch 0, step 3330, train loss=0.1424\n",
      "Epoch 0, step 3340, train loss=0.1831\n",
      "Epoch 0, step 3350, train loss=0.2980\n",
      "Epoch 0, step 3360, train loss=0.2727\n",
      "Epoch 0, step 3370, train loss=0.0544\n",
      "Epoch 0, step 3380, train loss=0.0990\n",
      "Epoch 0, step 3390, train loss=0.8691\n",
      "Epoch 0, step 3400, train loss=0.0676\n",
      "Epoch 0, step 3410, train loss=0.2533\n",
      "Epoch 0, step 3420, train loss=0.3049\n",
      "Epoch 0, step 3430, train loss=0.1421\n",
      "Epoch 0, step 3440, train loss=0.0543\n",
      "Epoch 0, step 3450, train loss=0.1343\n",
      "Epoch 0, step 3460, train loss=0.1022\n",
      "Epoch 0, step 3470, train loss=0.0513\n",
      "Epoch 0, step 3480, train loss=0.1755\n",
      "Epoch 0, step 3490, train loss=0.2491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 3500, train loss=0.1452\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma filter_compliant_stateful_ACS_maximal_allsubsets:\n",
      "      assumes a1: \"valid_reqs (get_ACS M)\" and a2: \"wf_graph \\<lparr> nodes = V, edges = E \\<rparr>\"\n",
      "      and a3: \"(set edgesList) = E\"\n",
      "      and a4: \"stateful = set (filter_compliant_stateful_ACS \\<lparr> nodes = V, edges = E \\<rparr> M edgesList)\"\n",
      "      and a5: \"X \\<subseteq> E - (stateful \\<union> backflows E)\" and a6: \"X \\<noteq> {}\"\n",
      "      shows \"\n",
      "      \\<not> \\<Union>(get_offending_flows (get_ACS M) (stateful_policy_to_network_graph \\<lparr> hosts = V, flows_fix = E, flows_state = stateful \\<union> X \\<rparr>))\n",
      "                \\<subseteq> backflows (filternew_flows_state \\<lparr> hosts = V, flows_fix = E, flows_state = stateful \\<union> X \\<rparr>)\" proof (state)\n",
      "this:\n",
      "  E \\<union> stateful \\<union> {x} = E\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. \\<Union>\n",
      "     (get_offending_flows (get_ACS M)\n",
      "       (stateful_policy_to_network_graph\n",
      "         \\<lparr>hosts = V, flows_fix = E,\n",
      "            flows_state = stateful \\<union> X\\<rparr>))\n",
      "    \\<subseteq> backflows\n",
      "                 (filternew_flows_state\n",
      "                   \\<lparr>hosts = V, flows_fix = E,\n",
      "                      flows_state =\n",
      "                        stateful \\<union> X\\<rparr>) \\<Longrightarrow>\n",
      "    False\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma filter_compliant_stateful_ACS_maximal_allsubsets:\n",
      "      assumes a1: \"valid_reqs (get_ACS M)\" and a2: \"wf_graph \\<lparr> nodes = V, edges = E \\<rparr>\"\n",
      "      and a3: \"(set edgesList) = E\"\n",
      "      and a4: \"stateful = set (filter_compliant_stateful_ACS \\<lparr> nodes = V, edges = E \\<rparr> M edgesList)\"\n",
      "      and a5: \"X \\<subseteq> E - (stateful \\<union> backflows E)\" and a6: \"X \\<noteq> {}\"\n",
      "      shows \"\n",
      "      \\<not> \\<Union>(get_offending_flows (get_ACS M) (stateful_policy_to_network_graph \\<lparr> hosts = V, flows_fix = E, flows_state = stateful \\<union> X \\<rparr>))\n",
      "                \\<subseteq> backflows (filternew_flows_state \\<lparr> hosts = V, flows_fix = E, flows_state = stateful \\<union> X \\<rparr>)\"\n",
      "\n",
      "proof (state)\n",
      "this:\n",
      "  E \\<union> stateful \\<union> {x} = E\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. \\<Union>\n",
      "     (get_offending_flows (get_ACS M)\n",
      "       (stateful_policy_to_network_graph\n",
      "         \\<lparr>hosts = V, flows_fix = E,\n",
      "            flows_state = stateful \\<union> X\\<rparr>))\n",
      "    \\<subseteq> backflows\n",
      "                 (filternew_flows_state\n",
      "                   \\<lparr>hosts = V, flows_fix = E,\n",
      "                      flows_state =\n",
      "                        stateful \\<union> X\\<rparr>) \\<Longrightarrow>\n",
      "    False\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by meson\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 3510, train loss=0.1574\n",
      "Epoch 0, step 3520, train loss=0.0717\n",
      "Epoch 0, step 3530, train loss=0.1129\n",
      "Epoch 0, step 3540, train loss=0.2054\n",
      "Epoch 0, step 3550, train loss=0.1689\n",
      "Epoch 0, step 3560, train loss=0.0814\n",
      "Epoch 0, step 3570, train loss=0.2006\n",
      "Epoch 0, step 3580, train loss=0.0607\n",
      "Epoch 0, step 3590, train loss=0.1938\n",
      "Epoch 0, step 3600, train loss=0.1166\n",
      "Epoch 0, step 3610, train loss=0.1625\n",
      "Epoch 0, step 3620, train loss=0.1442\n",
      "Epoch 0, step 3630, train loss=0.2020\n",
      "Epoch 0, step 3640, train loss=0.2104\n",
      "Epoch 0, step 3650, train loss=0.0704\n",
      "Epoch 0, step 3660, train loss=0.2327\n",
      "Epoch 0, step 3670, train loss=0.2467\n",
      "Epoch 0, step 3680, train loss=0.2425\n",
      "Epoch 0, step 3690, train loss=0.1961\n",
      "Epoch 0, step 3700, train loss=0.4397\n",
      "Epoch 0, step 3710, train loss=0.2778\n",
      "Epoch 0, step 3720, train loss=0.1359\n",
      "Epoch 0, step 3730, train loss=0.2390\n",
      "Epoch 0, step 3740, train loss=0.2998\n",
      "Epoch 0, step 3750, train loss=0.3832\n",
      "Epoch 0, step 3760, train loss=0.1392\n",
      "Epoch 0, step 3770, train loss=0.0480\n",
      "Epoch 0, step 3780, train loss=0.1730\n",
      "Epoch 0, step 3790, train loss=0.2899\n",
      "Epoch 0, step 3800, train loss=0.2253\n",
      "Epoch 0, step 3810, train loss=0.2142\n",
      "Epoch 0, step 3820, train loss=0.1152\n",
      "Epoch 0, step 3830, train loss=0.3181\n",
      "Epoch 0, step 3840, train loss=0.2908\n",
      "Epoch 0, step 3850, train loss=0.0772\n",
      "Epoch 0, step 3860, train loss=0.2001\n",
      "Epoch 0, step 3870, train loss=0.1521\n",
      "Epoch 0, step 3880, train loss=0.0791\n",
      "Epoch 0, step 3890, train loss=0.1423\n",
      "Epoch 0, step 3900, train loss=0.2305\n",
      "Epoch 0, step 3910, train loss=0.1131\n",
      "Epoch 0, step 3920, train loss=0.1081\n",
      "Epoch 0, step 3930, train loss=0.1548\n",
      "Epoch 0, step 3940, train loss=0.1694\n",
      "Epoch 0, step 3950, train loss=0.2099\n",
      "Epoch 0, step 3960, train loss=0.1098\n",
      "Epoch 0, step 3970, train loss=0.1452\n",
      "Epoch 0, step 3980, train loss=0.1898\n",
      "Epoch 0, step 3990, train loss=0.2262\n",
      "[Global Step 4000] Interim val_loss=0.2543\n",
      "  (New best val_loss!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 4000, train loss=0.0989\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma step_invariantI [intro]:\n",
      "  assumes *: \"\\<And>s a s'. \\<lbrakk> s\\<in>reachable A I; (s, a, s')\\<in>trans A; I a \\<rbrakk> \\<Longrightarrow> P (s, a, s')\"\n",
      "    shows \"A \\<TTurnstile>\\<^sub>A (I \\<rightarrow>) P\" \n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma step_invariantI [intro]:\n",
      "  assumes *: \"\\<And>s a s'. \\<lbrakk> s\\<in>reachable A I; (s, a, s')\\<in>trans A; I a \\<rbrakk> \\<Longrightarrow> P (s, a, s')\"\n",
      "    shows \"A \\<TTurnstile>\\<^sub>A (I \\<rightarrow>) P\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by (simp add: assms step_invariant_def)\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 4010, train loss=0.2207\n",
      "Epoch 0, step 4020, train loss=0.1276\n",
      "Epoch 0, step 4030, train loss=0.2231\n",
      "Epoch 0, step 4040, train loss=0.1537\n",
      "Epoch 0, step 4050, train loss=0.2766\n",
      "Epoch 0, step 4060, train loss=0.1573\n",
      "Epoch 0, step 4070, train loss=0.1848\n",
      "Epoch 0, step 4080, train loss=0.0766\n",
      "Epoch 0, step 4090, train loss=0.1082\n",
      "Epoch 0, step 4100, train loss=0.3715\n",
      "Epoch 0, step 4110, train loss=0.1775\n",
      "Epoch 0, step 4120, train loss=0.2323\n",
      "Epoch 0, step 4130, train loss=0.2952\n",
      "Epoch 0, step 4140, train loss=0.1411\n",
      "Epoch 0, step 4150, train loss=0.1106\n",
      "Epoch 0, step 4160, train loss=0.1900\n",
      "Epoch 0, step 4170, train loss=0.4689\n",
      "Epoch 0, step 4180, train loss=0.6844\n",
      "Epoch 0, step 4190, train loss=0.2958\n",
      "Epoch 0, step 4200, train loss=0.1281\n",
      "Epoch 0, step 4210, train loss=0.1257\n",
      "Epoch 0, step 4220, train loss=0.5072\n",
      "Epoch 0, step 4230, train loss=0.2497\n",
      "Epoch 0, step 4240, train loss=0.0839\n",
      "Epoch 0, step 4250, train loss=0.3213\n",
      "Epoch 0, step 4260, train loss=0.3998\n",
      "Epoch 0, step 4270, train loss=0.1231\n",
      "Epoch 0, step 4280, train loss=0.2522\n",
      "Epoch 0, step 4290, train loss=0.2911\n",
      "Epoch 0, step 4300, train loss=0.2988\n",
      "Epoch 0, step 4310, train loss=0.0728\n",
      "Epoch 0, step 4320, train loss=0.1539\n",
      "Epoch 0, step 4330, train loss=0.1755\n",
      "Epoch 0, step 4340, train loss=0.2336\n",
      "Epoch 0, step 4350, train loss=0.2888\n",
      "Epoch 0, step 4360, train loss=0.1499\n",
      "Epoch 0, step 4370, train loss=0.1427\n",
      "Epoch 0, step 4380, train loss=0.1764\n",
      "Epoch 0, step 4390, train loss=0.1071\n",
      "Epoch 0, step 4400, train loss=0.2920\n",
      "Epoch 0, step 4410, train loss=0.3651\n",
      "Epoch 0, step 4420, train loss=0.2515\n",
      "Epoch 0, step 4430, train loss=0.2077\n",
      "Epoch 0, step 4440, train loss=0.1869\n",
      "Epoch 0, step 4450, train loss=0.3526\n",
      "Epoch 0, step 4460, train loss=0.1024\n",
      "Epoch 0, step 4470, train loss=0.1576\n",
      "Epoch 0, step 4480, train loss=0.0823\n",
      "Epoch 0, step 4490, train loss=0.1564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 4500, train loss=0.4476\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma has_integral_integral_real:\n",
      "  fixes f::\"'a::euclidean_space \\<Rightarrow> real\"\n",
      "  assumes f: \"integrable lborel f\"\n",
      "  shows \"(f has_integral (integral\\<^sup>L lborel f)) UNIV\" proof (chain)\n",
      "picking this:\n",
      "  ((\\<lambda>x. max 0 (f x) - max 0 (- f x)) has_integral r - q) UNIV\n",
      "  (\\<lambda>x. max 0 (f x) - max 0 (- f x)) = f\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma has_integral_integral_real:\n",
      "  fixes f::\"'a::euclidean_space \\<Rightarrow> real\"\n",
      "  assumes f: \"integrable lborel f\"\n",
      "  shows \"(f has_integral (integral\\<^sup>L lborel f)) UNIV\"\n",
      "\n",
      "proof (chain)\n",
      "picking this:\n",
      "  ((\\<lambda>x. max 0 (f x) - max 0 (- f x)) has_integral r - q) UNIV\n",
      "  (\\<lambda>x. max 0 (f x) - max 0 (- f x)) = f\n",
      "\n",
      "\n",
      "Reference step:\n",
      " using eq by auto\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 4510, train loss=0.1535\n",
      "Epoch 0, step 4520, train loss=0.1802\n",
      "Epoch 0, step 4530, train loss=0.1411\n",
      "Epoch 0, step 4540, train loss=0.1441\n",
      "Epoch 0, step 4550, train loss=0.1633\n",
      "Epoch 0, step 4560, train loss=0.1598\n",
      "Epoch 0, step 4570, train loss=0.1104\n",
      "Epoch 0, step 4580, train loss=2.7644\n",
      "Epoch 0, step 4590, train loss=0.1283\n",
      "Epoch 0, step 4600, train loss=0.2312\n",
      "Epoch 0, step 4610, train loss=0.2503\n",
      "Epoch 0, step 4620, train loss=0.1605\n",
      "Epoch 0, step 4630, train loss=0.4701\n",
      "Epoch 0, step 4640, train loss=0.5090\n",
      "Epoch 0, step 4650, train loss=0.1293\n",
      "Epoch 0, step 4660, train loss=0.3277\n",
      "Epoch 0, step 4670, train loss=0.3625\n",
      "Epoch 0, step 4680, train loss=0.2166\n",
      "Epoch 0, step 4690, train loss=0.2230\n",
      "Epoch 0, step 4700, train loss=0.1255\n",
      "Epoch 0, step 4710, train loss=0.0489\n",
      "Epoch 0, step 4720, train loss=0.5295\n",
      "Epoch 0, step 4730, train loss=0.1917\n",
      "Epoch 0, step 4740, train loss=0.2240\n",
      "Epoch 0, step 4750, train loss=0.0778\n",
      "Epoch 0, step 4760, train loss=0.4502\n",
      "Epoch 0, step 4770, train loss=0.2813\n",
      "Epoch 0, step 4780, train loss=0.1211\n",
      "Epoch 0, step 4790, train loss=0.2686\n",
      "Epoch 0, step 4800, train loss=0.2943\n",
      "Epoch 0, step 4810, train loss=0.1548\n",
      "Epoch 0, step 4820, train loss=0.1396\n",
      "Epoch 0, step 4830, train loss=0.1050\n",
      "Epoch 0, step 4840, train loss=0.2144\n",
      "Epoch 0, step 4850, train loss=0.0989\n",
      "Epoch 0, step 4860, train loss=0.3547\n",
      "Epoch 0, step 4870, train loss=0.3534\n",
      "Epoch 0, step 4880, train loss=0.1933\n",
      "Epoch 0, step 4890, train loss=0.1250\n",
      "Epoch 0, step 4900, train loss=0.1027\n",
      "Epoch 0, step 4910, train loss=0.2658\n",
      "Epoch 0, step 4920, train loss=0.3662\n",
      "Epoch 0, step 4930, train loss=0.4112\n",
      "Epoch 0, step 4940, train loss=0.2449\n",
      "Epoch 0, step 4950, train loss=0.3080\n",
      "Epoch 0, step 4960, train loss=0.2219\n",
      "Epoch 0, step 4970, train loss=0.1906\n",
      "Epoch 0, step 4980, train loss=0.3609\n",
      "Epoch 0, step 4990, train loss=0.2152\n",
      "[Global Step 5000] Interim val_loss=0.2488\n",
      "  (New best val_loss!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 5000, train loss=0.3544\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma insertion_isovarspars_free :\n",
      "  \"insertion (nth_default 0 (list_update L var x)) (isolate_variable_sparse (p::real mpoly) var (i::nat))\n",
      "  =insertion (nth_default 0 (list_update L var y)) (isolate_variable_sparse (p::real mpoly) var (i::nat))\" proof (state)\n",
      "goal (1 subgoal):\n",
      " 1. insertion (nth_default 0 (L[var := x]))\n",
      "     (isolate_variable_sparse p var i) =\n",
      "    insertion (nth_default 0 (L[var := y]))\n",
      "     (isolate_variable_sparse p var i)\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma insertion_isovarspars_free :\n",
      "  \"insertion (nth_default 0 (list_update L var x)) (isolate_variable_sparse (p::real mpoly) var (i::nat))\n",
      "  =insertion (nth_default 0 (list_update L var y)) (isolate_variable_sparse (p::real mpoly) var (i::nat))\"\n",
      "\n",
      "proof (state)\n",
      "goal (1 subgoal):\n",
      " 1. insertion (nth_default 0 (L[var := x]))\n",
      "     (isolate_variable_sparse p var i) =\n",
      "    insertion (nth_default 0 (L[var := y]))\n",
      "     (isolate_variable_sparse p var i)\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by (simp add: not_in_isovarspar)\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 5010, train loss=0.2485\n",
      "Epoch 0, step 5020, train loss=0.1639\n",
      "Epoch 0, step 5030, train loss=0.1807\n",
      "Epoch 0, step 5040, train loss=0.3394\n",
      "Epoch 0, step 5050, train loss=0.2738\n",
      "Epoch 0, step 5060, train loss=0.1492\n",
      "Epoch 0, step 5070, train loss=0.2164\n",
      "Epoch 0, step 5080, train loss=0.4340\n",
      "Epoch 0, step 5090, train loss=0.3186\n",
      "Epoch 0, step 5100, train loss=0.1325\n",
      "Epoch 0, step 5110, train loss=0.4776\n",
      "Epoch 0, step 5120, train loss=0.3125\n",
      "Epoch 0, step 5130, train loss=0.2263\n",
      "Epoch 0, step 5140, train loss=0.1986\n",
      "Epoch 0, step 5150, train loss=0.1150\n",
      "Epoch 0, step 5160, train loss=0.0642\n",
      "Epoch 0, step 5170, train loss=0.1358\n",
      "Epoch 0, step 5180, train loss=0.0945\n",
      "Epoch 0, step 5190, train loss=0.3271\n",
      "Epoch 0, step 5200, train loss=0.1885\n",
      "Epoch 0, step 5210, train loss=0.5367\n",
      "Epoch 0, step 5220, train loss=0.0768\n",
      "Epoch 0, step 5230, train loss=0.2677\n",
      "Epoch 0, step 5240, train loss=0.1689\n",
      "Epoch 0, step 5250, train loss=0.2541\n",
      "Epoch 0, step 5260, train loss=0.1271\n",
      "Epoch 0, step 5270, train loss=0.3583\n",
      "Epoch 0, step 5280, train loss=0.3689\n",
      "Epoch 0, step 5290, train loss=0.1637\n",
      "Epoch 0, step 5300, train loss=0.3303\n",
      "Epoch 0, step 5310, train loss=0.4560\n",
      "Epoch 0, step 5320, train loss=0.3414\n",
      "Epoch 0, step 5330, train loss=0.1757\n",
      "Epoch 0, step 5340, train loss=0.1377\n",
      "Epoch 0, step 5350, train loss=0.2444\n",
      "Epoch 0, step 5360, train loss=0.2449\n",
      "Epoch 0, step 5370, train loss=0.2352\n",
      "Epoch 0, step 5380, train loss=0.1316\n",
      "Epoch 0, step 5390, train loss=0.1207\n",
      "Epoch 0, step 5400, train loss=0.0828\n",
      "Epoch 0, step 5410, train loss=0.2657\n",
      "Epoch 0, step 5420, train loss=0.1190\n",
      "Epoch 0, step 5430, train loss=0.2325\n",
      "Epoch 0, step 5440, train loss=0.2790\n",
      "Epoch 0, step 5450, train loss=0.6206\n",
      "Epoch 0, step 5460, train loss=0.2117\n",
      "Epoch 0, step 5470, train loss=0.0870\n",
      "Epoch 0, step 5480, train loss=0.3348\n",
      "Epoch 0, step 5490, train loss=0.4160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 5500, train loss=0.1273\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma length_initialization_list_le_length_start_addrs:\n",
      "  \"length initialization_list \\<ge> length start_addrs\" proof (state)\n",
      "this:\n",
      "  \n",
      "\n",
      "goal (2 subgoals):\n",
      " 1. \\<And>h ads.\n",
      "       length (fst (snd (foldl create_initial_object (h, ads, True) [])))\n",
      "       \\<le> length ads + length []\n",
      " 2. \\<And>a xs h ads.\n",
      "       (\\<And>h ads.\n",
      "           length\n",
      "            (fst (snd (foldl create_initial_object (h, ads, True) xs)))\n",
      "           \\<le> length ads + length xs) \\<Longrightarrow>\n",
      "       length\n",
      "        (fst (snd (foldl create_initial_object (h, ads, True) (a # xs))))\n",
      "       \\<le> length ads + length (a # xs)\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma length_initialization_list_le_length_start_addrs:\n",
      "  \"length initialization_list \\<ge> length start_addrs\"\n",
      "\n",
      "proof (state)\n",
      "this:\n",
      "  \n",
      "\n",
      "goal (2 subgoals):\n",
      " 1. \\<And>h ads.\n",
      "       length (fst (snd (foldl create_initial_object (h, ads, True) [])))\n",
      "       \\<le> length ads + length []\n",
      " 2. \\<And>a xs h ads.\n",
      "       (\\<And>h ads.\n",
      "           length\n",
      "            (fst (snd (foldl create_initial_object (h, ads, True) xs)))\n",
      "           \\<le> length ads + length xs) \\<Longrightarrow>\n",
      "       length\n",
      "        (fst (snd (foldl create_initial_object (h, ads, True) (a # xs))))\n",
      "       \\<le> length ads + length (a # xs)\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by simp\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 5510, train loss=0.1858\n",
      "Epoch 0, step 5520, train loss=0.4431\n",
      "Epoch 0, step 5530, train loss=0.1643\n",
      "Epoch 0, step 5540, train loss=0.3358\n",
      "Epoch 0, step 5550, train loss=0.0858\n",
      "Epoch 0, step 5560, train loss=0.3127\n",
      "Epoch 0, step 5570, train loss=0.0865\n",
      "Epoch 0, step 5580, train loss=0.2949\n",
      "Epoch 0, step 5590, train loss=0.0989\n",
      "Epoch 0, step 5600, train loss=0.2607\n",
      "Epoch 0, step 5610, train loss=0.1060\n",
      "Epoch 0, step 5620, train loss=0.5119\n",
      "Epoch 0, step 5630, train loss=0.2470\n",
      "Epoch 0, step 5640, train loss=0.3770\n",
      "Epoch 0, step 5650, train loss=0.3527\n",
      "Epoch 0, step 5660, train loss=0.1042\n",
      "Epoch 0, step 5670, train loss=0.1820\n",
      "Epoch 0, step 5680, train loss=0.1442\n",
      "Epoch 0, step 5690, train loss=0.3747\n",
      "Epoch 0, step 5700, train loss=0.1963\n",
      "Epoch 0, step 5710, train loss=0.1160\n",
      "Epoch 0, step 5720, train loss=0.1320\n",
      "Epoch 0, step 5730, train loss=0.1044\n",
      "Epoch 0, step 5740, train loss=0.1202\n",
      "Epoch 0, step 5750, train loss=0.3146\n",
      "Epoch 0, step 5760, train loss=0.4568\n",
      "Epoch 0, step 5770, train loss=0.3223\n",
      "Epoch 0, step 5780, train loss=0.0745\n",
      "Epoch 0, step 5790, train loss=0.2364\n",
      "Epoch 0, step 5800, train loss=0.1509\n",
      "Epoch 0, step 5810, train loss=0.5638\n",
      "Epoch 0, step 5820, train loss=0.2698\n",
      "Epoch 0, step 5830, train loss=0.2767\n",
      "Epoch 0, step 5840, train loss=0.2686\n",
      "Epoch 0, step 5850, train loss=0.2235\n",
      "Epoch 0, step 5860, train loss=0.1791\n",
      "Epoch 0, step 5870, train loss=0.6037\n",
      "Epoch 0, step 5880, train loss=0.1921\n",
      "Epoch 0, step 5890, train loss=0.2302\n",
      "Epoch 0, step 5900, train loss=0.2259\n",
      "Epoch 0, step 5910, train loss=0.2607\n",
      "Epoch 0, step 5920, train loss=0.0675\n",
      "Epoch 0, step 5930, train loss=0.1427\n",
      "Epoch 0, step 5940, train loss=0.0764\n",
      "Epoch 0, step 5950, train loss=0.0564\n",
      "Epoch 0, step 5960, train loss=0.1282\n",
      "Epoch 0, step 5970, train loss=0.0739\n",
      "Epoch 0, step 5980, train loss=0.0856\n",
      "Epoch 0, step 5990, train loss=0.2636\n",
      "[Global Step 6000] Interim val_loss=0.2471\n",
      "  (New best val_loss!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 6000, train loss=0.3397\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma linear_strict_order_split:\n",
      "  \"linear_strict_order x \\<longleftrightarrow> transitive x \\<and> split bot x (-1)\" \n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma linear_strict_order_split:\n",
      "  \"linear_strict_order x \\<longleftrightarrow> transitive x \\<and> split bot x (-1)\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reference step:\n",
      " using local.strict_order_var by blast\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 6010, train loss=0.1309\n",
      "Epoch 0, step 6020, train loss=0.3338\n",
      "Epoch 0, step 6030, train loss=0.0634\n",
      "Epoch 0, step 6040, train loss=0.5323\n",
      "Epoch 0, step 6050, train loss=0.1175\n",
      "Epoch 0, step 6060, train loss=0.1931\n",
      "Epoch 0, step 6070, train loss=0.3062\n",
      "Epoch 0, step 6080, train loss=0.1609\n",
      "Epoch 0, step 6090, train loss=0.2112\n",
      "Epoch 0, step 6100, train loss=0.1450\n",
      "Epoch 0, step 6110, train loss=0.2772\n",
      "Epoch 0, step 6120, train loss=0.1330\n",
      "Epoch 0, step 6130, train loss=0.1309\n",
      "Epoch 0, step 6140, train loss=0.2821\n",
      "Epoch 0, step 6150, train loss=0.3167\n",
      "Epoch 0, step 6160, train loss=0.3826\n",
      "Epoch 0, step 6170, train loss=0.1797\n",
      "Epoch 0, step 6180, train loss=0.2738\n",
      "Epoch 0, step 6190, train loss=0.2582\n",
      "Epoch 0, step 6200, train loss=0.0567\n",
      "Epoch 0, step 6210, train loss=0.9224\n",
      "Epoch 0, step 6220, train loss=0.3650\n",
      "Epoch 0, step 6230, train loss=0.0747\n",
      "Epoch 0, step 6240, train loss=0.1773\n",
      "Epoch 0, step 6250, train loss=0.2863\n",
      "Epoch 0, step 6260, train loss=0.1676\n",
      "Epoch 0, step 6270, train loss=0.3445\n",
      "Epoch 0, step 6280, train loss=0.0730\n",
      "Epoch 0, step 6290, train loss=0.2649\n",
      "Epoch 0, step 6300, train loss=0.1831\n",
      "Epoch 0, step 6310, train loss=0.0671\n",
      "Epoch 0, step 6320, train loss=0.5160\n",
      "Epoch 0, step 6330, train loss=0.1792\n",
      "Epoch 0, step 6340, train loss=0.2854\n",
      "Epoch 0, step 6350, train loss=0.1880\n",
      "Epoch 0, step 6360, train loss=0.1076\n",
      "Epoch 0, step 6370, train loss=0.0759\n",
      "Epoch 0, step 6380, train loss=0.0533\n",
      "Epoch 0, step 6390, train loss=0.4378\n",
      "Epoch 0, step 6400, train loss=0.2322\n",
      "Epoch 0, step 6410, train loss=0.1546\n",
      "Epoch 0, step 6420, train loss=0.1198\n",
      "Epoch 0, step 6430, train loss=0.2718\n",
      "Epoch 0, step 6440, train loss=0.3640\n",
      "Epoch 0, step 6450, train loss=0.5298\n",
      "Epoch 0, step 6460, train loss=0.1811\n",
      "Epoch 0, step 6470, train loss=0.1390\n",
      "Epoch 0, step 6480, train loss=0.3824\n",
      "Epoch 0, step 6490, train loss=0.2586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 6500, train loss=0.1173\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma surj_std_dual:\n",
      "  \"std_dual ` S \\<supseteq> dual_space S\" if \"subspace S\" \"finite B\" proof (prove)\n",
      "goal (3 subgoals):\n",
      " 1. extensional0 S y\n",
      " 2. extensional0 S (local.std_dual (\\<Sum>i\\<in>B. y i *\\<^sub>R i))\n",
      " 3. \\<And>x.\n",
      "       x \\<in> S \\<Longrightarrow>\n",
      "       y x = local.std_dual (\\<Sum>i\\<in>B. y i *\\<^sub>R i) x\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma surj_std_dual:\n",
      "  \"std_dual ` S \\<supseteq> dual_space S\" if \"subspace S\" \"finite B\"\n",
      "\n",
      "proof (prove)\n",
      "goal (3 subgoals):\n",
      " 1. extensional0 S y\n",
      " 2. extensional0 S (local.std_dual (\\<Sum>i\\<in>B. y i *\\<^sub>R i))\n",
      " 3. \\<And>x.\n",
      "       x \\<in> S \\<Longrightarrow>\n",
      "       y x = local.std_dual (\\<Sum>i\\<in>B. y i *\\<^sub>R i) x\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by (simp add: y)\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 6510, train loss=0.2444\n",
      "Epoch 0, step 6520, train loss=0.2463\n",
      "Epoch 0, step 6530, train loss=0.1799\n",
      "Epoch 0, step 6540, train loss=0.2393\n",
      "Epoch 0, step 6550, train loss=0.1650\n",
      "Epoch 0, step 6560, train loss=0.2137\n",
      "Epoch 0, step 6570, train loss=0.1537\n",
      "Epoch 0, step 6580, train loss=0.1971\n",
      "Epoch 0, step 6590, train loss=0.8030\n",
      "Epoch 0, step 6600, train loss=0.1972\n",
      "Epoch 0, step 6610, train loss=0.1174\n",
      "Epoch 0, step 6620, train loss=0.1123\n",
      "Epoch 0, step 6630, train loss=0.2599\n",
      "Epoch 0, step 6640, train loss=0.4222\n",
      "Epoch 0, step 6650, train loss=0.1254\n",
      "Epoch 0, step 6660, train loss=0.2567\n",
      "Epoch 0, step 6670, train loss=0.6881\n",
      "Epoch 0, step 6680, train loss=0.2057\n",
      "Epoch 0, step 6690, train loss=0.1141\n",
      "Epoch 0, step 6700, train loss=0.1353\n",
      "Epoch 0, step 6710, train loss=0.2742\n",
      "Epoch 0, step 6720, train loss=0.2736\n",
      "Epoch 0, step 6730, train loss=0.1327\n",
      "Epoch 0, step 6740, train loss=0.0897\n",
      "Epoch 0, step 6750, train loss=0.2159\n",
      "Epoch 0, step 6760, train loss=0.2596\n",
      "Epoch 0, step 6770, train loss=0.1810\n",
      "Epoch 0, step 6780, train loss=0.1943\n",
      "Epoch 0, step 6790, train loss=0.4004\n",
      "Epoch 0, step 6800, train loss=0.1177\n",
      "Epoch 0, step 6810, train loss=0.1688\n",
      "Epoch 0, step 6820, train loss=0.4641\n",
      "Epoch 0, step 6830, train loss=0.0936\n",
      "Epoch 0, step 6840, train loss=0.1766\n",
      "Epoch 0, step 6850, train loss=0.3573\n",
      "Epoch 0, step 6860, train loss=0.2607\n",
      "Epoch 0, step 6870, train loss=0.9173\n",
      "Epoch 0, step 6880, train loss=0.2709\n",
      "Epoch 0, step 6890, train loss=0.2500\n",
      "Epoch 0, step 6900, train loss=0.1700\n",
      "Epoch 0, step 6910, train loss=0.1557\n",
      "Epoch 0, step 6920, train loss=0.0634\n",
      "Epoch 0, step 6930, train loss=0.0703\n",
      "Epoch 0, step 6940, train loss=0.0533\n",
      "Epoch 0, step 6950, train loss=0.3371\n",
      "Epoch 0, step 6960, train loss=0.1481\n",
      "Epoch 0, step 6970, train loss=0.1570\n",
      "Epoch 0, step 6980, train loss=0.2719\n",
      "Epoch 0, step 6990, train loss=0.1425\n",
      "[Global Step 7000] Interim val_loss=0.2444\n",
      "  (New best val_loss!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 7000, train loss=0.1242\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " theorem WHATWHERE_Secure_Assign:\n",
      "  assumes dhind: \"e \\<equiv>\\<^bsub>DA x,(htchLoc \\<iota>)\\<^esub> e\"\n",
      "  assumes dheq_imp: \"\\<forall>m m' d \\<iota>'. (m \\<sim>\\<^bsub>d,(htchLoc \\<iota>')\\<^esub> m' \\<and> \n",
      "  \\<lbrakk>x :=\\<^bsub>\\<iota>\\<^esub> e\\<rbrakk>(m) =\\<^bsub>d\\<^esub> \\<lbrakk>x :=\\<^bsub>\\<iota>\\<^esub> e\\<rbrakk>(m'))\n",
      "  \\<longrightarrow> \\<lbrakk>x :=\\<^bsub>\\<iota>\\<^esub> e\\<rbrakk>(m) \\<sim>\\<^bsub>d,(htchLoc \\<iota>')\\<^esub> \\<lbrakk>x :=\\<^bsub>\\<iota>\\<^esub> e\\<rbrakk>(m')\" \n",
      "  shows \"WHATWHERE_Secure [x :=\\<^bsub>\\<iota>\\<^esub> e]\" proof (chain)\n",
      "picking this:\n",
      "  e \\<equiv>\\<^bsub>DA x,htchLoc \\<iota>\\<^esub> e\n",
      "  ?e \\<equiv>\\<^bsub>DA ?x,htchLoc ?\\<iota>\\<^esub> ?e \\<Longrightarrow>\n",
      "  NDC ?d (?x:=\\<^bsub>?\\<iota>\\<^esub> ?e) \\<or>\n",
      "  IDC ?d (?x:=\\<^bsub>?\\<iota>\\<^esub> ?e)\n",
      "   (htchLoc (pp (?x:=\\<^bsub>?\\<iota>\\<^esub> ?e)))\n",
      "  \\<not> IDC d (x:=\\<^bsub>\\<iota>\\<^esub> e) (htchLoc \\<iota>)\n",
      "\n",
      "\n",
      "Generated step:\n",
      " theorem WHATWHERE_Secure_Assign:\n",
      "  assumes dhind: \"e \\<equiv>\\<^bsub>DA x,(htchLoc \\<iota>)\\<^esub> e\"\n",
      "  assumes dheq_imp: \"\\<forall>m m' d \\<iota>'. (m \\<sim>\\<^bsub>d,(htchLoc \\<iota>')\\<^esub> m' \\<and> \n",
      "  \\<lbrakk>x :=\\<^bsub>\\<iota>\\<^esub> e\\<rbrakk>(m) =\\<^bsub>d\\<^esub> \\<lbrakk>x :=\\<^bsub>\\<iota>\\<^esub> e\\<rbrakk>(m'))\n",
      "  \\<longrightarrow> \\<lbrakk>x :=\\<^bsub>\\<iota>\\<^esub> e\\<rbrakk>(m) \\<sim>\\<^bsub>d,(htchLoc \\<iota>')\\<^esub> \\<lbrakk>x :=\\<^bsub>\\<iota>\\<^esub> e\\<rbrakk>(m')\" \n",
      "  shows \"WHATWHERE_Secure [x :=\\<^bsub>\\<iota>\\<^esub> e]\"\n",
      "\n",
      "proof (chain)\n",
      "picking this:\n",
      "  e \\<equiv>\\<^bsub>DA x,htchLoc \\<iota>\\<^esub> e\n",
      "  ?e \\<equiv>\\<^bsub>DA ?x,htchLoc ?\\<iota>\\<^esub> ?e \\<Longrightarrow>\n",
      "  NDC ?d (?x:=\\<^bsub>?\\<iota>\\<^esub> ?e) \\<or>\n",
      "  IDC ?d (?x:=\\<^bsub>?\\<iota>\\<^esub> ?e)\n",
      "   (htchLoc (pp (?x:=\\<^bsub>?\\<iota>\\<^esub> ?e)))\n",
      "  \\<not> IDC d (x:=\\<^bsub>\\<iota>\\<^esub> e) (htchLoc \\<iota>)\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by auto\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 7010, train loss=0.3427\n",
      "Epoch 0, step 7020, train loss=0.0781\n",
      "Epoch 0, step 7030, train loss=0.2135\n",
      "Epoch 0, step 7040, train loss=0.2050\n",
      "Epoch 0, step 7050, train loss=0.2722\n",
      "Epoch 0, step 7060, train loss=0.1411\n",
      "Epoch 0, step 7070, train loss=0.0799\n",
      "Epoch 0, step 7080, train loss=0.3012\n",
      "Epoch 0, step 7090, train loss=0.1403\n",
      "Epoch 0, step 7100, train loss=0.1817\n",
      "Epoch 0, step 7110, train loss=0.1278\n",
      "Epoch 0, step 7120, train loss=0.1063\n",
      "Epoch 0, step 7130, train loss=0.1400\n",
      "Epoch 0, step 7140, train loss=0.2500\n",
      "Epoch 0, step 7150, train loss=0.5610\n",
      "Epoch 0, step 7160, train loss=0.0672\n",
      "Epoch 0, step 7170, train loss=0.1974\n",
      "Epoch 0, step 7180, train loss=0.1480\n",
      "Epoch 0, step 7190, train loss=0.1096\n",
      "Epoch 0, step 7200, train loss=0.1870\n",
      "Epoch 0, step 7210, train loss=0.3116\n",
      "Epoch 0, step 7220, train loss=0.5280\n",
      "Epoch 0, step 7230, train loss=0.1336\n",
      "Epoch 0, step 7240, train loss=0.1692\n",
      "Epoch 0, step 7250, train loss=0.5646\n",
      "Epoch 0, step 7260, train loss=0.4706\n",
      "Epoch 0, step 7270, train loss=0.1541\n",
      "Epoch 0, step 7280, train loss=0.0920\n",
      "Epoch 0, step 7290, train loss=0.2110\n",
      "Epoch 0, step 7300, train loss=0.2157\n",
      "Epoch 0, step 7310, train loss=0.3768\n",
      "Epoch 0, step 7320, train loss=0.1056\n",
      "Epoch 0, step 7330, train loss=0.1027\n",
      "Epoch 0, step 7340, train loss=0.3144\n",
      "Epoch 0, step 7350, train loss=0.7927\n",
      "Epoch 0, step 7360, train loss=0.1780\n",
      "Epoch 0, step 7370, train loss=0.2516\n",
      "Epoch 0, step 7380, train loss=0.1423\n",
      "Epoch 0, step 7390, train loss=0.2945\n",
      "Epoch 0, step 7400, train loss=0.4391\n",
      "Epoch 0, step 7410, train loss=0.2367\n",
      "Epoch 0, step 7420, train loss=0.1760\n",
      "Epoch 0, step 7430, train loss=0.2282\n",
      "Epoch 0, step 7440, train loss=0.2083\n",
      "Epoch 0, step 7450, train loss=0.2084\n",
      "Epoch 0, step 7460, train loss=0.3196\n",
      "Epoch 0, step 7470, train loss=0.0499\n",
      "Epoch 0, step 7480, train loss=0.1396\n",
      "Epoch 0, step 7490, train loss=0.2054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 7500, train loss=0.1283\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma LeftDerivationFix_derivation_ge_is_nonterminal:\n",
      "  assumes ldfix: \"LeftDerivationFix \\<alpha> i D j \\<gamma>\"\n",
      "  assumes derivation_ge_d: \"derivation_ge D d\"\n",
      "  assumes is_nonterminal: \"is_nonterminal (\\<gamma> ! j)\"\n",
      "  shows \"(D = [] \\<and> \\<alpha> = \\<gamma> \\<and> i = j) \\<or> (i > d \\<and> j \\<ge> d)\" proof (chain)\n",
      "picking this:\n",
      "  \\<exists>U a1 a2 b1.\n",
      "     splits_at \\<alpha> i a1 U a2 \\<and>\n",
      "     splits_at \\<gamma> j b1 U a2 \\<and> LeftDerivation a1 D b1\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma LeftDerivationFix_derivation_ge_is_nonterminal:\n",
      "  assumes ldfix: \"LeftDerivationFix \\<alpha> i D j \\<gamma>\"\n",
      "  assumes derivation_ge_d: \"derivation_ge D d\"\n",
      "  assumes is_nonterminal: \"is_nonterminal (\\<gamma> ! j)\"\n",
      "  shows \"(D = [] \\<and> \\<alpha> = \\<gamma> \\<and> i = j) \\<or> (i > d \\<and> j \\<ge> d)\"\n",
      "\n",
      "proof (chain)\n",
      "picking this:\n",
      "  \\<exists>U a1 a2 b1.\n",
      "     splits_at \\<alpha> i a1 U a2 \\<and>\n",
      "     splits_at \\<gamma> j b1 U a2 \\<and> LeftDerivation a1 D b1\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by blast\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 7510, train loss=0.1950\n",
      "Epoch 0, step 7520, train loss=0.1134\n",
      "Epoch 0, step 7530, train loss=0.2046\n",
      "Epoch 0, step 7540, train loss=0.2209\n",
      "Epoch 0, step 7550, train loss=0.1164\n",
      "Epoch 0, step 7560, train loss=0.1060\n",
      "Epoch 0, step 7570, train loss=0.1474\n",
      "Epoch 0, step 7580, train loss=0.0818\n",
      "Epoch 0, step 7590, train loss=0.2359\n",
      "Epoch 0, step 7600, train loss=0.6916\n",
      "Epoch 0, step 7610, train loss=0.1160\n",
      "Epoch 0, step 7620, train loss=0.2260\n",
      "Epoch 0, step 7630, train loss=0.4850\n",
      "Epoch 0, step 7640, train loss=0.1636\n",
      "Epoch 0, step 7650, train loss=0.1017\n",
      "Epoch 0, step 7660, train loss=0.2021\n",
      "Epoch 0, step 7670, train loss=0.1943\n",
      "Epoch 0, step 7680, train loss=0.3358\n",
      "Epoch 0, step 7690, train loss=0.1441\n",
      "Epoch 0, step 7700, train loss=0.1818\n",
      "Epoch 0, step 7710, train loss=0.2273\n",
      "Epoch 0, step 7720, train loss=0.1390\n",
      "Epoch 0, step 7730, train loss=0.1254\n",
      "Epoch 0, step 7740, train loss=0.0912\n",
      "Epoch 0, step 7750, train loss=0.1536\n",
      "Epoch 0, step 7760, train loss=0.2083\n",
      "Epoch 0, step 7770, train loss=0.2443\n",
      "Epoch 0, step 7780, train loss=0.1649\n",
      "Epoch 0, step 7790, train loss=0.3055\n",
      "Epoch 0, step 7800, train loss=1.4429\n",
      "Epoch 0, step 7810, train loss=0.3537\n",
      "Epoch 0, step 7820, train loss=0.3548\n",
      "Epoch 0, step 7830, train loss=0.0767\n",
      "Epoch 0, step 7840, train loss=0.1874\n",
      "Epoch 0, step 7850, train loss=0.0962\n",
      "Epoch 0, step 7860, train loss=0.2342\n",
      "Epoch 0, step 7870, train loss=0.0994\n",
      "Epoch 0, step 7880, train loss=0.1213\n",
      "Epoch 0, step 7890, train loss=0.1524\n",
      "Epoch 0, step 7900, train loss=0.3582\n",
      "Epoch 0, step 7910, train loss=0.2384\n",
      "Epoch 0, step 7920, train loss=0.3536\n",
      "Epoch 0, step 7930, train loss=0.3900\n",
      "Epoch 0, step 7940, train loss=0.3052\n",
      "Epoch 0, step 7950, train loss=0.2045\n",
      "Epoch 0, step 7960, train loss=0.4151\n",
      "Epoch 0, step 7970, train loss=0.3483\n",
      "Epoch 0, step 7980, train loss=0.2827\n",
      "Epoch 0, step 7990, train loss=0.1979\n",
      "[Global Step 8000] Interim val_loss=0.2426\n",
      "  (New best val_loss!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/siai/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 8000, train loss=0.1266\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma no_rewritten_fermat3: \n",
      "  \"\\<not> (\\<exists> v w. v^3+w^3 = x^3 \\<and> v*w*x \\<noteq> 0 \\<and> even (x::int) \\<and> coprime v w)\" proof (chain)\n",
      "picking this:\n",
      "  \\<not> 1 \\<le> gcd (2 * p) (p\\<^sup>2 + 3 * q\\<^sup>2)\n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma no_rewritten_fermat3: \n",
      "  \"\\<not> (\\<exists> v w. v^3+w^3 = x^3 \\<and> v*w*x \\<noteq> 0 \\<and> even (x::int) \\<and> coprime v w)\"\n",
      "\n",
      "proof (chain)\n",
      "picking this:\n",
      "  \\<not> 1 \\<le> gcd (2 * p) (p\\<^sup>2 + 3 * q\\<^sup>2)\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by linarith\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 8010, train loss=0.2977\n",
      "Epoch 0, step 8020, train loss=0.1233\n",
      "Epoch 0, step 8030, train loss=0.1076\n",
      "Epoch 0, step 8040, train loss=0.4141\n",
      "Epoch 0, step 8050, train loss=0.2048\n",
      "Epoch 0, step 8060, train loss=0.1998\n",
      "Epoch 0, step 8070, train loss=0.2300\n",
      "Epoch 0, step 8080, train loss=0.1434\n",
      "Epoch 0, step 8090, train loss=0.0905\n",
      "Epoch 0, step 8100, train loss=0.3506\n",
      "Epoch 0, step 8110, train loss=0.3187\n",
      "Epoch 0, step 8120, train loss=5.0701\n",
      "Epoch 0, step 8130, train loss=0.1881\n",
      "Epoch 0, step 8140, train loss=0.1788\n",
      "Epoch 0, step 8150, train loss=0.2162\n",
      "Epoch 0, step 8160, train loss=0.2295\n",
      "Epoch 0, step 8170, train loss=0.2406\n",
      "Epoch 0, step 8180, train loss=0.1454\n",
      "Epoch 0, step 8190, train loss=0.1382\n",
      "Epoch 0, step 8200, train loss=0.1998\n",
      "Epoch 0, step 8210, train loss=0.1368\n",
      "Epoch 0, step 8220, train loss=0.2528\n",
      "Epoch 0, step 8230, train loss=0.2415\n",
      "Epoch 0, step 8240, train loss=0.6183\n",
      "Epoch 0, step 8250, train loss=0.1623\n",
      "Epoch 0, step 8260, train loss=0.1822\n",
      "Epoch 0, step 8270, train loss=0.5875\n",
      "Epoch 0, step 8280, train loss=0.1965\n",
      "Epoch 0, step 8290, train loss=0.2795\n",
      "Epoch 0, step 8300, train loss=0.1696\n",
      "Epoch 0, step 8310, train loss=0.5676\n",
      "Epoch 0, step 8320, train loss=0.6399\n",
      "Epoch 0, step 8330, train loss=0.2250\n",
      "Epoch 0, step 8340, train loss=0.1519\n",
      "Epoch 0, step 8350, train loss=0.2357\n",
      "Epoch 0, step 8360, train loss=0.3667\n",
      "Epoch 0, step 8370, train loss=0.3581\n",
      "Epoch 0, step 8380, train loss=0.1227\n",
      "Epoch 0, step 8390, train loss=0.2844\n",
      "Epoch 0, step 8400, train loss=0.3927\n",
      "Epoch 0, step 8410, train loss=0.2961\n",
      "Epoch 0, step 8420, train loss=0.2161\n",
      "Epoch 0, step 8430, train loss=0.2238\n",
      "Epoch 0, step 8440, train loss=0.1764\n",
      "Epoch 0, step 8450, train loss=0.2183\n",
      "Epoch 0, step 8460, train loss=0.1317\n",
      "Epoch 0, step 8470, train loss=0.1193\n",
      "Epoch 0, step 8480, train loss=0.3365\n",
      "Epoch 0, step 8490, train loss=0.1392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 8500, train loss=0.1032\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " theorem arith_prog_rel_primes_solution:\n",
      "  fixes n :: nat\n",
      "  assumes \\<open>n > 1\\<close>\n",
      "  shows \\<open>(prime n \\<or> (\\<exists> k. n = 2^k) \\<or> n = 6) \\<longleftrightarrow>  \n",
      "(\\<exists> a b m. m \\<noteq> 0 \\<and> {x | x. x < n \\<and> coprime x n} = {a+j*b| j::nat. j < m})\\<close> proof (state)\n",
      "this:\n",
      "  m - 1 \\<le> i\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. n = 2 + (m - 1) * b\n",
      "\n",
      "\n",
      "Generated step:\n",
      " theorem arith_prog_rel_primes_solution:\n",
      "  fixes n :: nat\n",
      "  assumes \\<open>n > 1\\<close>\n",
      "  shows \\<open>(prime n \\<or> (\\<exists> k. n = 2^k) \\<or> n = 6) \\<longleftrightarrow>  \n",
      "(\\<exists> a b m. m \\<noteq> 0 \\<and> {x | x. x < n \\<and> coprime x n} = {a+j*b| j::nat. j < m})\\<close>\n",
      "\n",
      "proof (state)\n",
      "this:\n",
      "  m - 1 \\<le> i\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. n = 2 + (m - 1) * b\n",
      "\n",
      "\n",
      "Reference step:\n",
      " using \\<open>i \\<le> m - 1\\<close> le_antisym by presburger\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 8510, train loss=0.3428\n",
      "Epoch 0, step 8520, train loss=0.2718\n",
      "Epoch 0, step 8530, train loss=0.3299\n",
      "Epoch 0, step 8540, train loss=0.3617\n",
      "Epoch 0, step 8550, train loss=0.1792\n",
      "Epoch 0, step 8560, train loss=0.3622\n",
      "Epoch 0, step 8570, train loss=0.1686\n",
      "Epoch 0, step 8580, train loss=0.1836\n",
      "Epoch 0, step 8590, train loss=0.1484\n",
      "Epoch 0, step 8600, train loss=0.1251\n",
      "Epoch 0, step 8610, train loss=0.1262\n",
      "Epoch 0, step 8620, train loss=0.2442\n",
      "Epoch 0, step 8630, train loss=0.1700\n",
      "Epoch 0, step 8640, train loss=0.2140\n",
      "Epoch 0, step 8650, train loss=0.2666\n",
      "Epoch 0, step 8660, train loss=0.1630\n",
      "Epoch 0, step 8670, train loss=0.1349\n",
      "Epoch 0, step 8680, train loss=0.2814\n",
      "Epoch 0, step 8690, train loss=0.1358\n",
      "Epoch 0, step 8700, train loss=0.2132\n",
      "Epoch 0, step 8710, train loss=0.1656\n",
      "Epoch 0, step 8720, train loss=0.0582\n",
      "Epoch 0, step 8730, train loss=0.6452\n",
      "Epoch 0, step 8740, train loss=0.1204\n",
      "Epoch 0, step 8750, train loss=0.3044\n",
      "Epoch 0, step 8760, train loss=0.3267\n",
      "Epoch 0, step 8770, train loss=0.1335\n",
      "Epoch 0, step 8780, train loss=0.1886\n",
      "Epoch 0, step 8790, train loss=0.1073\n",
      "Epoch 0, step 8800, train loss=0.0747\n",
      "Epoch 0, step 8810, train loss=0.0687\n",
      "Epoch 0, step 8820, train loss=0.1995\n",
      "Epoch 0, step 8830, train loss=0.3180\n",
      "Epoch 0, step 8840, train loss=0.0927\n",
      "Epoch 0, step 8850, train loss=0.1690\n",
      "Epoch 0, step 8860, train loss=0.2487\n",
      "Epoch 0, step 8870, train loss=0.1357\n",
      "Epoch 0, step 8880, train loss=0.0795\n",
      "Epoch 0, step 8890, train loss=0.5207\n",
      "Epoch 0, step 8900, train loss=0.1178\n",
      "Epoch 0, step 8910, train loss=0.4460\n",
      "Epoch 0, step 8920, train loss=0.3275\n",
      "Epoch 0, step 8930, train loss=0.3115\n",
      "Epoch 0, step 8940, train loss=0.3499\n",
      "Epoch 0, step 8950, train loss=0.1576\n",
      "Epoch 0, step 8960, train loss=0.3606\n",
      "Epoch 0, step 8970, train loss=0.1321\n",
      "Epoch 0, step 8980, train loss=0.0744\n",
      "Epoch 0, step 8990, train loss=0.1091\n",
      "[Global Step 9000] Interim val_loss=0.2432\n",
      "  (No improvement. Count=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 9000, train loss=0.2319\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " theorem (in J_conf_read) assumes wf: \"wf_J_prog P\"\n",
      "  shows subject_reduction:\n",
      "  \"\\<lbrakk> extTA,P,t \\<turnstile> \\<langle>e,s\\<rangle> -ta\\<rightarrow> \\<langle>e',s'\\<rangle>; E \\<turnstile> s \\<surd>; P,E,hp s \\<turnstile> e:T; P,hp s \\<turnstile> t \\<surd>t \\<rbrakk>\n",
      "  \\<Longrightarrow> \\<exists>T'. P,E,hp s' \\<turnstile> e':T' \\<and> P \\<turnstile> T' \\<le> T\"\n",
      "  and subjects_reduction:\n",
      "  \"\\<lbrakk> extTA,P,t \\<turnstile> \\<langle>es,s\\<rangle> [-ta\\<rightarrow>] \\<langle>es',s'\\<rangle>; E \\<turnstile> s \\<surd>; P,E,hp s \\<turnstile> es[:]Ts; P,hp s \\<turnstile> t \\<surd>t \\<rbrakk>\n",
      "  \\<Longrightarrow> \\<exists>Ts'. P,E,hp s' \\<turnstile> es'[:]Ts' \\<and> P \\<turnstile> Ts' [\\<le>] Ts\" proof (chain)\n",
      "picking this:\n",
      "  \\<exists>U. P,E,hp s' \\<turnstile> e' : U \\<and> P \\<turnstile> U \\<le> T'\n",
      "\n",
      "\n",
      "Generated step:\n",
      " theorem (in J_conf_read) assumes wf: \"wf_J_prog P\"\n",
      "  shows subject_reduction:\n",
      "  \"\\<lbrakk> extTA,P,t \\<turnstile> \\<langle>e,s\\<rangle> -ta\\<rightarrow> \\<langle>e',s'\\<rangle>; E \\<turnstile> s \\<surd>; P,E,hp s \\<turnstile> e:T; P,hp s \\<turnstile> t \\<surd>t \\<rbrakk>\n",
      "  \\<Longrightarrow> \\<exists>T'. P,E,hp s' \\<turnstile> e':T' \\<and> P \\<turnstile> T' \\<le> T\"\n",
      "  and subjects_reduction:\n",
      "  \"\\<lbrakk> extTA,P,t \\<turnstile> \\<langle>es,s\\<rangle> [-ta\\<rightarrow>] \\<langle>es',s'\\<rangle>; E \\<turnstile> s \\<surd>; P,E,hp s \\<turnstile> es[:]Ts; P,hp s \\<turnstile> t \\<surd>t \\<rbrakk>\n",
      "  \\<Longrightarrow> \\<exists>Ts'. P,E,hp s' \\<turnstile> es'[:]Ts' \\<and> P \\<turnstile> Ts' [\\<le>] Ts\"\n",
      "\n",
      "proof (chain)\n",
      "picking this:\n",
      "  \\<exists>U. P,E,hp s' \\<turnstile> e' : U \\<and> P \\<turnstile> U \\<le> T'\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by blast\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 9010, train loss=0.2749\n",
      "Epoch 0, step 9020, train loss=0.1959\n",
      "Epoch 0, step 9030, train loss=0.2789\n",
      "Epoch 0, step 9040, train loss=0.1223\n",
      "Epoch 0, step 9050, train loss=0.1015\n",
      "Epoch 0, step 9060, train loss=0.0782\n",
      "Epoch 0, step 9070, train loss=0.3971\n",
      "Epoch 0, step 9080, train loss=0.1261\n",
      "Epoch 0, step 9090, train loss=0.3042\n",
      "Epoch 0, step 9100, train loss=0.1118\n",
      "Epoch 0, step 9110, train loss=0.3244\n",
      "Epoch 0, step 9120, train loss=0.1328\n",
      "Epoch 0, step 9130, train loss=0.1170\n",
      "Epoch 0, step 9140, train loss=0.7267\n",
      "Epoch 0, step 9150, train loss=0.1721\n",
      "Epoch 0, step 9160, train loss=0.0999\n",
      "Epoch 0, step 9170, train loss=0.2731\n",
      "Epoch 0, step 9180, train loss=0.3982\n",
      "Epoch 0, step 9190, train loss=0.0991\n",
      "Epoch 0, step 9200, train loss=0.1666\n",
      "Epoch 0, step 9210, train loss=0.1627\n",
      "Epoch 0, step 9220, train loss=0.0678\n",
      "Epoch 0, step 9230, train loss=0.1473\n",
      "Epoch 0, step 9240, train loss=0.1705\n",
      "Epoch 0, step 9250, train loss=0.1106\n",
      "Epoch 0, step 9260, train loss=0.3576\n",
      "Epoch 0, step 9270, train loss=0.0759\n",
      "Epoch 0, step 9280, train loss=0.2610\n",
      "Epoch 0, step 9290, train loss=0.1935\n",
      "Epoch 0, step 9300, train loss=0.6884\n",
      "Epoch 0, step 9310, train loss=0.3856\n",
      "Epoch 0, step 9320, train loss=0.1687\n",
      "Epoch 0, step 9330, train loss=0.1524\n",
      "Epoch 0, step 9340, train loss=0.3313\n",
      "Epoch 0, step 9350, train loss=0.3118\n",
      "Epoch 0, step 9360, train loss=0.1702\n",
      "Epoch 0, step 9370, train loss=0.4256\n",
      "Epoch 0, step 9380, train loss=0.3091\n",
      "Epoch 0, step 9390, train loss=0.2360\n",
      "Epoch 0, step 9400, train loss=0.4612\n",
      "Epoch 0, step 9410, train loss=0.0851\n",
      "Epoch 0, step 9420, train loss=0.2651\n",
      "Epoch 0, step 9430, train loss=0.1419\n",
      "Epoch 0, step 9440, train loss=0.3222\n",
      "Epoch 0, step 9450, train loss=0.4420\n",
      "Epoch 0, step 9460, train loss=0.0799\n",
      "Epoch 0, step 9470, train loss=0.1092\n",
      "Epoch 0, step 9480, train loss=0.2820\n",
      "Epoch 0, step 9490, train loss=0.1702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 9500, train loss=0.5816\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lemma Exit_path_Low_path:\n",
      "  assumes \"n -as\\<rightarrow>* (_Exit_)\" and \"inner_node n\"\n",
      "  obtains a' as' where \"as = as'@[a']\" and \"n -as'\\<rightarrow>* (_Low_)\"\n",
      "  and \"kind a' = (\\<lambda>s. True)\\<^sub>\\<surd>\" \n",
      "\n",
      "\n",
      "Generated step:\n",
      " lemma Exit_path_Low_path:\n",
      "  assumes \"n -as\\<rightarrow>* (_Exit_)\" and \"inner_node n\"\n",
      "  obtains a' as' where \"as = as'@[a']\" and \"n -as'\\<rightarrow>* (_Low_)\"\n",
      "  and \"kind a' = (\\<lambda>s. True)\\<^sub>\\<surd>\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by (metis Exit_edge_Entry_or_Low Low_source_Exit_edge assms(1) assms(2) edge_det inner_node_def list.distinct(1) path.cases path_Entry_target(1) path_split(1) path_split(2) path_split(3) rev_exhaust)\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 9510, train loss=0.1502\n",
      "Epoch 0, step 9520, train loss=0.1701\n",
      "Epoch 0, step 9530, train loss=0.1277\n",
      "Epoch 0, step 9540, train loss=0.1272\n",
      "Epoch 0, step 9550, train loss=0.2566\n",
      "Epoch 0, step 9560, train loss=0.2432\n",
      "Epoch 0, step 9570, train loss=0.2081\n",
      "Epoch 0, step 9580, train loss=0.1794\n",
      "Epoch 0, step 9590, train loss=0.0727\n",
      "Epoch 0, step 9600, train loss=0.2721\n",
      "Epoch 0, step 9610, train loss=0.1342\n",
      "Epoch 0, step 9620, train loss=0.2657\n",
      "Epoch 0, step 9630, train loss=0.3067\n",
      "Epoch 0, step 9640, train loss=0.0632\n",
      "Epoch 0, step 9650, train loss=0.1341\n",
      "Epoch 0, step 9660, train loss=0.4091\n",
      "Epoch 0, step 9670, train loss=0.2563\n",
      "Epoch 0, step 9680, train loss=0.1376\n",
      "Epoch 0, step 9690, train loss=0.2800\n",
      "Epoch 0, step 9700, train loss=0.0862\n",
      "Epoch 0, step 9710, train loss=0.2624\n",
      "Epoch 0, step 9720, train loss=0.1089\n",
      "Epoch 0, step 9730, train loss=0.0904\n",
      "Epoch 0, step 9740, train loss=0.0989\n",
      "Epoch 0, step 9750, train loss=0.1954\n",
      "Epoch 0, step 9760, train loss=0.1525\n",
      "Epoch 0, step 9770, train loss=0.0714\n",
      "Epoch 0, step 9780, train loss=0.1886\n",
      "Epoch 0, step 9790, train loss=0.3715\n",
      "Epoch 0, step 9800, train loss=0.3363\n",
      "Epoch 0, step 9810, train loss=0.2201\n",
      "Epoch 0, step 9820, train loss=0.0915\n",
      "Epoch 0, step 9830, train loss=0.2453\n",
      "Epoch 0, step 9840, train loss=0.3248\n",
      "Epoch 0, step 9850, train loss=0.1489\n",
      "Epoch 0, step 9860, train loss=0.1673\n",
      "Epoch 0, step 9870, train loss=0.0906\n",
      "Epoch 0, step 9880, train loss=0.2801\n",
      "Epoch 0, step 9890, train loss=0.1429\n",
      "Epoch 0, step 9900, train loss=0.1657\n",
      "Epoch 0, step 9910, train loss=0.1945\n",
      "Epoch 0, step 9920, train loss=0.5300\n",
      "Epoch 0, step 9930, train loss=0.1252\n",
      "Epoch 0, step 9940, train loss=0.1695\n",
      "Epoch 0, step 9950, train loss=0.1517\n",
      "Epoch 0, step 9960, train loss=0.1504\n",
      "Epoch 0, step 9970, train loss=0.2213\n",
      "Epoch 0, step 9980, train loss=0.3835\n",
      "Epoch 0, step 9990, train loss=0.1733\n",
      "[Global Step 10000] Interim val_loss=0.2434\n",
      "  (No improvement. Count=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 10000, train loss=0.1519\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " instance atoms :: (atomic_boolean_algebra) atomic_boolean_algebra proof (state)\n",
      "this:\n",
      "  \\<exists>y. x = at_map y \\<and> x \\<noteq> \\<bottom>\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. \\<And>x.\n",
      "       x \\<noteq> \\<bottom> \\<Longrightarrow>\n",
      "       \\<exists>y. atom y \\<and> y \\<le> x\n",
      "\n",
      "\n",
      "Generated step:\n",
      " instance atoms :: (atomic_boolean_algebra) atomic_boolean_algebra\n",
      "\n",
      "proof (state)\n",
      "this:\n",
      "  \\<exists>y. x = at_map y \\<and> x \\<noteq> \\<bottom>\n",
      "\n",
      "goal (1 subgoal):\n",
      " 1. \\<And>x.\n",
      "       x \\<noteq> \\<bottom> \\<Longrightarrow>\n",
      "       \\<exists>y. atom y \\<and> y \\<le> x\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by (metis at_map_bot_pres atomicity)\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 10010, train loss=0.2537\n",
      "Epoch 0, step 10020, train loss=0.1449\n",
      "Epoch 0, step 10030, train loss=0.2462\n",
      "Epoch 0, step 10040, train loss=0.3909\n",
      "Epoch 0, step 10050, train loss=0.2666\n",
      "Epoch 0, step 10060, train loss=0.4367\n",
      "Epoch 0, step 10070, train loss=0.0879\n",
      "Epoch 0, step 10080, train loss=0.4265\n",
      "Epoch 0, step 10090, train loss=0.2588\n",
      "Epoch 0, step 10100, train loss=0.1999\n",
      "Epoch 0, step 10110, train loss=0.3399\n",
      "Epoch 0, step 10120, train loss=0.0949\n",
      "Epoch 0, step 10130, train loss=0.3375\n",
      "Epoch 0, step 10140, train loss=0.3111\n",
      "Epoch 0, step 10150, train loss=0.1683\n",
      "Epoch 0, step 10160, train loss=0.1730\n",
      "Epoch 0, step 10170, train loss=0.1810\n",
      "Epoch 0, step 10180, train loss=0.2605\n",
      "Epoch 0, step 10190, train loss=0.1231\n",
      "Epoch 0, step 10200, train loss=0.2946\n",
      "Epoch 0, step 10210, train loss=0.3611\n",
      "Epoch 0, step 10220, train loss=0.2866\n",
      "Epoch 0, step 10230, train loss=0.1861\n",
      "Epoch 0, step 10240, train loss=0.2055\n",
      "Epoch 0, step 10250, train loss=0.1470\n",
      "Epoch 0, step 10260, train loss=0.0988\n",
      "Epoch 0, step 10270, train loss=0.0946\n",
      "Epoch 0, step 10280, train loss=0.6242\n",
      "Epoch 0, step 10290, train loss=0.1211\n",
      "Epoch 0, step 10300, train loss=0.3280\n",
      "Epoch 0, step 10310, train loss=0.3749\n",
      "Epoch 0, step 10320, train loss=0.1610\n",
      "Epoch 0, step 10330, train loss=0.3142\n",
      "Epoch 0, step 10340, train loss=0.0730\n",
      "Epoch 0, step 10350, train loss=0.5379\n",
      "Epoch 0, step 10360, train loss=0.2754\n",
      "Epoch 0, step 10370, train loss=0.1950\n",
      "Epoch 0, step 10380, train loss=0.1368\n",
      "Epoch 0, step 10390, train loss=0.1238\n",
      "Epoch 0, step 10400, train loss=0.3461\n",
      "Epoch 0, step 10410, train loss=0.2668\n",
      "Epoch 0, step 10420, train loss=0.1172\n",
      "Epoch 0, step 10430, train loss=0.1070\n",
      "Epoch 0, step 10440, train loss=0.3926\n",
      "Epoch 0, step 10450, train loss=0.3033\n",
      "Epoch 0, step 10460, train loss=0.2952\n",
      "Epoch 0, step 10470, train loss=0.2530\n",
      "Epoch 0, step 10480, train loss=0.3838\n",
      "Epoch 0, step 10490, train loss=0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 10500, train loss=0.1071\n",
      "--------------------------------------------------\n",
      "Sample statement+state:\n",
      " lift_definition bot_ennreal :: ennreal is 0 \n",
      "\n",
      "\n",
      "Generated step:\n",
      " lift_definition bot_ennreal :: ennreal is 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reference step:\n",
      " by blast\n",
      "--------------------------------------------------\n",
      "Epoch 0, step 10510, train loss=0.1787\n",
      "Epoch 0, step 10520, train loss=0.3180\n",
      "Epoch 0, step 10530, train loss=0.1610\n",
      "Epoch 0, step 10540, train loss=0.2209\n",
      "Epoch 0, step 10550, train loss=0.1362\n",
      "Epoch 0, step 10560, train loss=0.4391\n",
      "Epoch 0, step 10570, train loss=0.1698\n",
      "Epoch 0, step 10580, train loss=0.3066\n",
      "Epoch 0, step 10590, train loss=0.2466\n",
      "Epoch 0, step 10600, train loss=0.1958\n",
      "Epoch 0, step 10610, train loss=0.4219\n",
      "Epoch 0, step 10620, train loss=0.5444\n",
      "Epoch 0, step 10630, train loss=0.1927\n",
      "Epoch 0, step 10640, train loss=0.1724\n",
      "Epoch 0, step 10650, train loss=0.1379\n",
      "Epoch 0, step 10660, train loss=0.2490\n",
      "Epoch 0, step 10670, train loss=0.2595\n",
      "Epoch 0, step 10680, train loss=0.3403\n",
      "Epoch 0, step 10690, train loss=0.3648\n",
      "Epoch 0, step 10700, train loss=0.1648\n",
      "Epoch 0, step 10710, train loss=0.2084\n",
      "Epoch 0, step 10720, train loss=0.1932\n",
      "Epoch 0, step 10730, train loss=0.2012\n",
      "Epoch 0, step 10740, train loss=0.1107\n",
      "Epoch 0, step 10750, train loss=0.1953\n",
      "Epoch 0, step 10760, train loss=0.4935\n",
      "Epoch 0, step 10770, train loss=0.1431\n",
      "Epoch 0, step 10780, train loss=0.1184\n",
      "Epoch 0, step 10790, train loss=0.5187\n",
      "Epoch 0, step 10800, train loss=0.1227\n",
      "Epoch 0, step 10810, train loss=0.1420\n",
      "Epoch 0, step 10820, train loss=0.2005\n",
      "Epoch 0, step 10830, train loss=0.1314\n",
      "Epoch 0, step 10840, train loss=0.2191\n",
      "Epoch 0, step 10850, train loss=0.1685\n",
      "Epoch 0, step 10860, train loss=0.3342\n",
      "Epoch 0, step 10870, train loss=0.3035\n",
      "Epoch 0, step 10880, train loss=0.3287\n",
      "Epoch 0, step 10890, train loss=0.1492\n",
      "Epoch 0, step 10900, train loss=0.0623\n",
      "Epoch 0, step 10910, train loss=0.1788\n",
      "Epoch 0, step 10920, train loss=1.6449\n",
      "Epoch 0, step 10930, train loss=0.2475\n",
      "Epoch 0, step 10940, train loss=0.2819\n",
      "Epoch 0, step 10950, train loss=0.2510\n",
      "Epoch 0, step 10960, train loss=0.6194\n",
      "Epoch 0, step 10970, train loss=0.1806\n",
      "Epoch 0, step 10980, train loss=0.2205\n",
      "Epoch 0, step 10990, train loss=0.3108\n",
      "[Global Step 11000] Interim val_loss=0.2446\n",
      "  (No improvement. Count=3)\n",
      "Early stopping triggered (no val improvement).\n",
      "Offline training done. Model saved to: offline_ckpt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MagnusData/deduplicated_dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_111333/1420489422.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mmain_grpo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_111333/1420489422.py\u001b[0m in \u001b[0;36mmain_grpo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Load dataset & split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train_val_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJSON_DATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train size: {len(train_data)}, Val size: {len(val_data)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_111333/1236131435.py\u001b[0m in \u001b[0;36mload_train_val_data\u001b[0;34m(json_path, test_size, random_seed)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_train_val_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MagnusData/deduplicated_dataset.json'"
     ]
    }
   ],
   "source": [
    "def main_grpo():\n",
    "    \"\"\"\n",
    "    Multi-GPU RL function using TRLs GRPO with:\n",
    "      - Chunked incremental training\n",
    "      - Sample output printing every 100 steps\n",
    "      - Standard early stopping based on reward\n",
    "      - RL validation on a held-out set\n",
    "    \"\"\"\n",
    "    JSON_DATASET_PATH = \"MagnusData/deduplicated_dataset.json\"\n",
    "    OFFLINE_CKPT = \"checkpoint_best\"  \n",
    "    RL_SAVE_DIR = \"outputs/Qwen-Theorem-GRPO\"\n",
    "\n",
    "    # Training settings\n",
    "    EVAL_EVERY = 1000   \n",
    "    PATIENCE = 3       \n",
    "    MAX_GLOBAL_STEPS = 10000\n",
    "    CHUNK_SIZE = 100    \n",
    "\n",
    "    # Initialize accelerator for multi-GPU support\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Setup Isabelle Checker\n",
    "    checker = Checker(\n",
    "        working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
    "        isa_path='/home/siai/Isabelle2022',\n",
    "        theory_file='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "        port=9000\n",
    "    )\n",
    "\n",
    "    # Load dataset & split\n",
    "    train_data, val_data = load_train_val_data(JSON_DATASET_PATH, test_size=0.1)\n",
    "    print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "    train_trl_data = build_trl_format(train_data)\n",
    "    val_trl_data   = build_trl_format(val_data)\n",
    "\n",
    "    # Load tokenizer & model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OFFLINE_CKPT)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        OFFLINE_CKPT,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"  # Enables multi-GPU training\n",
    "    )\n",
    "\n",
    "    # LoRA configuration (optional)\n",
    "    peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        lora_dropout=0.05,\n",
    "    )\n",
    "\n",
    "    # Reward function combining format + checker\n",
    "    def checker_reward(prompts, completions, answer, **kwargs):\n",
    "        return checker_reward_func(prompts, completions, answer, checker=checker)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=RL_SAVE_DIR,\n",
    "        run_name=\"Qwen-GRPO-theorems\",\n",
    "        learning_rate=1e-6,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.99,\n",
    "        weight_decay=0.1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=10,\n",
    "        bf16=True,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_generations=1,\n",
    "        max_prompt_length=512,\n",
    "        max_completion_length=512,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=9999999,\n",
    "        max_grad_norm=0.1,\n",
    "        report_to=\"wandb\",\n",
    "        log_on_each_node=False,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer with Accelerate\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[format_reward_func, checker_reward],\n",
    "        args=training_args,\n",
    "        train_dataset=train_trl_data,\n",
    "    )\n",
    "\n",
    "    # Prepare trainer for multi-GPU training\n",
    "    trainer.model, trainer.optimizer = accelerator.prepare(\n",
    "        trainer.model, trainer.optimizer\n",
    "    )\n",
    "\n",
    "    best_val_reward = -9999\n",
    "    no_improvement_count = 0\n",
    "    global_step = 0\n",
    "    stop_early = False\n",
    "\n",
    "    # RL Training Loop\n",
    "    while not stop_early and global_step < MAX_GLOBAL_STEPS:\n",
    "        steps_to_run = min(CHUNK_SIZE, MAX_GLOBAL_STEPS - global_step)\n",
    "        trainer.args.max_steps = steps_to_run\n",
    "        trainer.args.num_train_epochs = 1\n",
    "\n",
    "        # ---- Train for 'steps_to_run' steps\n",
    "        trainer.train()\n",
    "        global_step += steps_to_run\n",
    "        print(f\"Finished a block of {steps_to_run} RL steps, global_step={global_step}\")\n",
    "\n",
    "        # ---- Generate a sample output every chunk\n",
    "        if len(val_data) > 0:\n",
    "            sample = random.choice(val_data)\n",
    "            prompt = [\n",
    "                {\"role\": \"system\", \"content\": \"Prove the following theorem.\"},\n",
    "                {\"role\": \"user\", \"content\": sample[\"statement\"] + \"\\n\\n\" + sample[\"state\"]}\n",
    "            ]\n",
    "\n",
    "            completions = trainer.generate(\n",
    "                [prompt], max_new_tokens=50, do_sample=False, top_p=0.9, temperature=0.8\n",
    "            )\n",
    "            gen_text = completions[0][0]['content'] if completions else \"\"\n",
    "\n",
    "            print(\"-\"*50)\n",
    "            print(\"Sample statement+state:\\n\", sample[\"statement\"], sample[\"state\"])\n",
    "            print(\"RL-generated step:\\n\", gen_text)\n",
    "            print(\"Reference step:\\n\", sample[\"step\"])\n",
    "            print(\"-\"*50)\n",
    "\n",
    "        # RL Validation\n",
    "        if global_step % EVAL_EVERY == 0:\n",
    "            val_reward = evaluate_rl(trainer, val_trl_data, num_samples=200)\n",
    "            print(f\"[RL Validation] global_step={global_step}, val_reward={val_reward:.4f}\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_reward > best_val_reward:\n",
    "                best_val_reward = val_reward\n",
    "                no_improvement_count = 0\n",
    "                print(\"  (New best RL reward!)\")\n",
    "                accelerator.unwrap_model(trainer.model).save_pretrained(\"checkpoint_best_rl\")\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                print(f\"  (No improvement. Count={no_improvement_count})\")\n",
    "                if no_improvement_count >= PATIENCE:\n",
    "                    print(\"Early stopping triggered (RL val reward not improving).\")\n",
    "                    stop_early = True\n",
    "\n",
    "    print(f\"RL training complete at global_step={global_step}. Best val_reward={best_val_reward:.4f}\")\n",
    "    accelerator.unwrap_model(trainer.model).save_pretrained(RL_SAVE_DIR)\n",
    "    print(f\"RL final model saved in: {RL_SAVE_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_grpo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 333031, Val size: 37004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1f2eedd9074c8f9b1fff6fb568c447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbalaji-vir1997\u001b[0m (\u001b[33mbalaji-vir1997-stevens-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/balaji/Desktop/Theorem_Proving/wandb/run-20250205_155044-e3uwcdjv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/e3uwcdjv' target=\"_blank\">Qwen-GRPO-theorems</a></strong> to <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/e3uwcdjv' target=\"_blank\">https://wandb.ai/balaji-vir1997-stevens-institute-of-technology/huggingface/runs/e3uwcdjv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/balaji/.local/lib/python3.9/site-packages/torch/amp/autocast_mode.py:44: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `Qwen2ForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return func(*args, **kwargs)\n",
      "/home/balaji/.local/lib/python3.9/site-packages/trl/trainer/grpo_trainer.py:502: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/100 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/TensorCompare.cu:110: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain_grpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 118\u001b[0m, in \u001b[0;36mmain_grpo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m trainer\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_train_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# ---- Train for 'steps_to_run' steps\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m steps_to_run\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished a block of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps_to_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m RL steps, global_step=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2184\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2490\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2483\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2484\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2487\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2488\u001b[0m )\n\u001b[1;32m   2489\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2490\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2493\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2494\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2495\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2496\u001b[0m ):\n\u001b[1;32m   2497\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:3598\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3597\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3598\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3600\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3603\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3604\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/trl/trainer/grpo_trainer.py:422\u001b[0m, in \u001b[0;36mGRPOTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# Regular generation path\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n\u001b[0;32m--> 422\u001b[0m         prompt_completion_ids \u001b[38;5;241m=\u001b[39m \u001b[43munwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprompt_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m prompt_length \u001b[38;5;241m=\u001b[39m prompt_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    427\u001b[0m completion_ids \u001b[38;5;241m=\u001b[39m prompt_completion_ids[:, prompt_length:]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:2224\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2216\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2217\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2218\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2219\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2220\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2221\u001b[0m     )\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2237\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2238\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2244\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:3194\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3191\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[1;32m   3193\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3194\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3197\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3198\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3200\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:2402\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_grpo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
