{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ5x6xlUqG4N"
   },
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxKlbfLhqG4P"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sf7gq1YJqG4Q"
   },
   "source": [
    "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xpcb5j3qG4Q"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xnm_FUyqG4S"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1zyu9Ug2XEt"
   },
   "source": [
    "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59DIs5BMcvjN",
    "outputId": "a4b3de70-c99c-4e76-ee06-dab6a6505a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siai/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 15:03:56,441\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8-SLRUB2gwM"
   },
   "source": [
    "Load up `Llama 3.1 8B Instruct`, and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700,
     "referenced_widgets": [
      "d8d0dca36cfc47f0919924da07c231e8",
      "5f3d96b613e94e9984d4599ca9ca7b17",
      "66c3271554b1455eb56be55c9241e45e",
      "d36b61cf796c429080e93ea838a3759e",
      "94873c3c077e483790b34f95c421f484",
      "ea549fffa8c2469888d1668158bc105c",
      "98b432b98839428f85d91580c21e80e2",
      "fee4f852c9744a07b909e586e3615604",
      "3febcf8a8eca40c28aafc697f3ec8776",
      "b4e1eb8eeb064c88a2142e474fb8327f",
      "da10502506f9448c9de94f1ddd84d3b1",
      "e6cc388e78c14abfaa49d2be6fa1b5d9",
      "769bde36e2ba4434bddd78e7d5911be4",
      "3c522d78b1834068bd4b155d0f87a4d7",
      "a23afba19c2a4d3a90d771fc55f8d490",
      "6221f0be3b8d48e797c873565a216680",
      "1ac03aff5c314b00ac938c80eb7b2f8a",
      "88c63d94a05a42c49d5f8958a27987a6",
      "0ca67b0c4ca64eb788358a51308f6b97",
      "83c3c811923a4642aba156d1215b39d2",
      "e863bf099e064da7b482c21fe7b77de7",
      "697faad6643a43aca98015da4faef186"
     ]
    },
    "id": "DkIvEkIIkEyB",
    "outputId": "514dea04-804e-47a8-b891-ed3f4a6fb530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-27 15:04:03 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.536 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading kings-crown/Isabelle_FVELer_SFT with actual GPU utilization = 79.4%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 47.54 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 288.\n",
      "Unsloth: vLLM's KV Cache can use up to 22.96 GB. Also swap space = 6 GB.\n",
      "INFO 02-27 15:04:10 config.py:549] This model supports multiple tasks: {'score', 'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 02-27 15:04:10 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='kings-crown/Isabelle_FVELer_SFT', speculative_config=None, tokenizer='kings-crown/Isabelle_FVELer_SFT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=kings-crown/Isabelle_FVELer_SFT, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":288}, use_cached_outputs=False, \n",
      "INFO 02-27 15:04:10 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 02-27 15:04:11 model_runner.py:1110] Starting to load model kings-crown/Isabelle_FVELer_SFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W227 15:04:11.777014621 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-27 15:04:11 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.36it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  3.26it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.52it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.07it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-27 15:04:13 model_runner.py:1115] Loading model weights took 14.3854 GB\n",
      "INFO 02-27 15:04:13 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 02-27 15:04:15 worker.py:267] Memory profiling takes 1.34 seconds\n",
      "INFO 02-27 15:04:15 worker.py:267] the current vLLM instance can use total_gpu_memory (47.54GiB) x gpu_memory_utilization (0.79) = 37.74GiB\n",
      "INFO 02-27 15:04:15 worker.py:267] model weights take 14.39GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.58GiB; the rest of the memory reserved for KV Cache is 21.72GiB.\n",
      "INFO 02-27 15:04:15 executor_base.py:111] # cuda blocks: 25420, # CPU blocks: 7021\n",
      "INFO 02-27 15:04:15 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 198.59x\n",
      "INFO 02-27 15:04:20 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:26<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-27 15:04:46 model_runner.py:1562] Graph capturing finished in 26 secs, took 2.12 GiB\n",
      "INFO 02-27 15:04:46 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 33.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.2.15 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "import time\n",
    "\n",
    "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
    "lora_rank = 128 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"kings-crown/Isabelle_FVELer_SFT\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.8, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Checker(object):\n",
    "    \"\"\"A modified version of the Draft, Sketch, Prove proof-checking client.\n",
    "    (https://github.com/albertqjiang/draft_sketch_prove/blob/main/autoformalization/checker.py)\n",
    "\n",
    "    This checker supports Isabelle2022 via the new version of PISA\n",
    "    (https://albertqjiang.github.io/Portal-to-ISAbelle/).\n",
    "\n",
    "    It supports checking a miniF2F-style proof via `check`.\n",
    "\n",
    "    Finally, it replaces `sledgehammer` with a call to `normalhammer`.\n",
    "    \"\"\"\n",
    "    def __init__(self, working_dir, isa_path, theory_file_path, port=9000):\n",
    "        sys.path.append(os.environ.get('PISA_PATH', ''))\n",
    "        try:\n",
    "            from pisa_client import initialise_env\n",
    "            self.initialise_env = initialise_env\n",
    "        except ImportError:\n",
    "            print(\"Set $PISA_PATH to /yourpath/to/Portal-to-ISAbelle/src/main/python\")\n",
    "\n",
    "        self.working_dir = working_dir\n",
    "        self.isa_path = isa_path\n",
    "        self.theory_file_path = theory_file_path\n",
    "        self.port = port\n",
    "\n",
    "    def _initialize(self):\n",
    "        \"\"\"Initialize the PISA environment.\"\"\"\n",
    "        env = self.initialise_env(\n",
    "            self.port,\n",
    "            isa_path=self.isa_path,\n",
    "            theory_file_path=self.theory_file_path,\n",
    "            working_directory=self.working_dir\n",
    "        )\n",
    "        return env\n",
    "\n",
    "    def _exit(self, env):\n",
    "        \"\"\"Exit the environment and clean up resources.\"\"\"\n",
    "        try:\n",
    "            env.post('exit')\n",
    "        except Exception:\n",
    "            pass\n",
    "        os.system(\"ps aux | grep Isabelle | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "        os.system(\"ps aux | grep poly | awk '{print $2}' | xargs kill -9 > /dev/null 2>&1\")\n",
    "\n",
    "    def _parse_output(self, obs):\n",
    "        \"\"\"Parse the sledgehammer output, returning the relevant part.\"\"\"\n",
    "        return obs.split('<hammer>')[0] if '<hammer>' in obs else ''\n",
    "\n",
    "    def _run_step(self, step, i, tls_name, env):\n",
    "        \"\"\"Run a single proof step.\"\"\"\n",
    "        try:\n",
    "            obs, reward, done, metadata = env.step_to_top_level_state(\n",
    "                action=step,\n",
    "                tls_name=tls_name,\n",
    "                new_name=f'default_{i}'\n",
    "            )\n",
    "            return obs, reward, done, metadata, None\n",
    "        except Exception as e:\n",
    "            return '', 0, False, None, str(e)\n",
    "\n",
    "    def _run_sledgehammer(self, step, i, tls_name, env):\n",
    "        \"\"\"Run sledgehammer or fallback heuristics on a step.\"\"\"\n",
    "        heuristics = [\n",
    "            'by auto', 'by simp', 'by blast', 'by fastforce',\n",
    "            'by force', 'by eval', 'by presburger', 'by sos',\n",
    "            'by arith', 'by linarith', 'by (auto simp: field_simps)'\n",
    "        ]\n",
    "        for heuristic in heuristics:\n",
    "            step_ = step.replace('normalhammer', heuristic)\n",
    "            obs, reward, done, metadata, error = self._run_step(step_, i, tls_name, env)\n",
    "            if error is None:\n",
    "                obs = f'{heuristic} <hammer> {obs}'\n",
    "                return obs, reward, done, metadata, error\n",
    "        return self._run_step(step.replace(\"normalhammer\", \"sledgehammer\"), i, tls_name, env)\n",
    "\n",
    "    def check(self, statement_and_proof):\n",
    "        \"\"\"Check the given proof.\"\"\"\n",
    "        env = self._initialize()\n",
    "        env.initialise()\n",
    "\n",
    "        theory = self.wrap_theorem(statement_and_proof)\n",
    "        steps = self.get_parsed(env, theory)\n",
    "\n",
    "        result = self._check(env, steps)\n",
    "        self._exit(env)\n",
    "\n",
    "        # Output the result\n",
    "        #print(\"\\n==== Success: %s\" % result['success'])\n",
    "        #print(\"--- Complete proof:\\n%s\" % result['theorem_and_proof'])\n",
    "        return result\n",
    "\n",
    "    def _check(self, env, steps):\n",
    "        \"\"\"Run the proof steps and collect results.\"\"\"\n",
    "        success, reason, done = False, '', False\n",
    "        step_results = []\n",
    "        tls_name = 'default'\n",
    "\n",
    "        for i, step in enumerate(steps):\n",
    "            time0 = time.time()\n",
    "            if 'normalhammer' in step or 'sledgehammer' in step:\n",
    "                obs, reward, done, metadata, error = self._run_sledgehammer(step, i, tls_name, env)\n",
    "            else:\n",
    "                obs, reward, done, metadata, error = self._run_step(step, i, tls_name, env)\n",
    "\n",
    "            step_time = time.time() - time0\n",
    "            step_results.append({\n",
    "                'index': i, 'step': step, \n",
    "                'output': self._parse_output(obs), \n",
    "                'step_time': step_time\n",
    "            })\n",
    "\n",
    "            if error:\n",
    "                reason = error\n",
    "                break\n",
    "            tls_name = f'default_{i}'\n",
    "\n",
    "        success = done and reward == 1.0\n",
    "        return {\n",
    "            'success': success,\n",
    "            'reason': reason,\n",
    "            'num_steps': len(steps),\n",
    "            'last_step': len(step_results),\n",
    "            'step_results': step_results,\n",
    "            'theorem_and_proof': self.reconstruct(step_results) if success else ''\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct(step_results):\n",
    "        \"\"\"Reconstruct the complete proof.\"\"\"\n",
    "        return '\\n'.join(\n",
    "            step_result['output'].strip() if step_result['output'] else step_result['step'].strip()\n",
    "            for step_result in step_results[1:]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_theorem(theorem):\n",
    "        \"\"\"Wrap the theorem in a theory file.\"\"\"\n",
    "        return (\n",
    "            'theory Interactive imports HOL.HOL Complex_Main '\n",
    "            '\"HOL-Library.Code_Target_Numeral\" \"HOL-Library.Sum_of_Squares\" '\n",
    "            '\"Symmetric_Polynomials.Vieta\" \"HOL-Computational_Algebra.Computational_Algebra\" '\n",
    "            '\"HOL-Number_Theory.Number_Theory\" \\n begin\\n%s' % theorem\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parsed(env, theory):\n",
    "        \"\"\"Parse the theory and extract proof steps.\"\"\"\n",
    "        raw_steps = env.post(f\"<parse text> ${theory}\")\n",
    "        steps = [s.strip() for s in raw_steps.split('<SEP>') if s.strip() and s != '$']\n",
    "        processed_steps = []\n",
    "        for i, step in enumerate(steps):\n",
    "            if step.lower() == \"then\" and (i == 0 or steps[i - 1].startswith(\"proof\")):\n",
    "                continue\n",
    "            processed_steps.append(step)\n",
    "        return processed_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../')\n",
    "os.environ['PISA_PATH'] = '/home/siai/Portal-to-ISAbelle/src/main/python'\n",
    "\n",
    "#import dsp_utils\n",
    "\n",
    "checker = Checker(\n",
    "    working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
    "    isa_path='/home/siai/Isabelle2022',\n",
    "    theory_file_path='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "    port=9000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KGgPgk_5S8r"
   },
   "source": [
    "### Data Prep\n",
    "<a name=\"Data\"></a>\n",
    "\n",
    "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cXk993X6C2ZZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1138/1138 [00:00<00:00, 9890.36 examples/s] \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"Write a proof in Isabelle that appropriately proves the given statement in natural language.\n",
    "Make sure to wrap the proof within ``isabelle and ``` tags inside answer.\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "[Your explanation or chain of thought]\n",
    "</reasoning>\n",
    "<answer>\n",
    "``` \n",
    "isabelle \n",
    "[Your formal Isabelle code]\n",
    "``` \n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extracts the content inside <answer>...</answer> tags in XML format.\n",
    "    Handles missing tags, extra whitespace, and multiple matches.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None  # Return None for empty or invalid input\n",
    "\n",
    "    match = re.search(r\"<answer>\\s*(.*?)\\s*</answer>\", text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def extract_isabelle_snippet(text: str) -> str | None:\n",
    "\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None  # Handle empty input safely\n",
    "\n",
    "    if \":\" in text and \"```isabelle\" in text:\n",
    "        text = text.split(\"```isabelle\", 1)[1]  # Take the part after first occurrence\n",
    "        text = \"```isabelle\" + text  # Ensure consistency\n",
    "\n",
    "    code_match = re.search(r\"```isabelle\\s*(.*?)\\s*```\", text, re.DOTALL | re.IGNORECASE)\n",
    "    if code_match:\n",
    "        return code_match.group(1).strip()  # Return stripped match\n",
    "   \n",
    "    inline_match = re.search(r\"(lemma.*?proof.*?qed)\", text, re.DOTALL | re.IGNORECASE)\n",
    "    if inline_match:\n",
    "        return inline_match.group(1).strip()\n",
    "\n",
    "    return None  \n",
    "\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('kings-crown/FVELer_PISA_Proven', 'default')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['natural_language_statement']}\n",
    "        ],\n",
    "        'answer': extract_isabelle_snippet(x['formal_proof'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_isabelle_snippet(r) for r in responses]\n",
    "    #print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def checker_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_snippets = [extract_isabelle_snippet(r) for r in responses]\n",
    "\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_snippets[0]}\")\n",
    "\n",
    "    for content in extracted_snippets:\n",
    "        checker = Checker(\n",
    "            working_dir='/home/siai/Isabelle2022/src/HOL/Examples',\n",
    "            isa_path='/home/siai/Isabelle2022',\n",
    "            theory_file_path='/home/siai/Isabelle2022/src/HOL/Examples/Interactive.thy',\n",
    "            port=9000\n",
    "        )\n",
    "        #result = checker.check(content)\n",
    "        rewards = [2.0 if checker.check(content).get(\"success\", False) else 0.0 for content in extracted_snippets]\n",
    "    return rewards\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural_language_statement': 'The lemma states that if set \\\\( S \\\\) is a subset of set \\\\( T \\\\), and there exists an element \\\\( x \\\\) such that \\\\( S \\\\) is the singleton set containing \\\\( x \\\\), and there exists an element \\\\( y \\\\) such that \\\\( T \\\\) is the singleton set containing \\\\( y \\\\), then \\\\( S \\\\) is equal to \\\\( T \\\\).',\n",
       " 'formal_proof': 'To translate the informal proof into a structured Isabelle proof, we will follow the steps outlined in the informal reasoning and use `sledgehammer` to assist in finding any necessary lemmas or theorems. Here\\'s how the structured proof in Isabelle might look:\\n\\n```isabelle\\nlemma eq:\\n  assumes \"S \\\\<subseteq> T\"\\n    and \"\\\\<exists>x. S = {x}\"\\n    and \"\\\\<exists>y. T = {y}\"\\n  shows \"S = T\"\\nproof -\\n  from `\\\\<exists>x. S = {x}` obtain x where \"S = {x}\" by auto\\n  from `\\\\<exists>y. T = {y}` obtain y where \"T = {y}\" by auto\\n  have \"x \\\\<in> T\" using `S \\\\<subseteq> T` `S = {x}` by auto\\n  then have \"x = y\" using `T = {y}` by auto\\n  then have \"S = T\" using `S = {x}` `T = {y}` by simp\\n  thus ?thesis by simp\\nqed\\n```\\n\\n### Explanation:\\n\\n1. **Assumptions**: We start by assuming the conditions given in the lemma: `S \\\\<subseteq> T`, `\\\\<exists>x. S = {x}`, and `\\\\<exists>y. T = {y}`.\\n\\n2. **Existential Elimination**: We use `obtain` to extract the elements `x` and `y` such that `S = {x}` and `T = {y}`.\\n\\n3. **Subset Condition**: From the subset condition `S \\\\<subseteq> T` and the fact that `S = {x}`, we deduce that `x` must be an element of `T`.\\n\\n4. **Singleton Property**: Since `T = {y}`, the only element in `T` is `y`, so `x = y`.\\n\\n5. **Set Equality**: With `x = y`, we conclude that `S = {x} = {y} = T`.\\n\\n6. **Conclusion**: The proof concludes with `S = T`, as required by the lemma.\\n\\nThis structured proof closely follows the informal reasoning and uses basic set properties and logical deductions to reach the conclusion.',\n",
       " 'prompt': [{'content': 'Write a proof in Isabelle that appropriately proves the given statement in natural language.\\nMake sure to wrap the proof within ``isabelle and ``` tags inside answer.\\nRespond in the following format:\\n<reasoning>\\n[Your explanation or chain of thought]\\n</reasoning>\\n<answer>\\n``` \\nisabelle \\n[Your formal Isabelle code]\\n``` \\n</answer>\\n',\n",
       "   'role': 'system'},\n",
       "  {'content': 'The lemma states that if set \\\\( S \\\\) is a subset of set \\\\( T \\\\), and there exists an element \\\\( x \\\\) such that \\\\( S \\\\) is the singleton set containing \\\\( x \\\\), and there exists an element \\\\( y \\\\) such that \\\\( T \\\\) is the singleton set containing \\\\( y \\\\), then \\\\( S \\\\) is equal to \\\\( T \\\\).',\n",
       "   'role': 'user'}],\n",
       " 'answer': 'lemma eq:\\n  assumes \"S \\\\<subseteq> T\"\\n    and \"\\\\<exists>x. S = {x}\"\\n    and \"\\\\<exists>y. T = {y}\"\\n  shows \"S = T\"\\nproof -\\n  from `\\\\<exists>x. S = {x}` obtain x where \"S = {x}\" by auto\\n  from `\\\\<exists>y. T = {y}` obtain y where \"T = {y}\" by auto\\n  have \"x \\\\<in> T\" using `S \\\\<subseteq> T` `S = {x}` by auto\\n  then have \"x = y\" using `T = {y}` by auto\\n  then have \"S = T\" using `S = {x}` `T = {y}` by simp\\n  thus ?thesis by simp\\nqed'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux6iqP7z5YOo"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqkXK2D4d6p",
    "outputId": "9d5551f4-0276-47ca-e4ca-e96c846cc976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 2 to the `num_generations` of 6\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
    "    num_generations = 6, # Decrease if out of memory\n",
    "    max_prompt_length = 256,\n",
    "    max_completion_length = 3000,\n",
    "    num_train_epochs = 3, # Set to 1 for a full training run\n",
    "    max_steps = 500,\n",
    "    save_steps = 50,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"wandb\", # Can use Weights & Biases\n",
    "    output_dir = \"output_RL\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Mv8UZO5hz-"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vzOuSVCL_GA9",
    "outputId": "a71824d4-afd8-47ac-f0ea-eae276927e19"
   },
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ['PISA_PATH'] = '/home/siai/Portal-to-ISAbelle/src/main/python'\n",
    "\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func,\n",
    "        checker_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlaUdxC_VHpz"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "qtcz_lpbVC92",
    "outputId": "9b12655a-7905-42a8-d6f0-210ff74a6d73"
   },
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Colxz9TAVMsi"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AL-BcuB1VLIv"
   },
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwpbwnDBVRLg"
   },
   "source": [
    "Now we load the LoRA and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "zf_OY5WMVOxF",
    "outputId": "c34d81a7-192d-427d-81f0-cbca7009b7d1"
   },
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aDgFfhFYIAS"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NUEmHFSYNTp"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjXGTkp7YNtB"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52WMb3k_YPt8"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyEjW-WuYQIm"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n33Lwz_eqG4Z"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ca67b0c4ca64eb788358a51308f6b97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ac03aff5c314b00ac938c80eb7b2f8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c522d78b1834068bd4b155d0f87a4d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ca67b0c4ca64eb788358a51308f6b97",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83c3c811923a4642aba156d1215b39d2",
      "value": 1
     }
    },
    "3febcf8a8eca40c28aafc697f3ec8776": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5f3d96b613e94e9984d4599ca9ca7b17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea549fffa8c2469888d1668158bc105c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_98b432b98839428f85d91580c21e80e2",
      "value": ""
     }
    },
    "6221f0be3b8d48e797c873565a216680": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66c3271554b1455eb56be55c9241e45e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fee4f852c9744a07b909e586e3615604",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3febcf8a8eca40c28aafc697f3ec8776",
      "value": 1
     }
    },
    "697faad6643a43aca98015da4faef186": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "769bde36e2ba4434bddd78e7d5911be4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ac03aff5c314b00ac938c80eb7b2f8a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_88c63d94a05a42c49d5f8958a27987a6",
      "value": ""
     }
    },
    "83c3c811923a4642aba156d1215b39d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "88c63d94a05a42c49d5f8958a27987a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94873c3c077e483790b34f95c421f484": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98b432b98839428f85d91580c21e80e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a23afba19c2a4d3a90d771fc55f8d490": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e863bf099e064da7b482c21fe7b77de7",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_697faad6643a43aca98015da4faef186",
      "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:16&lt;00:00,‚Äá16.13s/it]\n"
     }
    },
    "b4e1eb8eeb064c88a2142e474fb8327f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d36b61cf796c429080e93ea838a3759e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4e1eb8eeb064c88a2142e474fb8327f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_da10502506f9448c9de94f1ddd84d3b1",
      "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:27&lt;00:00,‚Äá27.50s/it]\n"
     }
    },
    "d8d0dca36cfc47f0919924da07c231e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f3d96b613e94e9984d4599ca9ca7b17",
       "IPY_MODEL_66c3271554b1455eb56be55c9241e45e",
       "IPY_MODEL_d36b61cf796c429080e93ea838a3759e"
      ],
      "layout": "IPY_MODEL_94873c3c077e483790b34f95c421f484"
     }
    },
    "da10502506f9448c9de94f1ddd84d3b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6cc388e78c14abfaa49d2be6fa1b5d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_769bde36e2ba4434bddd78e7d5911be4",
       "IPY_MODEL_3c522d78b1834068bd4b155d0f87a4d7",
       "IPY_MODEL_a23afba19c2a4d3a90d771fc55f8d490"
      ],
      "layout": "IPY_MODEL_6221f0be3b8d48e797c873565a216680"
     }
    },
    "e863bf099e064da7b482c21fe7b77de7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea549fffa8c2469888d1668158bc105c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fee4f852c9744a07b909e586e3615604": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
